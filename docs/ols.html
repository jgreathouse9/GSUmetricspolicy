<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Econometrics for Policy Analysis - 6&nbsp; OLS Explained</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./treatmenteffects.html" rel="next">
<link href="./correlation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">OLS Explained</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Econometrics for Policy Analysis</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Syllabus: PMAP 4041, Fall 2024</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Policy Studies</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Mathematics and Econometric Theory</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basicprob.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Basic Probability Theory</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotic.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Asymptotic Theory</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./correlation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Correlation and Association</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">OLS Explained</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./treatmenteffects.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Causal Inference</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Applied Research Methods</span>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#math-preliminaries" id="toc-math-preliminaries" class="nav-link active" data-scroll-target="#math-preliminaries"><span class="toc-section-number">6.1</span>  Math Preliminaries</a>
  <ul class="collapse">
  <li><a href="#a-primer-on-data-types" id="toc-a-primer-on-data-types" class="nav-link" data-scroll-target="#a-primer-on-data-types"><span class="toc-section-number">6.1.1</span>  A Primer on Data Types</a></li>
  <li><a href="#review-of-lines-and-functions" id="toc-review-of-lines-and-functions" class="nav-link" data-scroll-target="#review-of-lines-and-functions"><span class="toc-section-number">6.1.2</span>  Review of Lines and Functions</a></li>
  </ul></li>
  <li><a href="#arrivederci-algebra-ciao-derrivatives." id="toc-arrivederci-algebra-ciao-derrivatives." class="nav-link" data-scroll-target="#arrivederci-algebra-ciao-derrivatives."><span class="toc-section-number">6.2</span>  Arrivederci, Algebra, Ciao Derrivatives.</a>
  <ul class="collapse">
  <li><a href="#power-rule" id="toc-power-rule" class="nav-link" data-scroll-target="#power-rule"><span class="toc-section-number">6.2.1</span>  Power Rule</a></li>
  <li><a href="#chain-rule" id="toc-chain-rule" class="nav-link" data-scroll-target="#chain-rule"><span class="toc-section-number">6.2.2</span>  Chain Rule</a></li>
  </ul></li>
  <li><a href="#an-extended-example" id="toc-an-extended-example" class="nav-link" data-scroll-target="#an-extended-example"><span class="toc-section-number">6.3</span>  An Extended Example</a>
  <ul class="collapse">
  <li><a href="#list-the-data" id="toc-list-the-data" class="nav-link" data-scroll-target="#list-the-data"><span class="toc-section-number">6.3.1</span>  List the Data</a></li>
  <li><a href="#define-our-econometric-model" id="toc-define-our-econometric-model" class="nav-link" data-scroll-target="#define-our-econometric-model"><span class="toc-section-number">6.3.2</span>  Define Our Econometric Model</a></li>
  <li><a href="#write-out-the-objective-function" id="toc-write-out-the-objective-function" class="nav-link" data-scroll-target="#write-out-the-objective-function"><span class="toc-section-number">6.3.3</span>  Write Out the Objective Function</a></li>
  <li><a href="#substitute-into-the-objective-function" id="toc-substitute-into-the-objective-function" class="nav-link" data-scroll-target="#substitute-into-the-objective-function"><span class="toc-section-number">6.3.4</span>  Substitute Into the Objective Function</a></li>
  <li><a href="#take-partial-derivatives" id="toc-take-partial-derivatives" class="nav-link" data-scroll-target="#take-partial-derivatives"><span class="toc-section-number">6.3.5</span>  Take Partial Derivatives</a></li>
  <li><a href="#get-the-betas" id="toc-get-the-betas" class="nav-link" data-scroll-target="#get-the-betas"><span class="toc-section-number">6.3.6</span>  Get the Betas</a></li>
  <li><a href="#our-ols-line-of-best-fit" id="toc-our-ols-line-of-best-fit" class="nav-link" data-scroll-target="#our-ols-line-of-best-fit"><span class="toc-section-number">6.3.7</span>  Our OLS Line of Best Fit</a></li>
  </ul></li>
  <li><a href="#inference-for-ols" id="toc-inference-for-ols" class="nav-link" data-scroll-target="#inference-for-ols"><span class="toc-section-number">6.4</span>  Inference For OLS</a>
  <ul class="collapse">
  <li><a href="#goodness-of-fit-measures-for-ols" id="toc-goodness-of-fit-measures-for-ols" class="nav-link" data-scroll-target="#goodness-of-fit-measures-for-ols"><span class="toc-section-number">6.4.1</span>  Goodness of Fit Measures for OLS</a></li>
  </ul></li>
  <li><a href="#assumptions-of-ols" id="toc-assumptions-of-ols" class="nav-link" data-scroll-target="#assumptions-of-ols"><span class="toc-section-number">6.5</span>  Assumptions of OLS</a>
  <ul class="collapse">
  <li><a href="#assumption-1-linear-in-parameters" id="toc-assumption-1-linear-in-parameters" class="nav-link" data-scroll-target="#assumption-1-linear-in-parameters"><span class="toc-section-number">6.5.1</span>  Assumption 1: Linear in Parameters</a></li>
  <li><a href="#assumption-2-random-sample" id="toc-assumption-2-random-sample" class="nav-link" data-scroll-target="#assumption-2-random-sample"><span class="toc-section-number">6.5.2</span>  Assumption 2: Random Sample</a></li>
  <li><a href="#assumption-3-no-perfect-collinearity" id="toc-assumption-3-no-perfect-collinearity" class="nav-link" data-scroll-target="#assumption-3-no-perfect-collinearity"><span class="toc-section-number">6.5.3</span>  Assumption 3: No Perfect Collinearity</a></li>
  <li><a href="#assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0" id="toc-assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0" class="nav-link" data-scroll-target="#assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0"><span class="toc-section-number">6.5.4</span>  Assumption 4: Strict Exogeneity: <span class="math inline">\(\mathbb{E}[\epsilon_{i} | x_{i1}, \ldots, x_{iK}] = 0\)</span></a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="toc-section-number">7</span>  Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">OLS Explained</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>This is the chapter on regression. We begin by covering data types. Then, we review the idea of a function and how it relates to a line. After a review of derivatives, we finally cover the computation of regression coefficients, inference, fit, and OLS assumptions.</p>
<section id="math-preliminaries" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="math-preliminaries"><span class="header-section-number">6.1</span> Math Preliminaries</h2>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is a statistics course, not a math course. However, math is the language which underlies statistics. This is the <em>only</em> part of the course where we use any calculus to explain ideas. Furthermore, you’ll never be expected (from me) to use calculus for any of your assignments.</p>
<p><em>With this said</em>, I do use the calculus to explain what is going on in regression. The calculus explains the <em>why</em>, which I think is always important to know if you’re implementing or interpreting regression. I presume however that you are like myself when I was in undergrad, in that you’ve either never taken calculus (me) or had little exposure to it. So, I link to tutorials on the calculus concepts that we employ here. Consult these links, if you’d like (they sure helped me in preparation of this chapter). However, they are completely optional. I try to explain everything as best I can, so they are provided as a supplement to the curious who want a better understanding.</p>
</div>
</div>
<section id="a-primer-on-data-types" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="a-primer-on-data-types"><span class="header-section-number">6.1.1</span> A Primer on Data Types</h3>
<p>For any dataset you ever work with, you’ll likely have different variables (columns). Regression is no exception. The predictors for regression must be numeric, naturally. These take a few different types. Note that if it is not numeric, we call it a “<em>string</em>”.</p>
<p>The most common kind is a ratio variable (a value we may express as a fraction/continuous variable), such as the employment rate.</p>
<table class="table">
<thead>
<tr class="header">
<th>State</th>
<th>Year</th>
<th>Employment Rate (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alabama</td>
<td>1990</td>
<td>55.3</td>
</tr>
<tr class="even">
<td>Alabama</td>
<td>1991</td>
<td>56.1</td>
</tr>
<tr class="odd">
<td>California</td>
<td>1990</td>
<td>62.1</td>
</tr>
<tr class="even">
<td>California</td>
<td>1991</td>
<td>61.5</td>
</tr>
<tr class="odd">
<td>Georgia</td>
<td>1990</td>
<td>58.4</td>
</tr>
<tr class="even">
<td>Georgia</td>
<td>1991</td>
<td>59.2</td>
</tr>
</tbody>
</table>
<p>A dummy variable is a binary variable that indicates the presence or absence of a characteristic. A dummy variable (also called an <em>indicator</em> or <em>categorical</em> variable) is a variable that takes on the values 0 or 1. For example, a simple dummy indicates whether a respondent in a survey is a male or a female. In this case, the number 1 “maps” on to the value for male, and 0 for female. Note that in this case, the simple average of these respective columns returns the proportion of our observations that are male or female.</p>
<table class="table">
<thead>
<tr class="header">
<th>Respondent ID</th>
<th>Gender (Male=1, Female=0)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>2</td>
<td>0</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1</td>
</tr>
<tr class="even">
<td>4</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Dummies can also be used to capture unobserved variation across groups. For instance, when predicting homicide rates across states like Alabama, California, and Georgia for 1990 and 1991, we can include dummy variables for each state. These dummies help account for unique, stable characteristics of each state, such as culture, that are hard to measure directly. In other words, if we think something “makes” Alabama, Alabama, compared to California or Georgia, we can include these kinds of variables to capture that unobserved variation. When including dummies in regression, we must always omit one category from the regression (for reasons we will explain below). So, for example, we could include Alabama/Georgia dummies or Georgia/California dummies, where California and Alabama would be what we call the <em>reference group</em>.</p>
<table class="table">
<thead>
<tr class="header">
<th>State</th>
<th>Year</th>
<th>Alabama (1/0)</th>
<th>California (1/0)</th>
<th>Georgia (1/0)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alabama</td>
<td>1990</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>Alabama</td>
<td>1991</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>California</td>
<td>1990</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>California</td>
<td>1991</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>Georgia</td>
<td>1990</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>Georgia</td>
<td>1991</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>There is also a notion of an <em>ordinal</em> variable, where the data at hand must obey a specific order. Suppose we ask people in a survey how religious they are on a scale from 1 to 10, where 1=Atheist and 10=Extremely Religious. Here, order matters, because 1 has a very different meaning from 10 in this instance. An ordinal variable has a clear, ordered ranking between its values.</p>
<table class="table">
<thead>
<tr class="header">
<th>Respondent ID</th>
<th>Religiosity (1-10)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>3</td>
</tr>
<tr class="even">
<td>2</td>
<td>7</td>
</tr>
<tr class="odd">
<td>3</td>
<td>5</td>
</tr>
<tr class="even">
<td>4</td>
<td>10</td>
</tr>
</tbody>
</table>
<p>These are the data types you will generally work with. When we move on to real datasets, their meaning will become much more apparent.</p>
</section>
<section id="review-of-lines-and-functions" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="review-of-lines-and-functions"><span class="header-section-number">6.1.2</span> Review of Lines and Functions</h3>
<p>In middle school, we learn about <a href="https://www.youtube.com/watch?v=VhokQhjl5t0">the basics of functions</a> in that when we plug in a number, we get another number in return. If you’re at the grocery store and grapes are 1 dollar and 50 cents per pound, we just weigh the grapes and multiply that number by 1.5. This could take the form of <span class="math inline">\((0,0), (1,1.5), (2,3)\)</span>, and so on. In fact, we can represent these data points in a table like this</p>
<table class="table">
<thead>
<tr class="header">
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.5</td>
</tr>
<tr class="odd">
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>These points form a line, <a href="https://www.youtube.com/watch?v=K_OI9LA54AA">the equation for which being</a> <span class="math inline">\(y=mx+b\)</span>. We can also think of this line as a function. It returns a value of <span class="math inline">\(y\)</span> given some values for previous expenditures and pounds of grapes. Here, <span class="math inline">\(y\)</span> is how much we pay in total, <span class="math inline">\(m\)</span> is the rate of change in how much we pay for every 1 pound of grapes bought, and <span class="math inline">\(b\)</span> is our value we pay if we get no grapes.</p>
<div class="cell" data-engine="jupyter" data-execution_count="1">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For this case, the function for the line is <span class="math inline">\(y=1.5x\)</span>. For here, <span class="math inline">\(b=0\)</span> because in this case, how much we pay is a function of pounds of grapes only. We could add a constant/<span class="math inline">\(b\)</span>, though. Suppose we’d already spent 10 dollars, and now how much we spend is a function of both some previous constant level of spending, and new amount of grapes bought. Now, our function is <span class="math inline">\(y=1.5x+10\)</span>. The way we find the <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> for a straight line is <a href="https://www.youtube.com/watch?v=MeU-KzdCBps">the “rise over run” method</a>, in this case</p>
<p><span class="math display">\[
m = \frac{y_2 - y_1}{x_2 - x_1} = \frac{3 - 0}{2 - 0} = \frac{3}{2} = 1.5
\]</span></p>
<p>Lines fit to points sometimes have discrepancies between the line and the data we see. We call this the residual, or <span class="math inline">\(\hat \epsilon = y-(mx+b)\)</span>. Also notice how the rate of change, or <span class="math inline">\(1.5x\)</span>, is the same at every point on the line: in this instance, you pay a dollar and fifty cents for <em>any</em> amount of grapes we get.</p>
<p>Regression, fundamentally, is about fitting a line (or a plane) to a set of data given some inputs. Going forward, I will use the words “outcome variable” or “dependent variable” to refer to the thing that we are studying the change of, and “predictors”, “covariates”, or “independent variables” to refer to the variables we think affect our outcome. In the grape example, our outcome is the total amount of money we spend, and the singular predictor we use is how many pounds of grapes we get. With all this being said though, this example is very simplistic. After all, the necessary information is known to us up front (price and weight). But… what if the data we have at hand are not nice and neat in terms of a function? Suppose we consider a more challenging example.</p>
<p>Take the idea of predicting <a href="https://raw.githubusercontent.com/danilofreire/homicides-sp-synth/master/data/df.csv">crime rates in Brazilian states</a> in the year 1990 using the inequality level as a predictor, or <a href="https://data.cdc.gov/Policy/Table-of-Gross-Cigarette-Tax-Revenue-Per-State-Orz/rkpp-igza/about_data">data on</a> the consumption of cigarettes in American states in the year 1980 using price as a predictor. We would presume some <em>function</em> exists that generates the crime rate for that Brazilian state in that year, or that consumption level for that American state in that year. We would also imagine, naturally, that the covairates inequality and tobacco price would affect these outcomes. But, does it make sense to expect for some deterministic function to predict these values, given some input? No.</p>
<p>The homicide rate or cigarette consumption rate in any state anywhere is not guaranteed. In some states, homicides or tobacco consumption is high, other times its low. Why? Well for homicide, some states are wealthier and some are poorer. Some states vary by racial compositions, or will differ by factors like age composition, alcohol use, and gun ownership. Thus… some cities have high homicide rates, others have low homicide rates. We can reason accordingly for cigarette consumption of American states. Natrually, one reason for this would be the price of cigarettes, as one might expect, since people tend to not want to buy more of a good as the price increases (<a href="https://sites.lsa.umich.edu/mje/2022/01/10/veblen-goods-why-sports-cars-and-diamonds-dont-obey-the-law-of-demand/">well… usually</a>.) The number of young people in that state may mean that younger people are risk takers and may be more likely to smoke than adults (or alternatively, young people may perceive smoking as something for older adults and smoke less). Levels of alcohol taxation may matter as well, since alcohol may be a substitute for tobacco, so states with higher taxation may smoke more, on average. Also, plain and simple measures like culture (and other unobservable things) may play a role. We can plot these data for illustration</p>
<div class="cell" data-engine="jupyter" data-execution_count="2">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Here, I draw a line between these input variables and the observed outcomes in question. The <span class="math inline">\(x\)</span> axis represents, respectively, inequality and price and the <span class="math inline">\(y\)</span> axes represent homicide rates and cigarette consumption. It is quite obvious though that no <em>deterministic</em> function exists here for either of these cases, as we have residuals. The line <em>imperfectly</em> describes the relationship between 1) inequality and homicide and 2) retail price of cigarettes and cigarette consumption per capita. So, we can’t find one line that fits perfectly to all of these data points. But, even if we cannot find a perfect, deterministic function that fits to all of these data points, how about we find the <em>optimal</em> line in the sense that it best <em>estimates</em> <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> by having the lowest possible values for <span class="math inline">\(\hat \epsilon\)</span> at every point on the line? We can see how this relates to the grape analogy above: the line passes through all of the observed data points, meaning it is optimal in the sense that the line has the lowest possible residual values.</p>
</section>
</section>
<section id="arrivederci-algebra-ciao-derrivatives." class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="arrivederci-algebra-ciao-derrivatives."><span class="header-section-number">6.2</span> Arrivederci, Algebra, Ciao Derrivatives.</h2>
<p>To do this though, we’ve now reached a point in the course where simple algebra is no longer our best guide. We now <em>must</em> use calculus, specifically the basics of <a href="https://www.youtube.com/watch?v=9vKqVkMQHKk">derivatives</a> and optimization. I mentioned the word <em>optimal</em> above to refer to the fit of the line, but now we’re going to get a much better sense of what is meant by this.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Okay, so here I’m <em>kind of</em> lying. You actually <strong>don’t</strong> <em>need</em> to say farewell to algebra (completely) to derive regression estimates, but <a href="https://www.youtube.com/watch?v=mIx2Oj5y9Q8&amp;t=16s">that process</a> “requires a ton of algebraic manipulation”. For those brave of heart who know algebra well, you can <em>probably</em> just watch the series of videos I just linked to and skip to <a href="https://jgreathouse9.github.io/GSUmetricspolicy/ols.html#inference-for-ols">this section</a> of the notes, but I <strong>do not</strong> recommend this at all. I find the calculus way via optimization a lot more intuitive.</p>
</div>
</div>
<p>Firstly though, <a href="https://www.youtube.com/watch?v=N2PpRnFqnqY">a primer on derivatives</a>. The derivative is the slope of a curve/line given a very small change in the value of the function. It is the instantaneous rate of change for a function. We do not need derivatives for linear functions, since we know what the rate of change is from real life (“a dollar fifty <em>per</em> pound”, “two <em>for</em> five”, etc.). For example, the derivative of <span class="math inline">\(50x=y\)</span> is just 50, since that is the value y changes by for any increase in <span class="math inline">\(x\)</span>. For a constant (say, 5), the derivative is always 0, since no matter what value <span class="math inline">\(x\)</span> takes, its value does not change at all.</p>
<p>When we set the first derivative of a function to 0 and solve, we reach an optimal point on <em>the original function</em>, <a href="https://www.youtube.com/watch?v=8aAU4r_pUUU">usually</a>. An optimal point (or “critical point”) is a place on the function where the value of the function is at the lowest or highest possible value the function can reach over a given set of inputs. We can find the critical points by solving an optimization problem. An optimization problem takes the form of <span class="math display">\[
\min_{\theta \in \Theta } f(\theta) \: \mathrm{ s.t. \:}  g(\theta) = 0, h(\theta) \leq 0,
\]</span> where there’s some function <span class="math inline">\(f(\theta)\)</span> (called the <em>objective function</em>) that is minimized over a set of <span class="math inline">\(g(\theta)\)</span> equality constraints and <span class="math inline">\(h(\theta)\)</span> inequality constraints. The word “<span class="math inline">\(\text{min}\)</span>” here means “minimize”. It means that we are seeking the values, or <em>arguments</em>, which minimize the output of that function. The symbols underneath this, <span class="math inline">\(\theta \in \Theta\)</span> represents the coefficients we are solving for. But this is all abstract. Let’s do some examples.</p>
<section id="power-rule" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="power-rule"><span class="header-section-number">6.2.1</span> Power Rule</h3>
<p>Suppose we shoot a basketball while we stand on a 2 foot plateau, which produces a trajectory function of <span class="math inline">\(h(t)= −5t^2 +20t+2\)</span>. Here <span class="math inline">\(h(t)\)</span> is a function representing the ball’s height over time in seconds. The <span class="math inline">\(-5t^2\)</span> reflects the downward pull of gravity. The <span class="math inline">\(20t\)</span> means that you threw the ball at 20 miles per hour originally. And the 2 means that we’re standing 2 feet above solid ground. We can find the <em>maximum</em> height of the ball by taking the derivative of the original quadratic function and solving it for 0. In this case, we use <a href="https://www.youtube.com/watch?v=BYTfCnR9Sl0">the power rule</a> for derivatives. The power rule for dertivatives, expressed formally as <span class="math inline">\(\frac{d}{dx}(x^n) = nx^{n-1}\)</span>, is where we subtract the exponent value of a function by 1 and place the original value to be multiplied by the base number. For example, the derivative of <span class="math inline">\(y=2x^3\)</span> is just <span class="math inline">\(6x^2\)</span>, since <span class="math inline">\(3-1=2\)</span> and <span class="math inline">\(2 \times 3 = 6\)</span>. The derivative of <span class="math inline">\(x^2\)</span> is <span class="math inline">\(2x\)</span>. With this in mind, we set up our objective function</p>
<p><span class="math display">\[
H = \underset{t \in \mathbb R}{\text{argmax}} \left(−5t^2 +20t+2 \right).
\]</span></p>
<p>Here, we seek the value of time, in seconds, where the ball is at the maximum height possible.</p>
<div class="cell" data-engine="jupyter" data-execution_count="3">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We then follow the rules I’ve explained above, differentiating with respect to (<em>w.r.t</em>) each term.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; h(t) = -5t^2 + 20t + 2 = \\
&amp; \frac{d}{dt}(-5t^2) + \frac{d}{dt}(20t) + \frac{d}{dt}(2)= \\
&amp; t(5\times 2) +1(20) = \overbrace{-10t + 20}^{\text{Derivative}}
\end{aligned}
\]</span></p>
<p>Let’s break this down. The derivative of <span class="math inline">\(-5x^2\)</span> must be <span class="math inline">\(-10x\)</span> by the power rule. All this means is that for every additional second, the ball falls by ten more feet due to gravity. The 20 reflects the initial velocity that we threw the ball at, or the 20 feet per second I mentioned above. And the derivative for 2 is 0 because time has no influence on the height of ground we threw the ball from, we started from where we started from, gravity is what it is, and thus this should not affect the rate of change of the height of the ball. We can then solve the derivative for 0 to find the optimal point.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; -10t + 20=0 \Rightarrow  \\
&amp; -10t = -20 \Rightarrow \\
&amp; \frac{-10t}{10} = \frac{-20}{-10} \Rightarrow \\
&amp; \boxed{t=2}
\end{aligned}
\]</span></p>
<p>Okay, so the vall is at its zenith after 2 seconds. We may now plug in the value of 2 into the original function to get the maximum height of the ball:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; -5t^2 + 20t + 2 \Rightarrow \\
&amp; -5(2)^2 + 20(2) + 2 \Rightarrow \\
&amp; -20+40+2=22
\end{aligned}
\]</span></p>
<p>From here, we can see why we set the derivative to 0. Since the derivative is the rate of change of the function at a specific point, and we know the ball is rising vertically, we also know the ball msut be slowing down over time. The maximum point is simply the height where the speed of the ball is 0 miles per hour, and therefore not changing anymore so that it may be pulled to earth.</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(t\)</span></th>
<th><span class="math inline">\(h(t)\)</span></th>
<th><span class="math inline">\(h'(t)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>2</td>
<td>20</td>
</tr>
<tr class="even">
<td>0.1</td>
<td>3.95</td>
<td>19</td>
</tr>
<tr class="odd">
<td>1</td>
<td>17</td>
<td>10</td>
</tr>
<tr class="even">
<td>1.5</td>
<td>19.75</td>
<td>5</td>
</tr>
<tr class="odd">
<td>2</td>
<td>22</td>
<td>0</td>
</tr>
<tr class="even">
<td>2.5</td>
<td>21.25</td>
<td>-5</td>
</tr>
</tbody>
</table>
<p>We can know that we are at a maximum by taking <a href="https://www.youtube.com/watch?v=-cW5hCsc9Yc">the second derivative</a> of our function, which would just be <span class="math inline">\(-10\)</span>. When the second derivative <span class="math inline">\(h(t)^{\prime \prime}&lt; 0\)</span>, we know that we are at a maximum point.</p>
</section>
<section id="chain-rule" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="chain-rule"><span class="header-section-number">6.2.2</span> Chain Rule</h3>
<p>The next rule of differentiation to know is something called <a href="https://youtu.be/XIQ-KnsAsbg?si=53HjEWztsQh0g6O2">the chain rule</a>. The chain rule is called that <a href="https://www.youtube.com/watch?v=FKraGDm2fUY">because</a> of the <em>chain</em> reaction we can think of when we think of the way the value of one function affects the value of another function. In mathematics this is called a <a href="https://youtu.be/wUNWjd4bMmw?si=J4GbqUBTLvQ9DV-B"><em>composite function</em></a>.</p>
<p>A common example in economics is where we wish to maximize profits. Suppose we are the manager of shipping for a company. Our job is to ship kilos of product to a city so that a wholeseller, or a <em>distributor</em>, may sell to vendors who will in turn sell to customers. However, we are not doing business in a competitive market. The local distributor has a monopoly over entry ports, and is able to set prices. The distributor will give us 75,000 dollars per <span class="math inline">\(x\)</span> kilos of product we give them. As the sellers, we must come up with the right amount of kilos to sell such that we maxmize our profits, given the price we face. Ideally, we could snap our fingers and sell them the product in an unlimited manner. In other words, the profit we’d make per ki would simply be <span class="math inline">\(r(x) = 75x\)</span>.</p>
<p>But unfortunately, we do have costs. We have a set of <em>fixed costs</em> comprising baseline expenditures of doing business with the distributor. For example, we may have transportation costs to move our product from home to the city, a certain amount of money for fuel, and other costs that we simply cannot avoid paying in order to do business at all (in the ball analogy, this would be how far we are off the grouund originally). We have <em>variable costs</em> which are things that we as the producers, in the very short run, have direct control over, say, how many drivers we have or how much plastic we use, the number of workers on our farms, and so on (in the ball analogy above, this would be how hard we throw the ball). And finally, we have marginal costs, or the costs that we bear as the business for each new ki produced (the downward pull of gravity in the above example).</p>
<p>For this example, our cost function is <span class="math inline">\(c(x)=0.25(x+6)^2+191\)</span> (note that all of these numbers are in 1000s, but we shall solve the profit exactly as I’ve written it). As we’ve discussed above, the constant will at some point go away, and we will be left with only the factors of production that actually change (our marginal and variable costs). But I’m getting ahead of myself. First, we have to think about what profit means.</p>
<p>In microeconomic theory, our profit function is <span class="math inline">\(\pi=r(x)-c(x)\)</span>, or the difference between how much we make in dollars versus how much it costed us in dollars to make it. Think of how much profit we make as a function of product sold. If we produced nothing, we’d be losing money since we still have to pay fixed costs (200 in this case). However, as we produce one more ki, we slowly increase the amount of money we make until our revenue equals our total costs (or, as much as earn selling the product is equal to how much it costed us to make the product). After this point, as we produce more, our revenue begins to exceed our costs (say, as we get more efficient in distributing workers and tools to make the product, thereby reducing costs). However at some point, the rate of change of profit will be equal to 0.</p>
<p>Why? Well, we can’t keep producing forever because we do not have infinite resources. This means that while we may detect a change in profits from 1 kilo to 100, at some point it will not make sense to produce another kilo because the cost it takes to make another kilo is greater than the revenue we would make (in other words, we’d need to employ more people than necessary or use more tools than necessary just to make some given price point). Like the ball example, we’re looking to ascend the profit function by producing more until the rate of change of profit is at 0. And, since we’ve been producing <em>more</em> up until this point, this must mean that any more production after this point will decrease our profits.</p>
<p>To maximize profit, we differentiate each component of the profit function, <span class="math inline">\(\pi'(x) = \frac{\mathrm{d}}{\mathrm{d}x}[r(x)] - \frac{\mathrm{d}}{\mathrm{d}x}[c(x)]\)</span>. Why? Well, we need to see what the rates of change are for both functions, revenue and costs, since our profit function is affected by both functions. In other words, we need for it to make sense to produce more kis. If the revenue we make is greater than our costs, then this means we still can produce more and make a profit. If our costs are greater than our revenue, we should produce less since this suggests we’re profiting less than what we could be. When we take the derivative of both and solve for 0, we can find the point at which our overall profits are maximized. Our objective function in this case looks like</p>
<p><span class="math display">\[
\underset{x \in \mathbb R}{\text{argmax}} \left(\underbrace{75x}_{r(x)} - [\underbrace{0.25(x+6)^2+191}_{c(x)}] \right).
\]</span> First, we substitute these functions <span class="math display">\[
\begin{aligned}
&amp;r(x) = 75x \\
&amp;c(x) = 0.25(x+6)^2 + 191
\end{aligned}
\]</span> and then take their derivatives accordingly. Beginning with revenue, <span class="math display">\[
\begin{aligned}
&amp;r(x) = 75x \\
&amp;\boxed{\frac{d}{dx}= 75}.
\end{aligned}
\]</span></p>
<p>Simple enough. Profit increases by 75,000 per ki sold. Now, for <span class="math inline">\(c(x) = 0.25(x+6)^2 + 191\)</span>, we apply the chain rule.</p>
<p>The power rule states <span class="math inline">\(\frac{d}{dx}(u^n) = n \cdot u^{n-1} \cdot \frac{du}{dx}\)</span>, where <span class="math inline">\(u = (x+6)\)</span> and <span class="math inline">\(n = 2\)</span>. First we differentiate the outer function by applying the power rule (notice how the constant vanishes)</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\frac{d}{dx}\left( 0.25(x+6)^2 +191\right) = 0.25 \cdot 2 \cdot (x+6)^{2-1} +191 \cdot \frac{d}{dx}(x+6) \\
&amp; [0.5(x+6)^1 + 0] \cdot 1 \Rightarrow 0.5(x+6).
\end{aligned}
\]</span> We are left with, after distributing the one-half term <span class="math display">\[
c^{\prime}(x) = 0.5(x+6) \Rightarrow (0.5\times x) + (0.5 \times 6) \Rightarrow 0.5(x+6)=\boxed{0.5x+3}.
\]</span></p>
<p>Next, we combine the derivatives together in the original equation</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\pi'(x) = r'(x) - c'(x) = \\
&amp; 75 - 0.5x - 3 = \\
&amp;72 - 0.5x.
\end{aligned}
\]</span></p>
<p>Now we solve for 0:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;72 - 0.5x = 0 \\
&amp;72= 0.5x  \\
&amp;\frac{72}{0.5}=\frac{0.5}{0.5}x \\
&amp;\boxed{\text{Optimal Production Level: }144=x}.
\end{aligned}
\]</span></p>
<p>To verify that we are maximizing, we take the derivative of the profit function’s derivative <span class="math display">\[
\begin{aligned}
&amp;\frac{d}{dx}[75 - 0.5x - 3] \\
&amp;\pi''(x) = -0.5.
\end{aligned}
\]</span> The derivative of the two constants are both 0, so those are deleted, and all we’re left with is the linear term. Since <span class="math inline">\(\pi''(x) &lt; 0\)</span>, we are at a maximum point. Therefore, the number of kilos we should sell our distributor is 144. What is profit? <span class="math inline">\((75x) - (0.25(x+6)^2 + 191)\)</span> where <span class="math inline">\(x = 144\)</span>. First, calculate <span class="math inline">\((x+6)^2\)</span>: <span class="math display">\[
(x+6)^2 = (144+6)^2 = 150^2
\]</span> <span class="math display">\[
150^2 = 22500
\]</span></p>
<p>Next, calculate <span class="math inline">\(0.25(x+6)^2\)</span>: <span class="math display">\[
0.25(x+6)^2 = 0.25 \times 22500 = 5625
\]</span></p>
<p>Next we calculate <span class="math inline">\(75 \times 144\)</span>: <span class="math display">\[
75 \times 144 = 10800.
\]</span></p>
<p>Now we add the total costs: <span class="math display">\[
5625 + 191 = 5816
\]</span></p>
<p>Subtract total revenue from total costs <span class="math display">\[
10800 - 5816 = 4984
\]</span></p>
<p>Therefore, our profit is <span class="math inline">\(\boxed{4,984,000}\)</span>. To verify that none of what we just did is voodoo, we can check this in Python by plotting the profit function and use <code>cvxpy</code>.</p>
<div class="cell" data-engine="jupyter" data-execution_count="4">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Ok, all done for now. We will use derivatives to solve for the value which minimizes the sum of residuals squared. This is known as <em>ordinary least squares</em> regression (OLS), also called <em>linear regression</em>, or simply just “regression”. OLS is the main estimator you’ll use for this class, and it is the main foundation of econometric analysis for public policy research. It will be much more involved than what we just did, but this provides the mathematical foundation for regression as an estimator.</p>
</section>
</section>
<section id="an-extended-example" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="an-extended-example"><span class="header-section-number">6.3</span> An Extended Example</h2>
<p>To introduce OLS, we can return to the equation of a line (<span class="math inline">\(y=mx+b\)</span>) where <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> are variables. Unlike the above examples where <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> were once known variables, now they are unknown quantities we must solve for. Below, <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> will take on the values of <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_0\)</span> respectively. With OLS, we have multiple predictors (typically), each of which affect the output of the function differently.</p>
<p>In the multivariable case, we take the <em>partial derivative</em> with resepct to each variable (that is, assuming that the other variables do not change). If this seems at all abstract to you, I will provide a detailed, clear derivation of the betas. Note that for all of the steps below, Stata, R, or Python does (and optimizes!) this process for you. I only provide this derivation so you have a source to refer to when you wish to know how and <em>why</em>, exactly, the machine returns the numbers that it returns to you.</p>
<p>Before we continue, let’s fix ideas. Suppose we wish to attend a nightclub and we wish to express how much we pay for that evening as a function (our outcome variable, a ratio level variable). At this nightclub, our outcome is a function of two things. We pay <em>some</em> one-time cost of money to enter, and then we pay <em>some</em> amount of money per new drink we buy (where number of drinks is also a ratio level variable). I say “<em>some</em>” because unlike the real world where we would know the price and entry fees by reading the sign, in this case we wish to estimate these values with only two known variables: how many drinks we bought and how much we paid.</p>
<div class="cell" data-engine="jupyter" data-execution_count="5">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="list-the-data" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="list-the-data"><span class="header-section-number">6.3.1</span> List the Data</h3>
<p>Say that we have data that looks like <span class="math inline">\((0, 30), (1, 35), (2, 40)\)</span>, where <span class="math inline">\(x\)</span>= number of drinks we buy (0,1,2) and <span class="math inline">\(y\)</span>=amount of money we spend that evening (30,35,40). In spreadsheet format, this looks like:</p>
<table class="table">
<thead>
<tr class="header">
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>30</td>
</tr>
<tr class="even">
<td>1</td>
<td>35</td>
</tr>
<tr class="odd">
<td>2</td>
<td>40</td>
</tr>
</tbody>
</table>
<p>If you want to, calculate the rise-over-run of these data points to derive <span class="math inline">\(m\)</span> and see what the answer might be in the end. Below though, we proceed by deriving what <span class="math inline">\(m\)</span> <em>must be</em>.</p>
<p><span class="math display">\[
m = \frac{35 - 30}{1 - 0} = \ldots
\]</span></p>
</section>
<section id="define-our-econometric-model" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="define-our-econometric-model"><span class="header-section-number">6.3.2</span> Define Our Econometric Model</h3>
<p>We begin by defining our model. That is, we specify our outcome and the variables which affect our outcome (the values we’re solving for, entry price and drink cost). Our model of how much we pay given some entry fees and additional drink costs looks like:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1x
\]</span></p>
<p>Here, <span class="math inline">\(y_i\)</span> is the total amount of money we spend that evening in dollars given the <span class="math inline">\(i\text{-th}\)</span> drink, <span class="math inline">\(\hat{\beta}_{0}\)</span> is how much we pay (also in dollars) to enter, <span class="math inline">\(\hat{\beta}_{1}\)</span> is how much we pay for the <span class="math inline">\(i\text{-th}\)</span> drink, and <span class="math inline">\(x\)</span> is the total number of drinks we get. Nothing <strong>at all</strong> about this is different, so far, from anything we’ve discussed above. I’ve simply substituted <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> with the Greek letter <span class="math inline">\(\beta\)</span> (“beta”) into the already familiar equation for a line.</p>
</section>
<section id="write-out-the-objective-function" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="write-out-the-objective-function"><span class="header-section-number">6.3.3</span> Write Out the Objective Function</h3>
<p>Now that we have our model, next let’s think about what the objective function would be. We already know that we wish to minimize the residuals of the OLS line. So, we can represent the objective function for OLS as <span class="math display">\[
\begin{aligned}
&amp; S = {\text{argmin}} \sum_{i=1}^{n} \hat{\epsilon}^{2}  \\
&amp; S = {\text{argmin}} \sum_{i=1}^{n} \left( y_i - \hat{y} \right)^{2} \\
&amp; \text{where } \hat{y} \text{ is defined as} \\
&amp; S = \underset{\hat{\beta}_0,\hat{\beta}_1}{\text{argmin}} \sum_{i=1}^{n} (y_i - (\overbrace{\hat{\beta}_0 + \hat{\beta}_1x}^{\text{Predictions}}))^2.
\end{aligned}
\]</span></p>
<p>Let’s explain what this means. The word “<span class="math inline">\(\text{argmin}\)</span>” here means “argument of the minimum”. It means that we are seeking the values which minimize the output of that function. I call the objective function “<span class="math inline">\(S\)</span>” for “spending”, but we can call it any letter we’d like. As above, we call the solutions <span class="math inline">\(\hat{\beta}_0,\hat{\beta}_1\)</span> <em>optimal</em> if they return the lowest possible values the function <span class="math inline">\(S\)</span> can produce. What values can <span class="math inline">\(S\)</span> produce? The sum of the squared residuals. The sigma symbol <span class="math inline">\(\sum_{i=1}^{n}\)</span> means we are adding up the <span class="math inline">\(i\text{-th}\)</span> squared residual to the <span class="math inline">\(n\text{-th}\)</span> data point/number of observations (in this case 3). This means that the line we compute will be as close as it can be to the observed data at every single data point. By the way, notice how instead of addition (as we did for maximization above), we now use subtraction in the objective function.</p>
<p>Now that we’ve defined our terms, we can think about what the mathematics of the objective function is actually saying. <span class="math inline">\(\hat{\epsilon}_i\)</span> is our predicted residuals, as we’ve mentioned above. The middle formula re-expresses the residuals. This expression should look familiar. <a href="https://jgreathouse9.github.io/GSUmetricspolicy/basicprob.html#variance">Recall</a> the formula for the variance of a variable <span class="math inline">\(\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]\)</span> ?. Here, we are taking our real observations <span class="math inline">\(y\)</span> and subtracting some <em>expected value</em> <span class="math inline">\(\hat{y}\)</span> (y-hat) from it. Because we are minimizing the residuals, another way of thinking about OLS is that is the line that maximizes the explained variance given our independent variables.</p>
<p>By the way, just to show the objective function is not some esoteric thing, we can literally plot the objective function.</p>
<div class="cell" data-engine="jupyter" data-execution_count="6">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>I plot the values returned by the objective function given the datapoints and some value for our coefficients. Of course, we wish to get the values for the betas which produce the lowest possible values for this plane. Clearly, the intercept can’t be 50 and the slope 20 (as this maximizes the residuals!).</p>
<p>As with the variance, one may ask <em>why</em> we are squaring the summed <span class="math inline">\(\hat{\epsilon}_i\)</span>, instead of say <a href="https://learningds.org/ch/04/modeling_loss_functions.html#mean-absolute-error">the absolute error</a>. <a href="https://www.youtube.com/watch?v=U46D7oEijlI">First of all</a>, minimizing the raw sum of the predicted residuals (that is, without squaring them) is a non-differentiable function. <a href="https://medium.com/@polanitzer/the-minimum-mean-absolute-error-mae-challenge-928dc081f031">We could</a> use the raw sum of the errors as an objective function (called the <em>mean absolute error</em> instead of the <em>mean squared error</em>), but <a href="https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/algorithm/2.linearRegressionGradientDescent.md#gradient-descent-to-solve-linear-regression-with-mean-absolute-error-mae-loss-function">have fun doing that</a>, as due to the non-differentiable nature of the absolute value function, we would need to use numerical methods, such as gradient descent, to compute its solution. By no means impossible… just computationally less tractable.</p>
<p>Using the squared residuals means that we are dealing a quadratic function which, as we did above, is easily differentiable. The squaring of residuals also penalizes worse predictions. Indeed, just as with the variance, all residuals <em>should</em> not be created equally. If the observed value is 20 but we predict 25, the residual is -5. Its squared residual is 25. But if the observed value is 40, and we predict 80, the “absolute” error is -40 and the squared error of is 1600. If we did not square them, we would be treating a residual of 5 as the same weight as a residual of 40. For proof, we can plot these</p>
<div class="cell" data-engine="jupyter" data-execution_count="7">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="substitute-into-the-objective-function" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="substitute-into-the-objective-function"><span class="header-section-number">6.3.4</span> Substitute Into the Objective Function</h3>
<p>First, we can substitute the real values as well as our model for prediction into the objective function. We already know the values x-takes. You either buy no drinks, 1 drink, or 2. So with this information, we can now find the amount of money we pay up front (<span class="math inline">\(\hat{\beta}_0\)</span>) and how much it costs for each drink (<span class="math inline">\(\hat{\beta}_1\)</span>) <span class="math display">\[
\begin{aligned}
S = &amp;\underbrace{(30 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 0))^2}_{\text{Term 1}} + \\
&amp;\underbrace{(35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1))^2}_{\text{Term 2}} + \\
&amp; \underbrace{(40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2))^2}_{\text{Term 3}}
\end{aligned}
\]</span></p>
<p>And when we look at the notation carefully, we see all of this makes sense: we are adding up the differences between our outcome, and the predictions of our model.</p>
</section>
<section id="take-partial-derivatives" class="level3" data-number="6.3.5">
<h3 data-number="6.3.5" class="anchored" data-anchor-id="take-partial-derivatives"><span class="header-section-number">6.3.5</span> Take Partial Derivatives</h3>
<p>To find the values of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, we take the partial derivatives of <span class="math inline">\(S\)</span> with respect to (w.r.t.) <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>. Here is a short sketch of how we do this: For simplicity, I break this into two sections, one section per coefficient. In this case, the chain rule and power rules for differentiation are our friends here. To hear more about combining the power rule and chain rule, <a href="https://www.youtube.com/watch?v=TI-j-fr6c4A">see here</a>. First, we differentiate w.r.t. <span class="math inline">\(\hat{\beta}_0\)</span> (entry fees), then we do the same for <span class="math inline">\(\hat{\beta}_1\)</span> (drink fees).</p>
<ol type="1">
<li><strong>Partial derivative w.r.t. <span class="math inline">\(\hat{\beta}_0\)</span>:</strong></li>
</ol>
<p>Here is our full objective function:</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_0} = \frac{\partial}{\partial \hat{\beta}_0} \left[ (30 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 0))^2 + (35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1))^2 + (40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2))^2 \right]
\]</span></p>
<p>By the chain rule, we can take the partial derivative by applying the power rule to the outer functions and the linear differentiation rule to the inner functions: <span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_0} = \frac{\partial}{\partial \hat{\beta}_0} (30 - \hat{\beta}_0+ \hat{\beta}_1 \cdot 0)^2 + \frac{\partial}{\partial \hat{\beta}_0} (35 - (\hat{\beta}_0 + \hat{\beta}_1))^2 + \frac{\partial}{\partial \hat{\beta}_0} (40 - (\hat{\beta}_0 + 2\hat{\beta}_1))^2.
\]</span></p>
<p>Note that the first term has <span class="math inline">\(\hat{\beta}_1 \cdot 0\)</span>, so I delete it from the remaining math. We get this result:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \frac{\partial S}{\partial \hat{\beta}_0} = 2(30 - (\hat{\beta}_0)) \cdot (-1) + 2(35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1)) \cdot (-1) + 2(40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2)) \cdot (-1).
\end{aligned}
\]</span></p>
<p>We put the 2 on the outside of the parentheses. We then take the derivative for the term for the inner functions <em>w.r.t.</em> <span class="math inline">\(\hat{\beta}_0\)</span>. Since the minus sign means that all terms within the inner functions become negative (for example, <span class="math inline">\(35-(\hat{\beta}_0 + \hat{\beta}_1\cdot 1)\)</span> or <span class="math inline">\(30 - (\hat{\beta}_0)\)</span>), the function is multplied by negative 1 since that is the dertivative of <span class="math inline">\(-\hat{\beta}_0\)</span>. If we recall the <a href="https://www.youtube.com/watch?v=3NHSwiv_pSE">distributive property</a>, we know we can factor out the negative 2 and negative 1.Thus, we can now simplify the above by using brackets:</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_0} = -2 \left[ (30 - \hat{\beta}_0) + (35 - \hat{\beta}_0 - \hat{\beta}_1) + (40 - \hat{\beta}_0 - 2\hat{\beta}_1) \right].
\]</span></p>
<p>Next I rearrange everything inside of the brackets: <span class="math display">\[
\begin{aligned}
&amp;(30 + 35 + 40) - (\hat{\beta}_0 - \hat{\beta}_0 + \hat{\beta}_0) + (\hat{\beta}_1 - 2\hat{\beta}_1) = \\
&amp;(105)+(- 3\hat{\beta}_0)+(-3\hat{\beta}_1).
\end{aligned}
\]</span> Finally, we just distribute the 2:</p>
<p><span class="math display">\[
\begin{aligned}
-2 \left[ 105 - 3\hat{\beta}_0 - 3\hat{\beta}_1 \right] \\
= \boxed{-210 + 6\hat{\beta}_0 + 6\hat{\beta}_1}.
\end{aligned}
\]</span></p>
<p>This is our partial derivative for the first coefficient, or for the entry fee.</p>
<ol start="2" type="1">
<li><strong>Partial derivative w.r.t. <span class="math inline">\(\hat{\beta}_1\)</span>:</strong></li>
</ol>
<p>We can follow a similar process for this partial derivative: <span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_1} = \frac{\partial}{\partial \hat{\beta}_1} \left[ (30 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 0))^2 + (35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1))^2 + (40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2))^2 \right]
\]</span></p>
<p>Using the chain rule, this looks like:</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_1} = \frac{\partial}{\partial \hat{\beta}_1} (30 - \hat{\beta}_0)^2 + \frac{\partial}{\partial \hat{\beta}_1} (35 - (\hat{\beta}_0 + \hat{\beta}_1))^2 + \frac{\partial}{\partial \hat{\beta}_1} (40 - (\hat{\beta}_0 + 2\hat{\beta}_1))^2
\]</span></p>
<p>where we apply the power rule to the outer terms and the linear differentiation rules to each inner term:</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_1} = 2(30 - \hat{\beta}_0) \cdot (0) + 2(35 - (\hat{\beta}_0 + \hat{\beta}_1)) \cdot (-1) + 2(40 - (\hat{\beta}_0 + 2\hat{\beta}_1)) \cdot (-2).
\]</span></p>
<p>As we can see, the 2 again is a common term, which we again put outside in brackets:</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_1} = 2 \left[ 0 \cdot (30 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 0)) - 1 \cdot (35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1)) - 2 \cdot (40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2)) \right].
\]</span></p>
<p>As before,the first term vanishes because it is multiplied by 0</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_1} = 2 \left[ - (35 - (\hat{\beta}_0 + \hat{\beta}_1)) - 2 \cdot (40 - (\hat{\beta}_0 + 2\hat{\beta}_1)) \right].
\]</span></p>
<p>Next, we get rid of the inner parentheses by distributing the negative 1</p>
<p><span class="math display">\[
2 \left[ - (35 - \hat{\beta}_0 - \hat{\beta}_1) - 2 \cdot (40 - \hat{\beta}_0 - 2\hat{\beta}_1) \right].
\]</span></p>
<p>We apply the distributive property again for the 1 and 2</p>
<p><span class="math display">\[
2 \left[ - (35 - \hat{\beta}_0 - \hat{\beta}_1) - (80 - 2\hat{\beta}_0 - 4\hat{\beta}_1) \right],
\]</span></p>
<p>and distribute the negative 1 again:</p>
<p><span class="math display">\[
2 \left[ -35 + \hat{\beta}_0 + \hat{\beta}_1 - 80 + 2\hat{\beta}_0 + 4\hat{\beta}_1 \right].
\]</span></p>
<p>Now we rearrange and put the same terms next to each other</p>
<p><span class="math display">\[
2 \left[ \hat{\beta}_0 + 2\hat{\beta}_0 + \hat{\beta}_1 + 4\hat{\beta}_1 - 35 - 80 \right]
\]</span></p>
<p>and simplify by combining them together:</p>
<p><span class="math display">\[
2 \left[ 3\hat{\beta}_0 + 5\hat{\beta}_1 - 115 \right].
\]</span></p>
<p>Thus after distributing the 2 inside of the brackets, the partial derivative of <span class="math inline">\(S\)</span> with respect to <span class="math inline">\(\hat{\beta}_1\)</span> is:</p>
<p><span class="math display">\[
\boxed{6\hat{\beta}_0 + 10\hat{\beta}_1 - 230}
\]</span></p>
<p>In fact, I’ll put both of them together:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\boxed{\frac{\partial S}{\partial \hat{\beta}_0}  = 6\hat{\beta}_0 + 10\hat{\beta}_1 - 230} \\
&amp;\boxed{\frac{\partial S}{\partial \hat{\beta}_1}  =-210 + 6\hat{\beta}_0 + 6\hat{\beta}_1}.
\end{aligned}
\]</span></p>
<p>Before we continue, do not lose sight of our goal: all these two equations represent are the instantaneous rates of change in our sum of squared residuals given some change in the variable in question. Now we must solve them for the optimal point.</p>
</section>
<section id="get-the-betas" class="level3" data-number="6.3.6">
<h3 data-number="6.3.6" class="anchored" data-anchor-id="get-the-betas"><span class="header-section-number">6.3.6</span> Get the Betas</h3>
<p>Okay, no more calculus. We can now return to <em>algebraland</em> to get our betas, with a slight modification. Remember how above after we calculated the normal derivative of the profit function or the ball trajectory we just solved <span class="math inline">\(t\)</span> or <span class="math inline">\(x\)</span> for 0? Well, now we don’t just have one variable! We have 2. As before, we still want to set these both equal to 0 because at the point both equal 0, our sum of squared residuals is no longer rising or falling (or, the critical point on the surface I plotted above). So, let’s write them out as such (I divided the first equation by 6 since all of its terms were divisible by 6). First, we add the constants to both RHS of both partial derivatives: <span class="math display">\[
\begin{aligned}
&amp;\hat{\beta}_0 + \hat{\beta}_1 = 35 \\
&amp;6\hat{\beta}_0 + 10\hat{\beta}_1 = 230.
\end{aligned}
\]</span></p>
<p>Here I use <a href="https://www.youtube.com/watch?v=V7H1oUHXPkg">a method called substitution</a> to solve the system, but there are <a href="https://youtu.be/vA-55wZtLeE?si=hUaVyHnDGXrAv4Ya">many</a> such ways we can solve this. I solve the first parital first since it is by far the easiest. The first step of substitution is to solve for one of our variables. <span class="math display">\[
\begin{aligned}
&amp;\hat{\beta}_0 + \hat{\beta}_1 = 35 \Rightarrow \\
&amp;\hat{\beta}_0 + (\hat{\beta}_1-\hat{\beta}_1) = 35-\hat{\beta}_1 \Rightarrow \\
&amp;\hat{\beta}_0 = 35 - \hat{\beta}_1
\end{aligned}
\]</span></p>
<p>Okay, so this is our expression for <span class="math inline">\(\beta_0\)</span>. Since we know the expression for the constant (the entry fee), we can plug it into the partial for <span class="math inline">\(\hat{\beta}_1\)</span> where the <span class="math inline">\(\beta_0\)</span> currently is and solve for <span class="math inline">\(\hat{\beta}_1\)</span>: <span class="math display">\[
\begin{aligned}
&amp;6\hat{\beta}_0 + 10\hat{\beta}_1 = 230. \\
&amp;6(35 - \hat{\beta}_1) + 10\hat{\beta}_1 = 230.
\end{aligned}
\]</span> Next, we distribute the 6 <span class="math display">\[
210 - 6\hat{\beta}_1 + 10\hat{\beta}_1 = 230
\]</span> and combine the terms <span class="math inline">\(- 6\hat{\beta}_1 + 10\hat{\beta}_1\)</span> together <span class="math display">\[
210 + 4\hat{\beta}_1 = 230.
\]</span> Next, we subtract 210 <span class="math display">\[
4\hat{\beta}_1 = 20.
\]</span> Finally, we divide by 4 <span class="math display">\[
\boxed{\hat{\beta}_1 = 5}.
\]</span></p>
<p>Now, we know our value for <span class="math inline">\(\hat{\beta}_1\)</span>!!! We know that for each drink we get, we pay 5 more dollars. Since we now know <em>this</em>, we substitute 5 into <span class="math inline">\(\hat{\beta}_0 + \hat{\beta}_1 = 35\)</span> where <span class="math inline">\(\hat{\beta}_1\)</span> is. Then, we have one equation to solve for, with our goal being to get the value of <span class="math inline">\(\hat{\beta}_0\)</span>. <span class="math display">\[
\hat{\beta}_0 + 5 = 35.
\]</span></p>
<p>Now, we simply subtract 5 from the RHS <span class="math display">\[
\boxed{\hat{\beta}_0 = 30}.
\]</span></p>
<p>The entry fee is 30 dollars.</p>
</section>
<section id="our-ols-line-of-best-fit" class="level3" data-number="6.3.7">
<h3 data-number="6.3.7" class="anchored" data-anchor-id="our-ols-line-of-best-fit"><span class="header-section-number">6.3.7</span> Our OLS Line of Best Fit</h3>
<p>So, our line of best fit is <span class="math inline">\(\hat{y} = 30 + 5x\)</span>. In social science, you’ll hear people throw around terms like “when we controlled for <em>this</em>” or “<em>adjusted for</em>” another variable, or “when we <em>hold constant</em> these set of variables”. This is what they mean by it! They, in the plainest language possible, mean that the dependent variable changes by <em>that amount</em> for every unitary increase in an independent variable, assuming the other values of the function do not change change.That’s what the partial derivative is, the change in a function given a change in one variable for that function. So here, assuming the club has a flat entry fee that does not change on a person to person basis, the amount the function changes by for every new drink is an increase of 5 dollars. Or, <em>compared</em> to the scenario where you only wanted to get in the club (and not drink at all, where <span class="math inline">\(x=0\)</span>), you spend 5 more dollars per each new drink you get. One may ask why we did this at all. Why <em>bother</em> with the partial derivative approach and the messy system of equations, why not simply display a regression table and go through the practical interpretation? After all, assuming we just did the following in Stata: <code>reg y x</code>, we would’ve gotten the exact same set of results that I just did quicker.</p>
<p>The primary reason is pedagogical. OLS was never derived for me in quite this manner in undergrad. So I believe you should see it done with a simple, tractable, familiar example, even though you’ll never do this for any work you ever do. This way, OLS is not a computerized black box you mindlessly use for a dataset- you actually can <em>see</em> where the numbers come from in a simplified way.</p>
<div class="cell" data-engine="jupyter" data-execution_count="8">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="inference-for-ols" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="inference-for-ols"><span class="header-section-number">6.4</span> Inference For OLS</h2>
<p>Now that we’ve conducted estimation, we can now conduct inference with these statistics we’ve generated. Indeed, this is the primary point of this at all, in a sense. We <em>want</em> to know if these estimates are different from some null hypothesis. To begin, recall the notation of <span class="math inline">\(\hat{\epsilon}_i\)</span> which denotes our residuals for the regression predictions. We can use this to generate the standard error/the uncertainty statistic associated with the respective regression estimate. We can begin with the residual sum of squares, calcualted like <span class="math inline">\(\text{RSS} = \sum (\hat{\epsilon}_{i})^2\)</span>. Put another way, it is all the variation <em>not</em> explained by our model. If <span class="math inline">\(RSS=0\)</span>, as was the case in the above example, then we have no need for inference since there’s nothing our model does <em>not</em> explain. We then can estimate the variance of the error like <span class="math inline">\(\hat{\sigma}^2 = \frac{\text{RSS}}{n - p}\)</span>, where <span class="math inline">\(n\)</span> is our number of observations and <span class="math inline">\(p\)</span> is our number of predictors (including the constant). We divide by <span class="math inline">\(n-p\)</span> because this takes into account our model’s residual degrees of freedom, or our model’s freedom to vary. Note as well that when <span class="math inline">\(n=p\)</span>, the error variance is not defined, meaning for OLS to be valid we need less predictors than observations. For a more detailed explanation of degrees of freedom, see <span class="citation" data-cites="dof">Pandey and Bright (<a href="#ref-dof" role="doc-biblioref">2008</a>)</span>.</p>
<p>For an example of how a regression table is presented, consider the above example that estimates the impact of tobacco prices on consumption across states</p>
<table class="table">
<colgroup>
<col style="width: 36%">
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 18%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Estimate</th>
<th>Std. Error</th>
<th>T-statistic</th>
<th><span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Intercept</td>
<td>323</td>
<td>56.3691</td>
<td>-3.31</td>
<td></td>
</tr>
<tr class="even">
<td>Coefficient (retprice)</td>
<td>-3.17</td>
<td>.958191</td>
<td>-3.028</td>
<td>0.22</td>
</tr>
</tbody>
</table>
<p>As we know from above, this suggests that a one dollar increase in the price of cigarettes implies a reduction of 3 in the rate of tobacco sales per capita. As we’ve discussed with normal descriptive statistics/classic hypothesis testing, we can also compute confidence intervals for these regression estimates. To do this, we need a standard error for our regression estimates. We compute:</p>
<p><span class="math display">\[
\frac{\frac{\text{RSS}}{n-(k+1)}}{\sum(x_i- \bar x)^2}
\]</span> We already know RSS from above. Then, we add the differences of each point for <span class="math inline">\(x\)</span> and its mean. This is:</p>
<p><span class="math display">\[
\frac{\frac{26015.2655}{37}}{765.81}=0.958191.
\]</span></p>
<p>Now that we have this, we can calculate the t-statistic for the beta, which is simply the coefficient divided by the standard error. We can also calculate confidence intervals for coefficients too. The formula for this should appear quite familiar <span class="math display">\[
\beta_j \pm t_{\alpha/2, \text{df}} \cdot \text{SE}(\beta_j)
\]</span></p>
<p>Here, <span class="math inline">\(\beta_j\)</span> is the coefficient of our model, <span class="math inline">\(t\)</span> is our test statistic (1.96 ususally), <span class="math inline">\(\alpha\)</span> is our acceptance region (0.05 in most cases if we want a 95% confidence interval), SE is our standard error as we’ve computed it above. For the price example, we do <span class="math inline">\(-3.171357+(1.96 \times .958191)\)</span> and <span class="math inline">\(-3.171357-(1.96 \times .958191)\)</span>, returning a 95% CI of <span class="math inline">\([-5.112836, -1.229877]\)</span>. There’s a little rounding error, but that’s what we get. The way to interpret the CI is as follows: given our sample size and input data, the true parameter of the effect of price on tobacco smoking rates lies within the range of -5.1 and -1.2. In other words, if some assumptions hold (which we will discuss below), a dollar increase in price may decrease the tobacco smoking rate by as much as 5 or as little as 1.</p>
<p>Just as we discussed in the preceding chapters, lots of statistics is justified asymptotically, based on the law of large numbers and CLT. In other words, as <span class="math inline">\(n\)</span> tends to infinity, <span class="math inline">\(\lim\limits_{n\to \infty}\)</span>, our betas will converege to the true population value and the standard errors will shrink. Ergo, as these shrink, the confidence intervals will tighten, meaning our estimates will be more precise. A practical consequence of this is that as a very general rule, having more observations in your dataset makes your OLS estimates more precise and less biased. For the above for example, we would not trust these estimates as much because they come from one year only. Ideally, to get a better sense of how price increases affect consumption, we’d need to collect these observations over time and adjust for other things that may affect cigarette consumption.</p>
<section id="goodness-of-fit-measures-for-ols" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="goodness-of-fit-measures-for-ols"><span class="header-section-number">6.4.1</span> Goodness of Fit Measures for OLS</h3>
<p>We typically thhink of two goodness of fit statistics when using OLS, the R-squared statistics and the Root Mean Squared Error</p>
<p><span class="math display">\[
\begin{aligned}
R^2 &amp;= 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}} &amp; \text{RMSE} &amp;= \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}.
\end{aligned}
\]</span> Here, for R-squared, we have two terms: first, <span class="math inline">\(\text{SS}_{\text{res}} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)</span> is the sum of squared residuals. <span class="math inline">\(\text{SS}_{\text{res}}\)</span> quantifies the amount of variance unexplained by the independent variables in the model, and <span class="math inline">\(\text{SS}_{\text{tot}}\)</span> is just the total amount of variance of <span class="math inline">\(y\)</span>. Think of R-squared as a ratio/percentage of how well the model explains our outcomes. An R-squared of 1 reflects the simple example we derived above, where the model perfectly explains the variation of our outcomes. R-squared usually scales with the amount of predictors in the model (that is, as we add more variables, the R-squared will increase). However, I think the best way to think about R-squared is a measure of how well our model does, compared to the average of our outcomes. In the nightclub example, if we just took the average of our outcomes, we’d guess for that evening, you’d spend 28.3 dollars. But, in this case, the model significantly outperforms this since it explains the variation perfectly. Note, that it is possible to have a negative R-squared. It is very rare, and it basically means that your model does a worse job than the simple average of the outcomes. I’ve seen this in my work, but it is very rare. I’ve only encountered it in the wild maybe twice. In the above example, including just price as a predictor explains about 22 percent of the variation, which is not bad considering it’s only one variable!</p>
<p>The Root Mean Squared error, or RMSE, is exactly as it sounds: it is the square root of our average of our <span class="math inline">\(\text{SS}_{\text{res}}\)</span>. For the tobacco, example, our RMSE is <span class="math inline">\(\frac{26015.2655}{3726.516}=26.516321\)</span>. In English, this simply means that when we use price to explain consumption for the year 1980, the model is off, on average, by about 27 packs. Again, considering that the average cigarette consumption in 1980 was 137,being off by 26 packs isn’t so bad. It suggests, as one would expect, that we’ve explained our dataset fairly well using price as an explanatory variable. As RMSE approaches 0, we explain our variation better, with an RMSE of 0 being perfection. Note that other goodness of fit metrics do exist; however, these are the most common ones you’ll encounter.</p>
</section>
</section>
<section id="assumptions-of-ols" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="assumptions-of-ols"><span class="header-section-number">6.5</span> Assumptions of OLS</h2>
<p>Keep in mind, despite all the detail we’ve discussed so far, do not lose sight of the larger picture: OLS is simply a mathematical estimation method. Its <em>validity</em> for explaining the external world (aside from having quality data) relies on a few assumptions (collectively called the Gauss-Markov assumptions) being defensible. I say defensible instead of true because practically they are never true. After all, this is statistics: almost all of statistics is true. All statistical research (pretty much, outside of simulations) is at least partly wrong because we live in a probalistic world where we don’t have all the answers. In other words, the assumptions are only as valid as we can defend for them. Below, I detail them.</p>
<section id="assumption-1-linear-in-parameters" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="assumption-1-linear-in-parameters"><span class="header-section-number">6.5.1</span> Assumption 1: Linear in Parameters</h3>
<p>The very first assumption is that the parameters for our model are linear. The classic regression model for OLS is</p>
<p><span class="math display">\[
y_{i} = \beta_{0} + \beta_{1} x_{i1} + \cdots + \beta_{K} x_{iK} + \epsilon_{i}.
\]</span></p>
<p>We call this a linear model. Why? How do we know if it’s a linear relationship, and what might violations of this look like? Let’s say we’re buying weed. Say the price per quarter ounce is 80 dollars. The impact of <span class="math inline">\(\beta_{1}\)</span> is the same everywhere in the function, <span class="math inline">\(y=80x\)</span>. But step back and ask ourselves, from the seller’s persepctive, if this makes sense: does it make sense for weed to cost the same for every weight amount? No! Why not? Well, for one, let’s say you’re selling a full gram or pound of weed. That’s <strong><em>so much</em></strong> weed that weed(wo)men/people will charge much much more for lone individuals who wish to buy this much. So while it may be 80 for a quarter ounce, it’ll now be, say, 900 per pound. In fact, we could express this as a piecewise function</p>
<p><span class="math display">\[
\beta_{j} =
\begin{cases}
    80 \text{ if } x &lt; 1 \\
    900 \text{ if } x &gt; 1. \\
\end{cases}
\]</span></p>
<p>Why might this be done? Firstly, that’s so much more product than the average person could smoke or use. So, anyone interested in this would need to pay premium prices for such an outlandish amount. Also, it allows the dealer to get the pound of weed off their hands– relative to ounces, pounds of weed are much more likely to be noticed by police and therefore punished by the law harsher. So, the quicker they sell, the quicker they may re-up. So, for the normal crowd of people who do not but pounds, they pay one price. For those who are abnormal in how much they demand (say, the distributor for the connect for cocaine markets), they pay another price altogether. We see price discrimination in legal markets too, such as Sams Club. We can see that a regression model here IS NOT linear in parameters, since the slope of the line will change at different values of the function.</p>
<p>People often confuse this assumption with non-linear values of our independent variables as they relate to our outcome. They conflate nonlinear regression <span class="math display">\[
y_{i} = \beta_{0}^{2} + \beta_{1}^{2} x_{i1} + \cdots + \beta_{K}^{2} x_{iK} + \epsilon_{i}
\]</span> with <span class="math display">\[
y_{i} = \beta_{0} + \beta_{1} x_{i1}^{2} + \cdots + \beta_{K} x_{iK}^{2} + \epsilon_{i},
\]</span> or an OLS model with non-linear relations between the inputs and the outcomes. Let me explain why this is wrong, because as it turns out, we can indeed model curvature. I’ve already given an example of when we’d have a nonlinear realtionship in terms of our betas. Now I discuss non-linearities in terms of our predictors. Let’s say we wish to model how much someone can dead lift given some input of age. Let’s say the true population parameter for the OLS model is 6 (we ignore the constant for exposition purposes) <span class="math display">\[
y_{i} = 6x_{i}
\]</span> What is our value for 0? 0, since you’re not yet born. For age 10? 60. For age 30? 180. For age 80? 480. I think we can already see why this relationship being modeled would be silly: it presumes that the older you get, the stronger one is as a hard and fast rule. Which, generally speaking, is true… but we also know that at some point, as with all things, glory fades. Someone that was once strong and in shape will not (in general) always be that way because the body declines with the passage of time. How do we take this into account for our regression model, though?</p>
<p><span class="math display">\[
y_{i} = \beta_{1}(x_{i1} \times x_{i}) +  x_{i} \equiv y_{i} = \beta_{1}x^{2}_{i}
\]</span> We simply square the original value of age, keeping its linear form in the regression model. That way, when age 4 is input in the model, the number our regression model reads in the computer is 16. When age 10 is put into the model, it reads 100. Of course, as one would expect, there’s likely some critical point for this function, where people begin to be able to lift less given some values of age. We never know this of course, but OLS can be used to estimate it in the manner that we’ve done.</p>
<p>Another example of being able to account for non-linearities from economics is the idea of modeling how much produce one may produce given a set of labor inputs. Suppose we’re cooking cocaine. With just two people, you can get work done, but it won’t be a lot. With three people, you can do more, and more with each additional person. However, there’s an idea in economics called diminishing marginal returns for the factors of production (in this case labor). You may be able to cook a lot with 10 or 20 people, but when you have 40 or 50 people, at some point we end up producing less because there’s too many proverbial cooks in the kitchen. So, if we wished to model output of cocaine as a function of labor, we’d likely wish to square the “number of workers” part of our model since it does not make sense to expect production to increase perfectly linearly with every new human working with you. So you see, the linear in parameters assumption deals with our betas impact on our predictor variables, not the input values of our predictor variables.</p>
<p>Note that when we include such terms in our model, called <em>interaction terms</em> in econometrics (I may cover this more later), we must include the linear term in the model as well. In Stata, this would look something like <code>reg y c.labor##c.labor</code>. Under the hood, this includes in the model <code>reg y labor labor2</code>.</p>
</section>
<section id="assumption-2-random-sample" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="assumption-2-random-sample"><span class="header-section-number">6.5.2</span> Assumption 2: Random Sample</h3>
<p>We next presume that we’ve collected a random sample of our population. The name <em>random</em> sample is something of an antiquated, romantic name to denote the idea that the sample we’ve collected is representative of the population we wish to map on to. Suppose we wish to investigate the relationship between introducing all virtual menus at a restaurant (the kind you scan on your phone) to see if it increases how much money they make for [<em>random marketing reasons</em>]. We take all the restaurants in Buckhead and Sandy Springs in Atlanta as a sample, comparing the ones that did this intervention to the ones that didn’t do the intervention. We get a coefficient of 10,000, suggesting that this intervention increased money made, on average, by 10,000 dollars compared to the places that didn’t do this. The issue with this idea is that our sample is not a random sample of the population. Sandy Springs and Buckhead, in particular, are among the wealthiest areas in Atlanta. We can’t generalize the effet of this intervention to the population (restuarants in Atlanta, say) becuase our units of intertested are decidedly <em>not</em> representative of Atlanta’s entire culinary scence. They have very specific customers that make a generalization to the bigger population a bad idea.</p>
<p>Another example can come from sports. Say we posit a relationship between height and skill at basketball. We take a sample of NBA players, run a few regressions for relevant metrics and have our software spit out coefficients at us. Can we generalize to the population? No!! The NBA is one of the most selective sports leagues on the planet. The NBA selects for height and skill, among other things. The worst player on the NBA bench is like a god from Olympus compared to the average human, physically and in terms of skill. They are <strong>not</strong> representative of even a human 2 standard deviations above the mean.</p>
<p>So, we cannot use NBA players generalize to the population, <em>unless</em> of course we are concerned only with NBA players. The same would apply to the first example: if we care only about the high-income restaurants in the city, then that’s great, but assuming we wish to generalize more broadly, we will need more data from other, more diverse units that have more information encoded in their outcome about the sample.</p>
</section>
<section id="assumption-3-no-perfect-collinearity" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="assumption-3-no-perfect-collinearity"><span class="header-section-number">6.5.3</span> Assumption 3: No Perfect Collinearity</h3>
<p>The simple way to think about this one is we cannot include redundant variables in our regressions. Suppose we wish to predict the ticket sales of NBA teams. In our regression, we include the number of games won as well as the number of games lost (2 variables). Well, these are mirror images of each other. The number of games you won is a direct function of the total games minus the number you lost, and the number you lost is a direct and perfect function of the total minus the number you won.</p>
<p>By extension, suppose we wish to compare women to men (say we wish to test that men earn more/less than women on average). We take data on 500 respondents who we’ve sampled randomly across a company. We have one column that denotes the respondent as male and the other as female. We cannot include both male and female columns in our models, these are perfect linear functions of one another. A female is necessarily not coded as male, and male is necessarily not coded as female. Practically, this means we must choose when we use a categorical variable in our models. Say our regression includes age ang gender as a predictor. If category 1 of gender is female and category 0 is male, then if the beta for “gender” is -30, we would interpret the beta for gender as “holding constant age/compared to men of the same age group, female respondents earn about 30 dollars less than men.” By extension, the coefficient for male (if we decided to include this group as the group of interest) would just be 30, with a similar interpretation in the other direction.</p>
</section>
<section id="assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0" class="level3" data-number="6.5.4">
<h3 data-number="6.5.4" class="anchored" data-anchor-id="assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0"><span class="header-section-number">6.5.4</span> Assumption 4: Strict Exogeneity: <span class="math inline">\(\mathbb{E}[\epsilon_{i} | x_{i1}, \ldots, x_{iK}] = 0\)</span></h3>
<p>Next we presume strict exogeneity. Formally, this means the average of our errors, given the set of covariates we’ve controlled for, is 0. It means our predictor variables may not be correlated with the error term. Note that the error term is different from the residuals: the error term includes unobserved characteristics that also may affect the outcome. In other words, we cannot omit important variables from our model.</p>
<p>For example, say we wish to see how the number of firefighters sent to a call affects the amount of damage from that fire. We wish to measure the association in other words. We conclude that there’s a positive relationship between number of firefighters sent and damage. Say, we use the number of trucks sent to a call to predict the damage in dollars for a sample size of 10,000,000 calls. We find from the bivariate model that every truck you send increases damage by 30,000 dollars. So, we elect to send <em>less</em> people to future calls. Is this a good idea? No!!!! People will die like that.</p>
<p>Presumably, the firefighters are not pouring gasoline on the fire, so perhaps we’ve <em>omitted</em> things from our model that might influence <em>both</em> how many people we send as well as fire damage. What else should we control for? Maybe, building size, building type, neighborhood income status, local temperature, and other relevant predictors to ensure that we are not blaming the outcomes on a spurious relationship. Indeed, on some level we would expect for the size of the fire to be correlated with the number of people sent to fight it. Thus, when we do not control for other relevant factors, our coefficients, no matter how precise, suffer from <em>omitted variable bias</em>. Strict exogeneity is pretty much <strong>never</strong> met in real life, but it basically posits that there’s no other critical variable missing from our regression model that may explain our outcome. This is also why it matters to critically think about the variables one will use in their regression model <em>before</em> they run regressions.</p>
</section>
</section>
<section id="summary" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Summary</h1>
<p>This undoubtably is the most weighty chapter, both in terms of mathematics and in terms of practical understanding. Regression is one of the building blocks for policy analysis, in addition to solid theoretical background and contextual knowledge of the policy being studied. The reason I chose to cover this first, in the first few weeks of the class instead of waiting until the end, is because I believe that the only way to <em>truly</em> understand regression is by use in applied examples. This is what you’ll wrestle with in your papers.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-dof" class="csl-entry" role="doc-biblioentry">
Pandey, Shanta, and Charlotte Lyn Bright. 2008. <span>“<span class="nocase">What Are Degrees of Freedom?</span>”</span> <em>Social Work Research</em> 32 (2): 119–28. <a href="https://doi.org/10.1093/swr/32.2.119">https://doi.org/10.1093/swr/32.2.119</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./correlation.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Correlation and Association</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./treatmenteffects.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Causal Inference</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>