<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Econometrics for Policy Analysis - 7&nbsp; OLS Explained</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./treatmenteffects.html" rel="next">
<link href="./correlation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">OLS Explained</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Econometrics for Policy Analysis</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Syllabus: PMAP 4041, Fall 2024</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Policy Studies</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Mathematics and Econometric Theory</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basicprob.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Basic Probability Theory</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stataintro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introduction to Stata</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotic.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Asymptotic Theory</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./correlation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Correlation and Association</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">OLS Explained</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Applied Research Methods</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./treatmenteffects.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Causal Inference</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dd.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Difference-in-Differences</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./present.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Presenting Results</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#data-structures" id="toc-data-structures" class="nav-link active" data-scroll-target="#data-structures"><span class="toc-section-number">7.1</span>  Data Structures</a>
  <ul class="collapse">
  <li><a href="#a-primer-on-data-types" id="toc-a-primer-on-data-types" class="nav-link" data-scroll-target="#a-primer-on-data-types"><span class="toc-section-number">7.1.1</span>  A Primer on Data Types</a></li>
  <li><a href="#a-primer-on-variable-types" id="toc-a-primer-on-variable-types" class="nav-link" data-scroll-target="#a-primer-on-variable-types"><span class="toc-section-number">7.1.2</span>  A Primer on Variable Types</a></li>
  <li><a href="#review-of-lines-and-functions" id="toc-review-of-lines-and-functions" class="nav-link" data-scroll-target="#review-of-lines-and-functions"><span class="toc-section-number">7.1.3</span>  Review of Lines and Functions</a></li>
  </ul></li>
  <li><a href="#arrivederci-algebra-ciao-derivatives." id="toc-arrivederci-algebra-ciao-derivatives." class="nav-link" data-scroll-target="#arrivederci-algebra-ciao-derivatives."><span class="toc-section-number">7.2</span>  Arrivederci, Algebra, Ciao Derivatives.</a>
  <ul class="collapse">
  <li><a href="#power-rule" id="toc-power-rule" class="nav-link" data-scroll-target="#power-rule"><span class="toc-section-number">7.2.1</span>  Power Rule</a></li>
  <li><a href="#chain-rule" id="toc-chain-rule" class="nav-link" data-scroll-target="#chain-rule"><span class="toc-section-number">7.2.2</span>  Chain Rule</a></li>
  </ul></li>
  <li><a href="#an-extended-example" id="toc-an-extended-example" class="nav-link" data-scroll-target="#an-extended-example"><span class="toc-section-number">7.3</span>  An Extended Example</a>
  <ul class="collapse">
  <li><a href="#list-the-data" id="toc-list-the-data" class="nav-link" data-scroll-target="#list-the-data"><span class="toc-section-number">7.3.1</span>  List the Data</a></li>
  <li><a href="#define-our-econometric-model" id="toc-define-our-econometric-model" class="nav-link" data-scroll-target="#define-our-econometric-model"><span class="toc-section-number">7.3.2</span>  Define Our Econometric Model</a></li>
  <li><a href="#write-out-the-objective-function" id="toc-write-out-the-objective-function" class="nav-link" data-scroll-target="#write-out-the-objective-function"><span class="toc-section-number">7.3.3</span>  Write Out the Objective Function</a></li>
  <li><a href="#substitute-into-the-objective-function" id="toc-substitute-into-the-objective-function" class="nav-link" data-scroll-target="#substitute-into-the-objective-function"><span class="toc-section-number">7.3.4</span>  Substitute Into the Objective Function</a></li>
  <li><a href="#take-partial-derivatives" id="toc-take-partial-derivatives" class="nav-link" data-scroll-target="#take-partial-derivatives"><span class="toc-section-number">7.3.5</span>  Take Partial Derivatives</a></li>
  <li><a href="#get-the-betas" id="toc-get-the-betas" class="nav-link" data-scroll-target="#get-the-betas"><span class="toc-section-number">7.3.6</span>  Get the Betas</a></li>
  <li><a href="#our-ols-line-of-best-fit" id="toc-our-ols-line-of-best-fit" class="nav-link" data-scroll-target="#our-ols-line-of-best-fit"><span class="toc-section-number">7.3.7</span>  Our OLS Line of Best Fit</a></li>
  </ul></li>
  <li><a href="#inference-for-ols" id="toc-inference-for-ols" class="nav-link" data-scroll-target="#inference-for-ols"><span class="toc-section-number">7.4</span>  Inference For OLS</a>
  <ul class="collapse">
  <li><a href="#goodness-of-fit-measures-for-ols" id="toc-goodness-of-fit-measures-for-ols" class="nav-link" data-scroll-target="#goodness-of-fit-measures-for-ols"><span class="toc-section-number">7.4.1</span>  Goodness of Fit Measures for OLS</a></li>
  </ul></li>
  <li><a href="#assumptions-of-ols" id="toc-assumptions-of-ols" class="nav-link" data-scroll-target="#assumptions-of-ols"><span class="toc-section-number">7.5</span>  Assumptions of OLS</a>
  <ul class="collapse">
  <li><a href="#assumption-1-linear-in-parameters" id="toc-assumption-1-linear-in-parameters" class="nav-link" data-scroll-target="#assumption-1-linear-in-parameters"><span class="toc-section-number">7.5.1</span>  Assumption 1: Linear in Parameters</a></li>
  <li><a href="#assumption-2-random-sample" id="toc-assumption-2-random-sample" class="nav-link" data-scroll-target="#assumption-2-random-sample"><span class="toc-section-number">7.5.2</span>  Assumption 2: Random Sample</a></li>
  <li><a href="#assumption-3-no-perfect-collinearity" id="toc-assumption-3-no-perfect-collinearity" class="nav-link" data-scroll-target="#assumption-3-no-perfect-collinearity"><span class="toc-section-number">7.5.3</span>  Assumption 3: No Perfect Collinearity</a></li>
  <li><a href="#assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0" id="toc-assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0" class="nav-link" data-scroll-target="#assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0"><span class="toc-section-number">7.5.4</span>  Assumption 4: Strict Exogeneity: <span class="math inline">\(\mathbb{E}[\epsilon_{i} | x_{i1}, \ldots, x_{iK}] = 0\)</span></a></li>
  </ul></li>
  <li><a href="#interpreting-ols" id="toc-interpreting-ols" class="nav-link" data-scroll-target="#interpreting-ols"><span class="toc-section-number">7.6</span>  Interpreting OLS</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="toc-section-number">8</span>  Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">OLS Explained</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>This is the chapter on regression. We begin by covering data types. Then, we review the idea of a function and how it relates to a line. After a review of derivatives, we finally cover the computation of regression coefficients, inference, fit, and OLS assumptions.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is a statistics course, not a math course. However, math is the language which underlies statistics. This is the <em>only</em> part of the course where we use any calculus to explain ideas. Furthermore, you’ll never be expected (from me) to use calculus for any of your assignments.</p>
<p><em>With this said</em>, I do use the calculus to explain what is going on in regression. The calculus explains the <em>why</em>, which I think is always important to know if you’re implementing or interpreting regression. I presume however that you are like myself when I was in undergrad, in that you’ve either never taken calculus (me) or had little exposure to it. So, I link to tutorials on the calculus concepts that we employ here. Consult these links, if you’d like (they sure helped me in preparation of this chapter). However, they are completely optional. I try to explain everything as best I can, so they are provided as a supplement to the curious who want a better understanding.</p>
</div>
</div>
<section id="data-structures" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="data-structures"><span class="header-section-number">7.1</span> Data Structures</h2>
<section id="a-primer-on-data-types" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="a-primer-on-data-types"><span class="header-section-number">7.1.1</span> A Primer on Data Types</h3>
<p>Up until this point, we have presumed that we are working with only one collection of units at one time point. This is called a cross-sectional dataset, where we have \(N&gt;1\) units and \(T=1\) time periods.</p>
<table class="table">
<thead>
<tr class="header">
<th>Country</th>
<th>Year</th>
<th>Units</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mexico</td>
<td>2000</td>
<td>1</td>
<td>50</td>
</tr>
<tr class="even">
<td>Guatemala</td>
<td>2000</td>
<td>1</td>
<td>45</td>
</tr>
</tbody>
</table>
<p>The opposite of this is called a time-series dataset, where we have <span class="math inline">\(N=1\)</span> units and <span class="math inline">\(T&gt;1\)</span> time periods.</p>
<table class="table">
<thead>
<tr class="header">
<th>Country</th>
<th>Year</th>
<th>Units</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mexico</td>
<td>2000</td>
<td>1</td>
<td>50</td>
</tr>
<tr class="even">
<td>Mexico</td>
<td>2001</td>
<td>1</td>
<td>55</td>
</tr>
</tbody>
</table>
<p>A panel dataset is where <span class="math inline">\(N&gt;1\)</span> and <span class="math inline">\(T&gt;1\)</span>.</p>
<table class="table">
<thead>
<tr class="header">
<th>Country</th>
<th>Year</th>
<th>Units</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mexico</td>
<td>2000</td>
<td>1</td>
<td>50</td>
</tr>
<tr class="even">
<td>Mexico</td>
<td>2001</td>
<td>1</td>
<td>55</td>
</tr>
<tr class="odd">
<td>Guatemala</td>
<td>2000</td>
<td>1</td>
<td>45</td>
</tr>
<tr class="even">
<td>Guatemala</td>
<td>2001</td>
<td>1</td>
<td>50</td>
</tr>
</tbody>
</table>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Because I think it’s valuable to work with panel data early (i.e., comparing the same units over more than one time point), all of the coruse’s examples rely on panel data structures. However, this but scratches the surface. There are entire course sequences on panel data and time-series data.</p>
</div>
</div>
</section>
<section id="a-primer-on-variable-types" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="a-primer-on-variable-types"><span class="header-section-number">7.1.2</span> A Primer on Variable Types</h3>
<p>For any dataset you ever work with, you’ll likely have different variables (columns). Regression is no exception. The predictors for regression must be numeric, naturally. These take a few different types. Note that if it is not numeric, we call it a “<em>string</em>”.</p>
<p>The most common kind is a ratio variable (a value we may express as a fraction/continuous variable), such as the employment rate.</p>
<table class="table">
<thead>
<tr class="header">
<th>State</th>
<th>Year</th>
<th>Employment Rate (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alabama</td>
<td>1990</td>
<td>55.3</td>
</tr>
<tr class="even">
<td>Alabama</td>
<td>1991</td>
<td>56.1</td>
</tr>
<tr class="odd">
<td>California</td>
<td>1990</td>
<td>62.1</td>
</tr>
<tr class="even">
<td>California</td>
<td>1991</td>
<td>61.5</td>
</tr>
<tr class="odd">
<td>Georgia</td>
<td>1990</td>
<td>58.4</td>
</tr>
<tr class="even">
<td>Georgia</td>
<td>1991</td>
<td>59.2</td>
</tr>
</tbody>
</table>
<p>A dummy variable is a binary variable that indicates the presence or absence of a characteristic. A dummy variable (also called an <em>indicator</em> or <em>categorical</em> variable) is a variable that takes on the values 0 or 1. For example, a simple dummy indicates whether a respondent in a survey is a male or a female. In this case, the number 1 “maps” on to the value for male, and 0 for female. Note that in this case, the simple average of these respective columns returns the proportion of our observations that are male or female.</p>
<table class="table">
<thead>
<tr class="header">
<th>Respondent ID</th>
<th>Gender (Male=1, Female=0)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>2</td>
<td>0</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1</td>
</tr>
<tr class="even">
<td>4</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Dummies can also be used to capture unobserved variation across groups. For instance, when predicting homicide rates across states like Alabama, California, and Georgia for 1990 and 1991, we can include dummy variables for each state. These dummies help account for unique, stable characteristics of each state, such as culture, that are hard to measure directly. In other words, if we think something “makes” Alabama, Alabama, compared to California or Georgia, we can include these kinds of variables to capture that unobserved variation. When including dummies in regression, we must always omit one category from the regression (for reasons we will explain below). So, for example, we could include Alabama/Georgia dummies or Georgia/California dummies, where California and Alabama would be what we call the <em>reference group</em>.</p>
<table class="table">
<thead>
<tr class="header">
<th>State</th>
<th>Year</th>
<th>Alabama (1/0)</th>
<th>California (1/0)</th>
<th>Georgia (1/0)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alabama</td>
<td>1990</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>Alabama</td>
<td>1991</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>California</td>
<td>1990</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>California</td>
<td>1991</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>Georgia</td>
<td>1990</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>Georgia</td>
<td>1991</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>There is also a notion of an <em>ordinal</em> variable, where the data at hand must obey a specific order. Suppose we ask people in a survey how religious they are on a scale from 1 to 10, where 1=Atheist and 10=Extremely Religious. Here, order matters, because 1 has a very different meaning from 10 in this instance. An ordinal variable has a clear, ordered ranking between its values.</p>
<table class="table">
<thead>
<tr class="header">
<th>Respondent ID</th>
<th>Religiosity (1-10)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>3</td>
</tr>
<tr class="even">
<td>2</td>
<td>7</td>
</tr>
<tr class="odd">
<td>3</td>
<td>5</td>
</tr>
<tr class="even">
<td>4</td>
<td>10</td>
</tr>
</tbody>
</table>
<p>These are the data types you will generally work with. When we move on to real datasets, their meaning will become much more apparent.</p>
</section>
<section id="review-of-lines-and-functions" class="level3" data-number="7.1.3">
<h3 data-number="7.1.3" class="anchored" data-anchor-id="review-of-lines-and-functions"><span class="header-section-number">7.1.3</span> Review of Lines and Functions</h3>
<p>In middle school, we learn about <a href="https://www.youtube.com/watch?v=VhokQhjl5t0">the basics of functions</a> in that when we plug in a number, we get another number in return. If you’re at the grocery store and grapes are 1 dollar and 50 cents per pound, we just weigh the grapes and multiply that number by 1.5. This could take the form of <span class="math inline">\((0,0), (1,1.5), (2,3)\)</span>, and so on. In fact, we can represent these data points in a table like this</p>
<table class="table">
<thead>
<tr class="header">
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.5</td>
</tr>
<tr class="odd">
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>These points form a line, <a href="https://www.youtube.com/watch?v=K_OI9LA54AA">the equation for which being</a> <span class="math inline">\(y=mx+b\)</span>. We can also think of this line as a function. It returns a value of <span class="math inline">\(y\)</span> given some values for previous expenditures and pounds of grapes. Here, <span class="math inline">\(y\)</span> is how much we pay in total, <span class="math inline">\(m\)</span> is the rate of change in how much we pay for every 1 pound of grapes bought, and <span class="math inline">\(b\)</span> is our value we pay if we get no grapes.</p>
<div class="cell" data-engine="jupyter" data-execution_count="1">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For this case, the function for the line is <span class="math inline">\(y=1.5x\)</span>. For here, <span class="math inline">\(b=0\)</span> because in this case, how much we pay is a function of pounds of grapes only. We could add a constant/<span class="math inline">\(b\)</span>, though. Suppose we’d already spent 10 dollars, and now how much we spend is a function of both some previous constant level of spending, and new amount of grapes bought. Now, our function is <span class="math inline">\(y=1.5x+10\)</span>. The way we find the <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> for a straight line is <a href="https://www.youtube.com/watch?v=MeU-KzdCBps">the “rise over run” method</a>, in this case</p>
<p><span class="math display">\[
m = \frac{y_2 - y_1}{x_2 - x_1} = \frac{3 - 0}{2 - 0} = \frac{3}{2} = 1.5
\]</span></p>
<p>Lines fit to points sometimes have discrepancies between the line and the data we see. We call this the residual, or <span class="math inline">\(\hat \epsilon = y-(mx+b)\)</span>. Also notice how the rate of change, or <span class="math inline">\(1.5x\)</span>, is the same at every point on the line: in this instance, you pay a dollar and fifty cents for <em>any</em> amount of grapes we get.</p>
<p>Regression, fundamentally, is about fitting a line (or a plane) to a set of data given some inputs. Going forward, I will use the words “outcome variable” or “dependent variable” to refer to the thing that we are studying the change of, and “predictors”, “covariates”, or “independent variables” to refer to the variables we think affect our outcome. In the grape example, our outcome is the total amount of money we spend, and the singular predictor we use is how many pounds of grapes we get. With all this being said though, this example is very simplistic. After all, the necessary information is known to us up front (price and weight). But… what if the data we have at hand are not nice and neat in terms of a function? Suppose we consider a more challenging example.</p>
<p>Take the idea of predicting <a href="https://raw.githubusercontent.com/danilofreire/homicides-sp-synth/master/data/df.csv">crime rates in Brazilian states</a> in the year 1990 using the inequality level as a predictor, or <a href="https://data.cdc.gov/Policy/Table-of-Gross-Cigarette-Tax-Revenue-Per-State-Orz/rkpp-igza/about_data">data on</a> the consumption of cigarettes in American states in the year 1980 using price as a predictor. We would presume some <em>function</em> exists that generates the crime rate for that Brazilian state in that year, or that consumption level for that American state in that year. We would also imagine, naturally, that the covairates inequality and tobacco price would affect these outcomes. But, does it make sense to expect for some deterministic function to predict these values, given some input? No.</p>
<p>The homicide rate or cigarette consumption rate in any state anywhere is not guaranteed. In some states, homicides or tobacco consumption is high, other times its low. Why? Well for homicide, some states are wealthier and some are poorer. Some states vary by racial compositions, or will differ by factors like age composition, alcohol use, and gun ownership. Thus… some cities have high homicide rates, others have low homicide rates. We can reason accordingly for cigarette consumption of American states. Naturally, one reason for this would be the price of cigarettes, as one might expect, since people tend to not want to buy more of a good as the price increases (<a href="https://sites.lsa.umich.edu/mje/2022/01/10/veblen-goods-why-sports-cars-and-diamonds-dont-obey-the-law-of-demand/">well… usually</a>.) The number of young people in that state may mean that younger people are risk takers and may be more likely to smoke than adults (or alternatively, young people may perceive smoking as something for older adults and smoke less). Levels of alcohol taxation may matter as well, since alcohol may be a substitute for tobacco, so states with higher taxation may smoke more, on average. Also, plain and simple measures like culture (and other unobservable things) may play a role. We can plot these data for illustration</p>
<div class="cell" data-engine="jupyter" data-execution_count="2">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Here, I draw a line between these input variables and the observed outcomes in question. The <span class="math inline">\(x\)</span> axis represents, respectively, inequality and price and the <span class="math inline">\(y\)</span> axes represent homicide rates and cigarette consumption. It is quite obvious though that no <em>deterministic</em> function exists here for either of these cases, as we have residuals. The line <em>imperfectly</em> describes the relationship between 1) inequality and homicide and 2) retail price of cigarettes and cigarette consumption per capita. So, we can’t find one line that fits perfectly to all of these data points. But, even if we cannot find a perfect, deterministic function that fits to all of these data points, how about we find the <em>optimal</em> line in the sense that it best <em>estimates</em> <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> by having the lowest possible values for <span class="math inline">\(\hat \epsilon\)</span> at every point on the line? We can see how this relates to the grape analogy above: the line passes through all of the observed data points, meaning it is optimal in the sense that the line has the lowest possible residual values.</p>
</section>
</section>
<section id="arrivederci-algebra-ciao-derivatives." class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="arrivederci-algebra-ciao-derivatives."><span class="header-section-number">7.2</span> Arrivederci, Algebra, Ciao Derivatives.</h2>
<p>To do this though, we’ve now reached a point in the course where simple algebra is no longer our best guide. We now <em>must</em> use calculus, specifically <a href="https://youtu.be/9vKqVkMQHKk?si=8c_Dnzf7KJbNtwHX">the basics</a> of <a href="https://www.youtube.com/watch?v=9vKqVkMQHKk">derivatives</a> and optimization. I mentioned the word <em>optimal</em> above to refer to the fit of the line, but now we’re going to get a much better sense of what is meant by this.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Okay, so here I’m <em>kind of</em> lying. You actually <strong>don’t</strong> <em>need</em> to say farewell to algebra (completely) to derive regression estimates, but <a href="https://www.youtube.com/watch?v=mIx2Oj5y9Q8&amp;t=16s">that process</a> “requires a ton of algebraic manipulation”. For those brave of heart who know algebra well, you can <em>probably</em> just watch the series of videos I just linked to and skip to <a href="https://jgreathouse9.github.io/GSUmetricspolicy/ols.html#inference-for-ols">this section</a> of the notes, but I <strong>do not</strong> recommend this at all. I find the calculus way via optimization a lot more intuitive.</p>
</div>
</div>
<p>Firstly though, <a href="https://www.youtube.com/watch?v=N2PpRnFqnqY">a primer on derivatives</a>. The derivative is the slope of a curve/line given a very small change in the value of the function. It is the instantaneous rate of change for a function. We do not need derivatives for linear functions, since we know what the rate of change is from real life (“a dollar fifty <em>per</em> pound”, “two <em>for</em> five”, etc.). For example, the derivative of <span class="math inline">\(50x=y\)</span> is just 50, since that is the value y changes by for any increase in <span class="math inline">\(x\)</span>. For a constant (say, 5), the derivative is always 0, since no matter what value <span class="math inline">\(x\)</span> takes, its value does not change at all.</p>
<p>When we set the first derivative of a function to 0 and solve, we reach an optimal point on <em>the original function</em>, <a href="https://www.youtube.com/watch?v=8aAU4r_pUUU">usually</a>. An optimal point (or “critical point”) is a place on the function where the value of the function is at the lowest or highest possible value the function can reach over a given set of inputs. We can find the critical points by solving an optimization problem. An optimization problem takes the form of <span class="math inline">\(f(x)\)</span> <span class="math display">\[
\operatorname*{arg min}_{\theta \in \Theta } f(\theta) \: \mathrm{ s.t. \:}  g(\theta) = 0, h(\theta) \leq 0,
\]</span> where there’s some function <span class="math inline">\(f(\theta)\)</span> (called the <em>objective function</em>) that is minimized over a set of <span class="math inline">\(g(\theta)\)</span> equality constraints and <span class="math inline">\(h(\theta)\)</span> inequality constraints. The word “<span class="math inline">\(\operatorname*{arg min}\)</span>” here means “argument of the minimum”. It means that we are seeking the values, or <em>arguments</em>, which minimize the output of that function. The symbols underneath this, <span class="math inline">\(\theta \in \Theta\)</span> represents the coefficients we are solving for. These are the values which will minimize the function. But this is all abstract. Let’s do some examples.</p>
<section id="power-rule" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="power-rule"><span class="header-section-number">7.2.1</span> Power Rule</h3>
<p>Suppose we shoot a basketball while we stand on a 2 foot plateau, which produces a trajectory function of <span class="math inline">\(h(t)= −5t^2 +20t+2\)</span>. Here <span class="math inline">\(h(t)\)</span> is a function representing the ball’s height over time in seconds. The <span class="math inline">\(-5t^2\)</span> reflects the downward pull of gravity. The <span class="math inline">\(20t\)</span> means that you threw the ball at 20 miles per hour originally. And the 2 means that we’re standing 2 feet above solid ground. We can find the <em>maximum</em> height of the ball by taking the derivative of the original quadratic function and solving it for 0. In this case, we use <a href="https://www.youtube.com/watch?v=BYTfCnR9Sl0">the power rule</a> for derivatives. The power rule for dertivatives, expressed formally as <span class="math inline">\(\frac{d}{dx}(x^n) = nx^{n-1}\)</span>, is where we subtract the exponent value of a function by 1 and place the original value to be multiplied by the base number. For example, the derivative of <span class="math inline">\(y=2x^3\)</span> is just <span class="math inline">\(6x^2\)</span>, since <span class="math inline">\(3-1=2\)</span> and <span class="math inline">\(2 \times 3 = 6\)</span>. The derivative of <span class="math inline">\(x^2\)</span> is <span class="math inline">\(2x\)</span>. With this in mind, we set up our objective function</p>
<p><span class="math display">\[
H = \operatorname*{arg max}_{t \in \mathbb R_{&gt; 0}} \left(−5t^2 +20t+2 \right).
\]</span></p>
<p>where <span class="math inline">\(t\)</span> (a postitive real number) is time in seconds, expressed as <span class="math inline">\(\mathbb{R}_{&gt;0} = \left\{ t \in \mathbb{R} \mid x &gt; 0 \right\}\)</span>. We seek the value of <span class="math inline">\(t\)</span> where the ball is at the maximum height possible.</p>
<div class="cell" data-engine="jupyter" data-execution_count="3">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We then follow the rules I’ve explained above, differentiating with respect to (<em>w.r.t</em>) each term.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; h(t) = -5t^2 + 20t + 2 = \\
&amp; \dfrac{\mathrm{d}}{\mathrm{d}t}(-5t^2) + \dfrac{\mathrm{d}}{\mathrm{d}t}(20t) + \dfrac{\mathrm{d}}{\mathrm{d}t}(2)= \\
&amp; t(5\times 2) +1(20) = \overbrace{-10t + 20}^{\text{Derivative}}
\end{aligned}
\]</span></p>
<p>Let’s break this down. The derivative of <span class="math inline">\(-5x^2\)</span> must be <span class="math inline">\(-10x\)</span> by the power rule. All this means is that for every additional second, the ball falls by ten more feet due to gravity. The 20 reflects the initial velocity that we threw the ball at, or the 20 feet per second I mentioned above. And the derivative for 2 is 0 because time has no influence on the height of ground we threw the ball from, we started from where we started from, gravity is what it is, and thus this should not affect the rate of change of the height of the ball. We can then solve the derivative for 0 to find the optimal point.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; -10t + 20=0 \Rightarrow  \\
&amp; -10t = -20 \Rightarrow \\
&amp; \frac{-10t}{-10} = \frac{-20}{-10} \Rightarrow \\
&amp; \boxed{t=2}
\end{aligned}
\]</span></p>
<p>Okay, so the ball is at its zenith after 2 seconds. We may now plug in the value of 2 into the original function to get the maximum height of the ball:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; -5t^2 + 20t + 2 \Rightarrow \\
&amp; -5(2)^2 + 20(2) + 2 \Rightarrow \\
&amp; -20+40+2=22
\end{aligned}
\]</span></p>
<p>From here, we can see why we set the derivative to 0. Since the derivative is the rate of change of the function at a specific point, and we know the ball is rising vertically since we shot it upwards, we also know the ball must be slowing down over time as it rises. The maximum point is simply the height where the speed of the ball is 0 miles per hour, and therefore not changing anymore so that it may be pulled to earth.</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(t\)</span></th>
<th><span class="math inline">\(h(t)\)</span></th>
<th><span class="math inline">\(h'(t)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>2</td>
<td>20</td>
</tr>
<tr class="even">
<td>1</td>
<td>17</td>
<td>10</td>
</tr>
<tr class="odd">
<td>2</td>
<td>22</td>
<td>0</td>
</tr>
<tr class="even">
<td>2.5</td>
<td>21.25</td>
<td>-5</td>
</tr>
</tbody>
</table>
<p>We can know that we are at a maximum by taking <a href="https://www.youtube.com/watch?v=-cW5hCsc9Yc">the second derivative</a> of our function, which would just be <span class="math inline">\(-10\)</span>. When the second derivative, expressed as <span class="math inline">\(\dfrac{\mathrm{d}^{2}}{\mathrm{d}t^{2}}&lt; 0\)</span>, we know that we are at a maximum point. And since <span class="math inline">\(-10 &lt; 0\)</span>, we know that this is a maximum point.</p>
</section>
<section id="chain-rule" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="chain-rule"><span class="header-section-number">7.2.2</span> Chain Rule</h3>
<p>The next rule of differentiation to know is something called <a href="https://youtu.be/XIQ-KnsAsbg?si=53HjEWztsQh0g6O2">the chain rule</a>. The chain rule is called that <a href="https://www.youtube.com/watch?v=FKraGDm2fUY">because</a> of the <em>chain</em> reaction we can think of when we think of the way the value of one function affects the value of another function. In mathematics this is called a <a href="https://youtu.be/wUNWjd4bMmw?si=J4GbqUBTLvQ9DV-B"><em>composite function</em></a>.</p>
<p>A common example in economics is where we wish to maximize profits. Suppose we are the manager of shipping for a company. Our job is to ship kilos of product to a city so that a wholeseller, or a <em>distributor</em>, may sell to vendors who will in turn sell to customers. However, we are not doing business in a competitive market. The local distributor has a monopoly over entry ports, and is able to set prices. The distributor will give us 75,000 dollars per <span class="math inline">\(x\)</span> kilos of product we give them. As the sellers, we must come up with the right amount of kilos to sell such that we maxmize our profits, given the price we face. Ideally, we could snap our fingers and sell them the product in an unlimited manner. In other words, the profit we’d make per ki would simply be <span class="math inline">\(r(x) = 75x\)</span>.</p>
<p>But unfortunately, we do have costs. We have a set of <em>fixed costs</em> comprising baseline expenditures of doing business with the distributor. For example, we may have transportation costs to move our product from home to the city, a certain amount of money for fuel, and other costs that we simply cannot avoid paying in order to do business at all (in the ball analogy, this would be how far we are off the grouund originally). We have <em>variable costs</em> which are things that we as the producers, in the very short run, have direct control over, say, how many drivers we have or how much plastic we use, the number of workers on our farms, and so on (in the ball analogy above, this would be how hard we throw the ball). And finally, we have marginal costs, or the costs that we bear as the business for each new ki produced (the downward pull of gravity in the above example).</p>
<p>For this example, our cost function is <span class="math inline">\(c(x)=0.25(x+6)^2+191\)</span> (note that all of these numbers are in 1000s, but we shall solve the profit exactly as I’ve written it). As we’ve discussed above, the constant will at some point go away, and we will be left with only the factors of production that actually change (our marginal and variable costs). But I’m getting ahead of myself. First, we have to think about what profit means.</p>
<p>In microeconomic theory, our profit function is <span class="math inline">\(\pi(x)=r(x)-c(x)\)</span>, or the difference between how much we make in dollars versus how much it costed us in dollars to produce all that we’ve sold. If we produced nothing, we’d be losing money since we still have to pay fixed costs (200 in this case). However, as we produce one <em>more</em> ki, we slowly increase the amount of money we make until our revenue equals our total costs. This is known as the break even point, when we’re not profiting or operating at a loss, we’re making just enough to remain in business. After this point, as we produce and sell more, our revenue begins to grow relative to our costs (say, as we get more efficient in distributing workers and tools to make the product). However at some point, the rate of change of profit will be equal to 0. Why? Well, we can’t keep producing forever because we do not have infinite resources. This means that while we may detect a change in profits from 1 kilo to 100, at some point it will not make sense to produce another kilo because the cost it takes to make another kilo is greater than the revenue we would make from selling it. While we’d still be making profit at that level, or profits would not be maximized.</p>
<p>Like the ball example, we’re looking to ascend the profit function by producing more until the rate of change of profit is at 0. So, to maximize profit, we differentiate each component of the profit function, <span class="math inline">\(\dfrac{\mathrm{d}\pi(x)}{\mathrm{d}x} = \dfrac{\mathrm{d}r(x)}{\mathrm{d}x} - \dfrac{\mathrm{d}c(x)}{\mathrm{d}x}\)</span>. When we take the derivative of both revenue and cost functions, combine them together, and solve for 0, we can find the point at which our overall profits are maximized. As before, we may express this as an objective function, where our <em>objective</em> is to find the level of production value that maximizes our profit. Our objective function in this case looks like</p>
<p><span class="math display">\[
\operatorname*{arg max}_{x \in \mathbb R} \left(\underbrace{75x}_{r(x)} - [\underbrace{0.25(x+6)^2+191}_{c(x)}] \right).
\]</span> We consider these functions separately <span class="math display">\[
\begin{aligned}
&amp;r(x) = 75x \\
&amp;c(x) = 0.25(x+6)^2 + 191
\end{aligned}
\]</span> and then take their derivatives. Beginning with revenue, <span class="math display">\[
\begin{aligned}
&amp;r(x) = 75x \\
&amp;\boxed{\dfrac{\mathrm{d}r}{\mathrm{d}x} = 75}.
\end{aligned}
\]</span></p>
<p>Simple enough. Profit increases by 75,000 per ki sold. Now, for <span class="math inline">\(c(x) = 0.25(x+6)^2 + 191\)</span>, we apply the chain rule. The outer function (or everything outside the parentheses here) is <span class="math inline">\(u(y) = 0.25y^2 + 191.\)</span> The inner function is <span class="math inline">\(y(x)=x+6\)</span>.</p>
<p>First, we differentiate the outer function <span class="math inline">\(u(y)\)</span> by applying the power rule:</p>
<p><span class="math display">\[
\frac{\mathrm{d}u}{\mathrm{d}y} = 0.25 \cdot 2y = 0.5y.
\]</span></p>
<p>The constant vanishes, and the 2 from the quadratic comes down and we multiply it by 0.25. Now we differentiate the inner function:</p>
<p><span class="math display">\[
\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{\mathrm{d}}{\mathrm{d}x}(x + 6) = 1.
\]</span></p>
<p>The chain rule states: <span class="math inline">\(\frac{\mathrm{d}}{\mathrm{d}x} u(y(x)) = \frac{\mathrm{d}u}{\mathrm{d}y} \cdot \frac{\mathrm{d}y}{\mathrm{d}x}\)</span>. Applying the chain rule:</p>
<p><span class="math display">\[
\frac{\mathrm{d}u}{\mathrm{d}x} = \frac{\mathrm{d}u}{\mathrm{d}y} \cdot \frac{\mathrm{d}y}{\mathrm{d}x} = 0.5y \cdot 1 = 0.5(x + 6).
\]</span></p>
<p>So, we are left with:</p>
<p><span class="math display">\[
\frac{\mathrm{d}u}{\mathrm{d}x} = 0.5(x+6).
\]</span></p>
<p>After distributing the one-half term:</p>
<p><span class="math display">\[
0.5(x+6) = 0.5x + 0.5 \cdot 6 = 0.5x + 3.
\]</span></p>
<p>Therefore, the final result is:</p>
<p><span class="math display">\[
\boxed{0.5x + 3}.
\]</span></p>
<p>Next, we combine the derivatives together in the original equation</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\dfrac{\mathrm{d}\pi(x)}{\mathrm{d}x} = \dfrac{\mathrm{d}r(x)}{\mathrm{d}x} - \dfrac{\mathrm{d}c(x)}{\mathrm{d}x} = \\
&amp; 75 - (0.5x + 3) = \\
&amp;75-3-0.5x = \\
&amp;72 - 0.5x.
\end{aligned}
\]</span></p>
<p>Now we solve for 0:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;72 - 0.5x = 0 \\
&amp;72= 0.5x  \\
&amp;\frac{72}{0.5}=\frac{0.5}{0.5}x \\
&amp;\boxed{\text{Optimal Production Level: }144=x}.
\end{aligned}
\]</span></p>
<p>To verify that we are maximizing, we take the second derivative of the profit function, <span class="math inline">\(\dfrac{\mathrm{d}^2 \pi}{\mathrm{d}x^2} = \dfrac{\mathrm{d}^2 r}{\mathrm{d}x^2} - \dfrac{\mathrm{d}^2 c}{\mathrm{d}x^2}\)</span>. To do this, we do: <span class="math display">\[
\begin{aligned}
&amp;\dfrac{\mathrm{d}}{\mathrm{d}x}=[75 - 0.5x - 3] = -0.5.
\end{aligned}
\]</span> The derivative of the two constants are both 0, so those are deleted. All we’re left with is the linear term. Since <span class="math inline">\(\dfrac{\mathrm{d}^2 \pi}{\mathrm{d}x^2}&lt;0\)</span>, we are at a maximum point. Therefore, the number of kilos we should sell our distributor is 144. What is profit? <span class="math inline">\((75x) - (0.25(x+6)^2 + 191)\)</span> where <span class="math inline">\(x = 144\)</span>. First, calculate <span class="math inline">\((x+6)^2\)</span>: <span class="math display">\[
(x+6)^2 = (144+6)^2 = 150^2
\]</span> <span class="math display">\[
150^2 = 22500
\]</span></p>
<p>Next, calculate <span class="math inline">\(0.25(x+6)^2\)</span>: <span class="math display">\[
0.25(x+6)^2 = 0.25 \times 22500 = 5625
\]</span></p>
<p>This is the sum of our marginal costs and the variable costs. Next we calculate <span class="math inline">\(75 \times 144\)</span>: <span class="math display">\[
75 \times 144 = 10800.
\]</span></p>
<p>This is our total revenue. Now we add the marginal, variable, and fixed costs: <span class="math display">\[
5625 + 191 = 5816.
\]</span></p>
<p>Subtract total revenue from total costs <span class="math display">\[
10800 - 5816 = 4984
\]</span></p>
<p>Therefore, since I said all these numbers were in 1000s, our profit from 144 kis sold is <span class="math inline">\(\boxed{4,984,000}\)</span>. To verify that none of what we just did is voodoo, we can check this by plotting the profit function.</p>
<div class="cell" data-engine="jupyter" data-execution_count="4">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Ok, all done for now. We will use derivatives to solve for the value which minimizes the sum of residuals squared. This is known as <em>ordinary least squares</em> regression (OLS), also called <em>linear regression</em>, or simply just “regression”. OLS is the main estimator you’ll use for this class, and it is the main foundation of econometric analysis for public policy research. It will be much more involved than what we just did, but this provides the mathematical foundation for regression as an estimator.</p>
</section>
</section>
<section id="an-extended-example" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="an-extended-example"><span class="header-section-number">7.3</span> An Extended Example</h2>
<p>To introduce OLS, we can return to the equation of a line (<span class="math inline">\(y=mx+b\)</span>) where <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> are variables. Unlike the above examples where <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> were once known variables, now they are unknown quantities we must solve for. Below, <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> will take on the values of <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_0\)</span> respectively. With OLS, we have multiple predictors (typically), each of which affect the output of the function differently.</p>
<p>In the multivariable case, we take the <em>partial derivative</em> <em>w.r.t.</em> each variable (that is, assuming that the other variables do not change). If this seems at all abstract to you, I will provide a detailed, clear derivation of the betas. Note that for all of the steps below, Stata, R, or Python does (and optimizes!) this process for you. I only provide this derivation so you have a source to refer to when you wish to know how and <em>why</em>, exactly, the machine returns the numbers that it returns to you. I also believe a clear explanation of the math will help you understand how to interpret the results that we get.</p>
<p>Before we continue, let’s fix ideas. Suppose we wish to attend a nightclub and we wish to express how much we pay for that evening as a function (our outcome variable, a ratio level variable). At this nightclub, our outcome is a function of two things. We pay <em>some</em> one-time cost of money to enter, and then we pay <em>some</em> amount of money per new drink we buy (where number of drinks is also a ratio level variable). I say “<em>some</em>” because unlike the real world where we would know the price and entry fees by reading the sign, in this case we wish to estimate these values with only two known variables: how many drinks we bought and how much we paid.</p>
<div class="cell" data-engine="jupyter" data-execution_count="5">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="list-the-data" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="list-the-data"><span class="header-section-number">7.3.1</span> List the Data</h3>
<p>Say that we have data that looks like <span class="math inline">\((0, 30), (1, 35), (2, 40)\)</span>, where <span class="math inline">\(x\)</span>= number of drinks we buy (0,1,2) and <span class="math inline">\(y\)</span>=amount of money we spend that evening (30,35,40). In spreadsheet format, this looks like:</p>
<table class="table">
<thead>
<tr class="header">
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>30</td>
</tr>
<tr class="even">
<td>1</td>
<td>35</td>
</tr>
<tr class="odd">
<td>2</td>
<td>40</td>
</tr>
</tbody>
</table>
<p>If you want to, calculate the rise-over-run of these data points to derive <span class="math inline">\(m\)</span> and see what the answer might be in the end. Below though, we proceed by deriving what <span class="math inline">\(m\)</span> <em>must be</em>.</p>
<p><span class="math display">\[
m = \frac{35 - 30}{1 - 0} = \ldots
\]</span></p>
</section>
<section id="define-our-econometric-model" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="define-our-econometric-model"><span class="header-section-number">7.3.2</span> Define Our Econometric Model</h3>
<p>We begin by defining our model. That is, we specify our outcome and the variables which affect our outcome (the values we’re solving for, entry price and drink cost). Our model of how much we pay given some entry fees and additional drink costs looks like:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1x_i
\]</span></p>
<p>Here, <span class="math inline">\(y_i\)</span> is the total amount of money we spend that evening in dollars given the <span class="math inline">\(i\text{-th}\)</span> drink, <span class="math inline">\(\hat{\beta}_{0}\)</span> is how much we pay (also in dollars) to enter, <span class="math inline">\(\hat{\beta}_{1}\)</span> is how much we pay for the <span class="math inline">\(i\text{-th}\)</span> drink, and <span class="math inline">\(x\)</span> is the total number of drinks we get. Nothing <strong>at all</strong> about this is different, so far, from anything we’ve discussed above. I’ve simply substituted <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> with the Greek letter <span class="math inline">\(\beta\)</span> (“beta”) into the already familiar equation for a line.</p>
</section>
<section id="write-out-the-objective-function" class="level3" data-number="7.3.3">
<h3 data-number="7.3.3" class="anchored" data-anchor-id="write-out-the-objective-function"><span class="header-section-number">7.3.3</span> Write Out the Objective Function</h3>
<p>Now that we have our model, next let’s think about what the objective function would be. We already know that we wish to minimize the residuals of the OLS line. So, we can represent the objective function for OLS as <span class="math display">\[
\begin{aligned}
&amp; S = {\text{argmin}} \sum_{i=1}^{n} \hat{\epsilon}^{2}  \\
&amp; S = {\text{argmin}} \sum_{i=1}^{n} \left( y_i - \hat{y} \right)^{2} \\
&amp; \text{where } \hat{y} \text{ is defined as} \\
&amp; S = \underset{\hat{\beta}_0,\hat{\beta}_1}{\text{argmin}} \sum_{i=1}^{n} (y_i - (\overbrace{\hat{\beta}_0 + \hat{\beta}_1x}^{\text{Predictions}}))^2.
\end{aligned}
\]</span></p>
<p>As above, we call the solutions <span class="math inline">\(\hat{\beta}_0,\hat{\beta}_1\)</span> <em>optimal</em> if they return the lowest possible values the function <span class="math inline">\(S\)</span> can produce. What values can <span class="math inline">\(S\)</span> produce? The sum of the squared residuals. The sigma symbol <span class="math inline">\(\sum_{i=1}^{n}\)</span> means we are adding up the <span class="math inline">\(i\text{-th}\)</span> squared residual to the <span class="math inline">\(n\text{-th}\)</span> data point/number of observations (in this case 3). This means that the line we compute will be as close as it can be to the observed data at every single data point. By the way, just to show the objective function is not an optical illusion or arcane expression, we can literally plot the objective function in three dimensions, where we have the slope and intercept values plotted against our total squared residuals.</p>
<div class="cell" data-engine="jupyter" data-execution_count="6">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>I plot the values returned by the objective function given the datapoints and some value for our coefficients. Of course, we wish to get the values for the betas which produce the lowest possible values for this plane. Clearly, the intercept can’t be 50 and the slope 20 (as this maximizes the residuals!).</p>
<p>The middle formula, <span class="math inline">\(\left( y_i - \hat{y} \right)^{2}\)</span>, re-expresses the residuals. This expression should look familiar. <a href="https://jgreathouse9.github.io/GSUmetricspolicy/basicprob.html#variance">Recall</a> the formula for the variance of a variable <span class="math inline">\(\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]\)</span> ? Here, we are taking our real observations <span class="math inline">\(y\)</span> and subtracting some <em>expected value</em> <span class="math inline">\(\hat{y}\)</span> (y-hat) from it. Because we are minimizing the residuals (or, the model prediction error), another way of thinking about OLS is that is the line that maximizes the explained variance given our independent variables. The lines that has the least-wrong predictions is also the line that explains its variation patterns best.</p>
<p>As with the variance, one may ask <em>why</em> we are squaring the summed <span class="math inline">\(\hat{\epsilon}_i\)</span>, instead of say <a href="https://learningds.org/ch/04/modeling_loss_functions.html#mean-absolute-error">the absolute error</a>. <a href="https://www.youtube.com/watch?v=U46D7oEijlI">First of all</a>, minimizing the raw sum of the predicted residuals (that is, without squaring them) is a non-differentiable function. <a href="https://medium.com/@polanitzer/the-minimum-mean-absolute-error-mae-challenge-928dc081f031">We could</a> use the raw sum of the errors as an objective function (called the <em>mean absolute error</em> instead of the <em>mean squared error</em>), but <a href="https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/algorithm/2.linearRegressionGradientDescent.md#gradient-descent-to-solve-linear-regression-with-mean-absolute-error-mae-loss-function">have fun doing that</a>, as due to the non-differentiable nature of the absolute value function, we would need to use numerical methods, such as gradient descent, to compute its solution. By no means impossible… just computationally less tractable.</p>
<p>Using the squared residuals means that we are dealing a quadratic function which, as we did above, is easily differentiable. The squaring of residuals also penalizes worse predictions. Indeed, just as with the variance, all residuals <em>should</em> not be created equally. If the observed value is 20 but we predict 25, the residual is -5. Its squared residual is 25. But if the observed value is 40, and we predict 80, the “absolute” error is -40 and the squared error of is 1600. If we did not square them, we would be treating a residual of 5 as the same weight as a residual of 40. For proof, we can plot these</p>
<div class="cell" data-engine="jupyter" data-execution_count="7">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="substitute-into-the-objective-function" class="level3" data-number="7.3.4">
<h3 data-number="7.3.4" class="anchored" data-anchor-id="substitute-into-the-objective-function"><span class="header-section-number">7.3.4</span> Substitute Into the Objective Function</h3>
<p>First, we can substitute the real values as well as our model for prediction into the objective function. We already know the values x-takes. You either buy no drinks, 1 drink, or 2. So with this information, we can now find the amount of money we pay up front (<span class="math inline">\(\hat{\beta}_0\)</span>) and how much it costs for each drink (<span class="math inline">\(\hat{\beta}_1\)</span>) <span class="math display">\[
\begin{aligned}
S = &amp;\underbrace{(30 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 0))^2}_{\text{Term 1}} + \\
&amp;\underbrace{(35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1))^2}_{\text{Term 2}} + \\
&amp; \underbrace{(40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2))^2}_{\text{Term 3}}
\end{aligned}
\]</span></p>
<p>And when we look at the notation carefully, we see all of this makes sense: we are adding up the differences between our outcome, and the predictions of our model.</p>
</section>
<section id="take-partial-derivatives" class="level3" data-number="7.3.5">
<h3 data-number="7.3.5" class="anchored" data-anchor-id="take-partial-derivatives"><span class="header-section-number">7.3.5</span> Take Partial Derivatives</h3>
<p>To find the values of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, we take the partial derivatives of <span class="math inline">\(S\)</span> with respect to (w.r.t.) <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>. Here is a short sketch of how we do this: For simplicity, I break this into two sections, one section per coefficient. In this case, the chain rule and power rules for differentiation are our friends here. To hear more about combining the power rule and chain rule, <a href="https://www.youtube.com/watch?v=TI-j-fr6c4A">see here</a>. First, we differentiate <em>w.r.t.</em> <span class="math inline">\(\hat{\beta}_0\)</span> (entry fees), then we do the same for <span class="math inline">\(\hat{\beta}_1\)</span> (drink fees).</p>
<ol type="1">
<li><strong>Partial derivative w.r.t. <span class="math inline">\(\hat{\beta}_0\)</span>:</strong></li>
</ol>
<p>Here is our full objective function:</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_0} = \frac{\partial}{\partial \hat{\beta}_0} \left[ (30 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 0))^2 + (35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1))^2 + (40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2))^2 \right].
\]</span></p>
<p>By the chain rule, we can take the partial derivative by applying the power rule to the outer functions and the linear differentiation rule to the inner functions: <span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_0} = \frac{\partial}{\partial \hat{\beta}_0} (30 - \hat{\beta}_0+ \hat{\beta}_1 \cdot 0)^2 + \frac{\partial}{\partial \hat{\beta}_0} (35 - (\hat{\beta}_0 + \hat{\beta}_1))^2 + \frac{\partial}{\partial \hat{\beta}_0} (40 - (\hat{\beta}_0 + 2\hat{\beta}_1))^2.
\]</span></p>
<p>In the first term, we have <span class="math inline">\(\hat{\beta}_1 \cdot 0\)</span>, so we keep the other part of the function but the <span class="math inline">\(\hat{\beta}_1\)</span> goes away since that’s what anything multiplied by 0 means. So, the quadratic power goes outside the parentheses, and the derivative of <span class="math inline">\(-\hat{\beta}_1\)</span> is just <span class="math inline">\(-1\)</span>. So by application of the chain rule, we get this result:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \frac{\partial S}{\partial \hat{\beta}_0} = 2(30 - (\hat{\beta}_0)) \cdot (-1) + 2(35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1)) \cdot (-1) + 2(40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2)) \cdot (-1).
\end{aligned}
\]</span></p>
<p>See how all these three terms are multiplied by <span class="math inline">\(-1\)</span> and <span class="math inline">\(2\)</span>? Well, by <a href="https://www.youtube.com/watch?v=3NHSwiv_pSE">distributive property</a>, we know we can factor out the 2 and negative 1. That returns this result:</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_0} = -2 \left[ (30 - \hat{\beta}_0) + (35 - \hat{\beta}_0 - \hat{\beta}_1) + (40 - \hat{\beta}_0 - 2\hat{\beta}_1) \right].
\]</span></p>
<p>Next I rearrange everything inside of the brackets: <span class="math display">\[
\begin{aligned}
&amp;(30 + 35 + 40) - (\hat{\beta}_0 - \hat{\beta}_0 + \hat{\beta}_0) + (\hat{\beta}_1 - 2\hat{\beta}_1) = \\
&amp;(105)+(- 3\hat{\beta}_0)+(-3\hat{\beta}_1).
\end{aligned}
\]</span> Finally, we just distribute the 2:</p>
<p><span class="math display">\[
\begin{aligned}
-2 \left[ 105 - 3\hat{\beta}_0 - 3\hat{\beta}_1 \right] \\
= \boxed{-210 + 6\hat{\beta}_0 + 6\hat{\beta}_1}.
\end{aligned}
\]</span></p>
<p>This is our partial derivative for the first coefficient, or for the entry fee.</p>
<ol start="2" type="1">
<li><strong>Partial derivative w.r.t. <span class="math inline">\(\hat{\beta}_1\)</span>:</strong></li>
</ol>
<p>We can follow a similar process for this partial derivative: <span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_1} = \frac{\partial}{\partial \hat{\beta}_1} \left[ (30 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 0))^2 + (35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1))^2 + (40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2))^2 \right]
\]</span></p>
<p>Using the chain rule, this looks like:</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_1} = \frac{\partial}{\partial \hat{\beta}_1} (30 - \hat{\beta}_0)^2 + \frac{\partial}{\partial \hat{\beta}_1} (35 - (\hat{\beta}_0 + \hat{\beta}_1))^2 + \frac{\partial}{\partial \hat{\beta}_1} (40 - (\hat{\beta}_0 + 2\hat{\beta}_1))^2.
\]</span></p>
<p>We apply the power rule to the outer terms and the linear differentiation rules to each inner term. As before, the 2 simply goes in from of the parentheses and the derivative of <span class="math inline">\(\hat{\beta}_1\)</span> is taken. Note that for the first term, <span class="math inline">\(\hat{\beta}_1\)</span> is multiplied by 0, so since this is multiplied by the outer function, the first term vanishes completely.</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_1} = 2(2(35 - (\hat{\beta}_0 + \hat{\beta}_1)) \cdot (-1) + 2(40 - (\hat{\beta}_0 + 2\hat{\beta}_1)) \cdot (-2).
\]</span></p>
<p>As we can see, the 2 again is a common term, which we again put outside in brackets:</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_1} = 2 \left[- 1 \cdot (35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1)) - 2 \cdot (40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2)) \right].
\]</span></p>
<p>When we simplify the inner terms, we get:</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_1} = 2 \left[ - (35 - (\hat{\beta}_0 + \hat{\beta}_1)) - 2 \cdot (40 - (\hat{\beta}_0 + 2\hat{\beta}_1)) \right].
\]</span></p>
<p>We apply the distributive property again for the 1 and 2:</p>
<p><span class="math display">\[
2 \left[ -35 + \hat{\beta}_0 + \hat{\beta}_1 - 80 + 2\hat{\beta}_0 + 4\hat{\beta}_1 \right].
\]</span></p>
<p>Now we rearrange by putting the same terms next to each other:</p>
<p><span class="math display">\[
2 \left[ \hat{\beta}_0 + 2\hat{\beta}_0 + \hat{\beta}_1 + 4\hat{\beta}_1 - 35 - 80 \right].
\]</span></p>
<p>and simplify by combining them together:</p>
<p><span class="math display">\[
2 \left[ 3\hat{\beta}_0 + 5\hat{\beta}_1 - 115 \right].
\]</span></p>
<p>Thus after distributing the 2 inside of the brackets, the partial derivative of <span class="math inline">\(S\)</span> with respect to <span class="math inline">\(\hat{\beta}_1\)</span> is:</p>
<p><span class="math display">\[
\boxed{6\hat{\beta}_0 + 10\hat{\beta}_1 - 230}.
\]</span></p>
<p>Now we’ve taken the partial derivatives of both our variables, entry fees (which we presume are constant) and the number of drinks we buy. This can be represented like:</p>
<p><span class="math display">\[
\nabla S = \begin{bmatrix}
\frac{\partial S}{\partial \hat{\beta}_0} \\
\frac{\partial S}{\partial \hat{\beta}_1}
\end{bmatrix}
= \begin{bmatrix}
-210 + 6\hat{\beta}_0 + 6\hat{\beta}_1 \\
-230 + 6\hat{\beta}_0 + 10\hat{\beta}_1
\end{bmatrix}.
\]</span></p>
<p>Technically, in mathematics, we’d call this <a href="https://youtu.be/tIpKfDc295M?si=UPGYqJTsrkZIrePP">the gradient</a>. Before we continue though, do not lose sight of our goal: all these two equations represent are the instantaneous rates of change in our sum of squared residuals given some change in the variable in question. Our goal is still to find the the values for these betas that minimize our objective function.</p>
</section>
<section id="get-the-betas" class="level3" data-number="7.3.6">
<h3 data-number="7.3.6" class="anchored" data-anchor-id="get-the-betas"><span class="header-section-number">7.3.6</span> Get the Betas</h3>
<p>Okay, no more calculus. We can now return to <em>algebraland</em> to get our betas, with a slight modification.</p>
<p>Remember how above after we calculated the normal derivative of the profit function or the ball trajectory we just solved the derivative for 0? In that case, we had only one variable, <span class="math inline">\(t\)</span> or <span class="math inline">\(x\)</span>. Well, now we don’t just have one variable! We have 2 <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span>.</p>
<p>As before, we still want to set these both equal to 0 because at the point both equal 0, our sum of squared residuals is no longer rising or falling (or, it is the the critical point on the surface I plotted above). So, let’s write these equations again (I divided the first equation by 6 since all of its terms were divisible by 6). To solve both equations simultaneously, first we add the constants to both RHS of both partial derivatives: <span class="math display">\[
\begin{aligned}
&amp;\hat{\beta}_0 + \hat{\beta}_1 = 35 \\
&amp;6\hat{\beta}_0 + 10\hat{\beta}_1 = 230.
\end{aligned}
\]</span></p>
<p>Here I use <a href="https://www.youtube.com/watch?v=V7H1oUHXPkg">a method called substitution</a> to solve the system, but there are <a href="https://youtu.be/vA-55wZtLeE?si=hUaVyHnDGXrAv4Ya">many</a> such ways we can solve this. In substitution, we solve one equation first and substitute the solution for a variable into the other equation. I solve the first parital for <span class="math inline">\(\hat{\beta}_0\)</span> since it is the easiest. So, we subtract <span class="math inline">\(\hat{\beta}_1\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\hat{\beta}_0 + \hat{\beta}_1 = 35 \Rightarrow \\
&amp;\hat{\beta}_0 = 35 - \hat{\beta}_1.
\end{aligned}
\]</span></p>
<p>Okay, so this is our expression for <span class="math inline">\(\beta_0\)</span>. Since we now know the expression for the constant (the entry fee), we can plug this into the partial for <span class="math inline">\(\hat{\beta}_1\)</span> where the <span class="math inline">\(\beta_0\)</span> currently is and solve for <span class="math inline">\(\hat{\beta}_1\)</span>. We do: <span class="math display">\[
\begin{aligned}
&amp;6\hat{\beta}_0 + 10\hat{\beta}_1 = 230 \\
&amp;6(35 - \hat{\beta}_1) + 10\hat{\beta}_1 = 230.
\end{aligned}
\]</span> Next, we distribute the 6 <span class="math display">\[
210 - 6\hat{\beta}_1 + 10\hat{\beta}_1 = 230
\]</span> and combine the terms <span class="math inline">\(- 6\hat{\beta}_1 + 10\hat{\beta}_1\)</span> together. That returns this result: <span class="math display">\[
210 + 4\hat{\beta}_1 = 230.
\]</span> Next, we subtract 210 <span class="math display">\[
4\hat{\beta}_1 = 20.
\]</span> Finally, we divide by 4 <span class="math display">\[
\boxed{\hat{\beta}_1 = 5}.
\]</span></p>
<p>Now, we know our value for <span class="math inline">\(\hat{\beta}_1\)</span>!!! We know that for each drink we get, we pay 5 more dollars. Since we now know <em>this</em>, we substitute 5 into <span class="math inline">\(\hat{\beta}_0 + \hat{\beta}_1 = 35\)</span> where <span class="math inline">\(\hat{\beta}_1\)</span> is. Then, we have one equation to solve for, with our goal being to get the value of <span class="math inline">\(\hat{\beta}_0\)</span>. We can put this value into the partial derivative for <span class="math inline">\(\hat{\beta}_0\)</span>: <span class="math display">\[
\hat{\beta}_0 + 5 = 35.
\]</span></p>
<p>Now, we simply subtract 5 from the RHS <span class="math display">\[
\boxed{\hat{\beta}_0 = 30}.
\]</span></p>
<p>The entry fee is 30 dollars.</p>
</section>
<section id="our-ols-line-of-best-fit" class="level3" data-number="7.3.7">
<h3 data-number="7.3.7" class="anchored" data-anchor-id="our-ols-line-of-best-fit"><span class="header-section-number">7.3.7</span> Our OLS Line of Best Fit</h3>
<div class="cell" data-engine="jupyter" data-execution_count="8">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>So, our line of best fit is <span class="math inline">\(\hat{y} = 30 + 5x\)</span>. In social science, you’ll hear people throw around terms like “when we controlled for <em>this</em>” or “<em>adjusted for</em>” another variable, or “when we <em>hold constant</em> these set of variables”. This is what they mean by it! They, in the plainest language possible, mean that the dependent variable changes by <em>that amount</em> for every unitary increase in an independent variable, assuming the other values of the function do not change change. That’s exactly what the partial derivative is, the change in a function given a change in one variable for that function. So here, assuming the club has a flat entry fee that does not change on a person to person basis, the amount the function changes by for every new drink is an increase of 5 dollars. Or, <em>compared</em> to the scenario where you only wanted to get in the club (and not drink at all, where <span class="math inline">\(x=0\)</span>), you spend 5 more dollars per each new drink you get. One may ask why we did this at all. Why <em>bother</em> with the partial derivative approach and the messy system of equations, why not simply display a regression table and go through the practical interpretation? After all, assuming we just did the following in Stata: <code>reg y x</code>, we would’ve gotten the exact same set of results that I just did quicker.</p>
<p>The primary reason is pedagogical. OLS was never derived for me in quite this manner in undergrad. So I believe you should see it done with a simple, tractable, familiar example, even though you’ll never do this for any work you ever do. This way, OLS is not a computerized black box you mindlessly use for a dataset- you actually can <em>see</em> where the numbers come from in a simplified way.</p>
</section>
</section>
<section id="inference-for-ols" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="inference-for-ols"><span class="header-section-number">7.4</span> Inference For OLS</h2>
<p>Now that we’ve conducted estimation, we can now conduct inference with these statistics we’ve generated. Indeed, this is the primary point of this at all, in a sense. We <em>want</em> to know if these estimates are different from some null hypothesis. To begin, recall the notation of <span class="math inline">\(\hat{\epsilon}_i\)</span> which denotes our residuals for the regression predictions. We can use this to generate the standard error/the uncertainty statistic associated with the respective regression estimate. We can begin with the residual sum of squares, calculated like <span class="math inline">\(\text{RSS} = \sum (\hat{\epsilon}_{i})^2\)</span>. Put another way, it is all the variation <em>not</em> explained by our model. If <span class="math inline">\(RSS=0\)</span>, as was the case in the above example, then we have no need for inference since there’s nothing our model does <em>not</em> explain. We then can estimate the variance of the error like <span class="math inline">\(\hat{\sigma}^2 = \frac{\text{RSS}}{n - p}\)</span>, where <span class="math inline">\(n\)</span> is our number of observations and <span class="math inline">\(p\)</span> is our number of predictors (including the constant). We divide by <span class="math inline">\(n-p\)</span> because this takes into account our model’s residual degrees of freedom, or our model’s freedom to vary. Note as well that when <span class="math inline">\(n=p\)</span>, the error variance is not defined, meaning for OLS to be valid we need less predictors than observations. For a more detailed explanation of degrees of freedom, see <span class="citation" data-cites="dof">Pandey and Bright (<a href="#ref-dof" role="doc-biblioref">2008</a>)</span>.</p>
<p>For an example of how a regression table is presented, consider the above example that estimates the impact of tobacco prices on consumption across states</p>
<table class="table">
<colgroup>
<col style="width: 36%">
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 18%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Estimate</th>
<th>Std. Error</th>
<th>T-statistic</th>
<th><span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Intercept</td>
<td>323</td>
<td>56.3691</td>
<td>-3.31</td>
<td></td>
</tr>
<tr class="even">
<td>Coefficient (retprice)</td>
<td>-3.17</td>
<td>.958191</td>
<td>-3.028</td>
<td>0.22</td>
</tr>
</tbody>
</table>
<p>As we know from above, this suggests that a one dollar increase in the price of cigarettes implies a reduction of 3 in the rate of tobacco sales per capita. As we’ve discussed with normal descriptive statistics/classic hypothesis testing, we can also compute confidence intervals for these regression estimates. To do this, we need a standard error for our regression estimates. We compute:</p>
<p><span class="math display">\[
\frac{\frac{\text{RSS}}{n-(k+1)}}{\sum(x_i- \bar x)^2}
\]</span> We already know RSS from above. Then, we add the differences of each point for <span class="math inline">\(x\)</span> and its mean. This is:</p>
<p><span class="math display">\[
\frac{\frac{26015.2655}{37}}{765.81}=0.958191.
\]</span></p>
<p>Now that we have this, we can calculate the t-statistic for the beta, which is simply the coefficient divided by the standard error. We can also calculate confidence intervals for coefficients too. The formula for this should appear quite familiar <span class="math display">\[
\beta_j \pm t_{\alpha/2, \text{df}} \cdot \text{SE}(\beta_j)
\]</span></p>
<p>Here, <span class="math inline">\(\beta_j\)</span> is the coefficient of our model, <span class="math inline">\(t\)</span> is our test statistic (1.96 ususally), <span class="math inline">\(\alpha\)</span> is our acceptance region (0.05 in most cases if we want a 95% confidence interval), SE is our standard error as we’ve computed it above. For the price example, we do <span class="math inline">\(-3.171357+(1.96 \times .958191)\)</span> and <span class="math inline">\(-3.171357-(1.96 \times .958191)\)</span>, returning a 95% CI of <span class="math inline">\([-5.112836, -1.229877]\)</span>. There’s a little rounding error, but that’s what we get. The way to interpret the CI is as follows: given our sample size and input data, the true parameter of the effect of price on tobacco smoking rates lies within the range of -5.1 and -1.2. In other words, if some assumptions hold (which we will discuss below), a dollar increase in price may decrease the tobacco smoking rate by as much as 5 or as little as 1.</p>
<p>Just as we discussed in the preceding chapters, lots of statistics is justified asymptotically, based on the law of large numbers and CLT. In other words, as <span class="math inline">\(n\)</span> tends to infinity, <span class="math inline">\(\lim\limits_{n\to \infty}\)</span>, our betas will converege to the true population value and the standard errors will shrink. Ergo, as these shrink, the confidence intervals will tighten, meaning our estimates will be more precise. A practical consequence of this is that as a very general rule, having more observations in your dataset makes your OLS estimates more precise and less biased. For the above for example, we would not trust these estimates as much because they come from one year only. Ideally, to get a better sense of how price increases affect consumption, we’d need to collect these observations over time and adjust for other things that may affect cigarette consumption.</p>
<section id="goodness-of-fit-measures-for-ols" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="goodness-of-fit-measures-for-ols"><span class="header-section-number">7.4.1</span> Goodness of Fit Measures for OLS</h3>
<p>We typically thhink of two goodness of fit statistics when using OLS, the R-squared statistics and the Root Mean Squared Error</p>
<p><span class="math display">\[
\begin{aligned}
R^2 &amp;= 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}} &amp; \text{RMSE} &amp;= \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}.
\end{aligned}
\]</span> Here, for R-squared, we have two terms: first, <span class="math inline">\(\text{SS}_{\text{res}} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)</span> is the sum of squared residuals. <span class="math inline">\(\text{SS}_{\text{res}}\)</span> quantifies the amount of variance unexplained by the independent variables in the model, and <span class="math inline">\(\text{SS}_{\text{tot}}\)</span> is just the total amount of variance of <span class="math inline">\(y\)</span>. Think of R-squared as a ratio/percentage of how well the model explains our outcomes. An R-squared of 1 reflects the simple example we derived above, where the model perfectly explains the variation of our outcomes. R-squared usually scales with the amount of predictors in the model (that is, as we add more variables, the R-squared will increase). However, I think the best way to think about R-squared is a measure of how well our model does, compared to the average of our outcomes. In the nightclub example, if we just took the average of our outcomes, we’d guess for that evening, you’d spend 28.3 dollars. But, in this case, the model significantly outperforms this since it explains the variation perfectly. Note, that it is possible to have a negative R-squared. It is very rare, and it basically means that your model does a worse job than the simple average of the outcomes. I’ve seen this in my work, but it is very rare. I’ve only encountered it in the wild maybe twice. In the above example, including just price as a predictor explains about 22 percent of the variation, which is not bad considering it’s only one variable!</p>
<p>The Root Mean Squared error, or RMSE, is exactly as it sounds: it is the square root of our average of our <span class="math inline">\(\text{SS}_{\text{res}}\)</span>. For the tobacco, example, our RMSE is <span class="math inline">\(\frac{26015.2655}{3726.516}=26.516321\)</span>. In English, this simply means that when we use price to explain consumption for the year 1980, the model is off, on average, by about 27 packs. Again, considering that the average cigarette consumption in 1980 was 137,being off by 26 packs isn’t so bad. It suggests, as one would expect, that we’ve explained our dataset fairly well using price as an explanatory variable. As RMSE approaches 0, we explain our variation better, with an RMSE of 0 being perfection. Note that other goodness of fit metrics do exist; however, these are the most common ones you’ll encounter.</p>
</section>
</section>
<section id="assumptions-of-ols" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="assumptions-of-ols"><span class="header-section-number">7.5</span> Assumptions of OLS</h2>
<p>Keep in mind, despite all the detail we’ve discussed so far, do not lose sight of the larger picture: OLS is simply a mathematical estimation method. Its <em>validity</em> for explaining the external world (aside from having quality data) relies on a few assumptions (collectively called the Gauss-Markov assumptions) being defensible. I say defensible instead of true because practically they are never true. After all, this is statistics: almost all of statistics is true. All statistical research (pretty much, outside of simulations) is at least partly wrong because we live in a probalistic world where we don’t have all the answers. In other words, the assumptions are only as valid as we can defend for them. Below, I detail them.</p>
<section id="assumption-1-linear-in-parameters" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="assumption-1-linear-in-parameters"><span class="header-section-number">7.5.1</span> Assumption 1: Linear in Parameters</h3>
<p>The very first assumption is that the parameters for our model are linear. The classic regression model for OLS is</p>
<p><span class="math display">\[
y_{i} = \beta_{0} + \beta_{1} x_{i1} + \cdots + \beta_{K} x_{iK} + \epsilon_{i}.
\]</span></p>
<p>We call this a linear model. Why? Because the value of beta is constant at all points in the population (or it is assumed to be). How do we know if it’s a linear relationship, and what might violations of this look like?</p>
<div id="exm-lingroceries" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.1 (Groceries) </strong></span>Let’s say we’re at the grocerty store. The banana peppers are 2 bucks per pound of peppers. But when you go to the cashier, and you have 2 pounds of peppers, they tell you you pay 5 dollars. Well… <span class="math inline">\(2 \times 2=4\)</span>. So, you should be paying 4 dollars, but you’re rang up for 5. This suggests that the amount you pay per pound is NOT constant over time, otherwise you’d pay 4 dollars instead of 5.</p>
</div>
<p>Another example:</p>
<div id="exm-linweed" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.2 (Weed) </strong></span>Let’s say we’re buying weed. Say the price per quarter ounce is 80 dollars. The impact of <span class="math inline">\(\beta_{1}\)</span> is the same everywhere in the function, <span class="math inline">\(y=80x\)</span>. But step back and ask ourselves, from the seller’s persepctive, if this makes sense: does it make sense for weed to cost the same for every weight amount? No! Why not? Well, for one, let’s say you’re selling a full gram or pound of weed. That’s <strong><em>so much</em></strong> weed that weed(wo)men/people will charge much much more for lone individuals who wish to buy this much. So while it may be 80 for a quarter ounce, it’ll now be, say, 900 per pound. In fact, we could express this as a piecewise function</p>
<p><span class="math display">\[
\beta_{j} =
\begin{cases}
    80 \text{ if } x &lt; 1 \\
    900 \text{ if } x &gt; 1. \\
\end{cases}
\]</span></p>
<p>Why might this be done? Firstly, that’s so much more product than the average person could smoke or use. So, anyone interested in this would need to pay premium prices for such an outlandish amount. Also, it allows the dealer to get the pound of weed off their hands– relative to ounces, pounds of weed are much more likely to be noticed by police and therefore punished by the law harsher. So, the quicker they sell, the quicker they may re-up. So, for the normal crowd of people who do not but pounds, they pay one price. For those who are abnormal in how much they demand (say, the distributor for the connect for cocaine markets), they pay another price altogether. We see price discrimination in legal markets too, such as Sams Club. We can see that a regression model here IS NOT linear in parameters, since the slope of the line will change at different values of the function.</p>
</div>
<p>People often confuse this assumption with non-linear values of our independent variables as they relate to our outcome. They conflate nonlinear regression, which takes the form of <span class="math display">\[
y_{i} = \beta_{0}^{2} + \beta_{1}^{2} x_{i1} + \cdots + \beta_{K}^{2} x_{iK} + \epsilon_{i}
\]</span> with this <span class="math display">\[
y_{i} = \beta_{0} + \beta_{1} x_{i1}^{2} + \cdots + \beta_{K} x_{iK}^{2} + \epsilon_{i},
\]</span> or an OLS model with non-linear relations between the inputs and the outcomes. Let me explain why this is wrong, because as it turns out, we can indeed model curvature or non-linear predictions. I’ve already given an example of when we’d have a nonlinear realtionship in terms of our betas. Now I discuss non-linearities in terms of our <em>predictors</em>. Let’s say we wish to model how much someone can dead lift given some input of age. Let’s say the true population parameter for the OLS model is 6 (we ignore the constant for exposition purposes) <span class="math display">\[
y_{i} = 6x_{i}
\]</span> What is our value for 0? 0, since you’re not yet born. For age 10? 60. For age 30? 180. For age 80? 480. I think we can already see why this relationship being modeled would be silly: it presumes that the older you get, the stronger one is as a hard and fast rule. Which, generally speaking, is true… but we also know that at some point, as with all things, glory fades. Someone that was once strong and in shape will not (in general) always be that way because the body declines with the passage of time. How do we take this into account for our regression model, though?</p>
<p><span class="math display">\[
y_{i} = \beta_{1}(x_{i1} \times x_{i}) +  x_{i} \equiv y_{i} = \beta_{1}x^{2}_{i}
\]</span> We simply square the original value of age, keeping its linear form in the regression model. That way, when age 4 is input in the model, the number our regression model reads in the computer is 16. When age 10 is put into the model, it reads 100. Of course, as one would expect, there’s likely some critical point for this function, where people begin to be able to lift less given some values of age. We never know this of course, but OLS can be used to estimate it in the manner that we’ve done.</p>
<div id="exm-linlabor" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.3 (Labor Productivity) </strong></span>Another example of being able to account for non-linearities from economics is the idea of modeling how much produce one may produce given a set of labor inputs. Suppose we’re cooking cocaine. With just two people, you can get work done, but it won’t be a lot. With three people, you can do more, and more with each additional person. However, there’s an idea in economics called diminishing marginal returns for the factors of production (in this case labor). You may be able to cook a lot with 10 or 20 people, but when you have 40 or 50 people, at some point we end up producing less because there’s too many proverbial cooks in the kitchen. So, if we wished to model output of cocaine as a function of labor, we’d likely wish to square the “number of workers” part of our model since it does not make sense to expect production to increase perfectly linearly with every new human working with you. So you see, the linear in parameters assumption deals with our betas impact on our predictor variables, not the input values of our predictor variables.</p>
</div>
<p>Note that when we include such terms in our model, called <em>interaction terms</em> in econometrics (I may cover this more later, but <a href="https://www.youtube.com/watch?v=gjKzpxY4EqE">see this</a>), we must include the linear term in the model as well. In Stata, this would look something like <code>reg y c.labor##c.labor</code>. Under the hood, this includes in the model <code>reg y labor labor2</code>. Note that whenever you include an interaction term in your model, you MUST include the original terms. For example, say some unit is treated after some time point, and we want the treatment effect for that treated unit in that period when the treatment was active. One way of doing this in Stata is like</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode stata code-with-copy"><code class="sourceCode stata"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">clear</span> *</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>cls</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>u id <span class="fu">year</span> cigsale <span class="kw">using</span> <span class="st">"https://github.com/jgreathouse9/FDIDTutorial/raw/main/smoking.dta"</span>, <span class="kw">clear</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>g treat =<span class="fu">cond</span>(id==3,1,0) <span class="co">// yes if unit is california, else no</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>g <span class="kw">post</span> = <span class="fu">cond</span>(<span class="fu">year</span> &gt;=1989,1,0) <span class="co">// 1 if year &gt;= 1989, else no</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">regress</span> cigsale i.treat##i.<span class="kw">post</span>, <span class="kw">vce</span>(cl id)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>See how treat and post are both included in the model? This is an example of how to use an interaction term in a regression model.</p>
</section>
<section id="assumption-2-random-sample" class="level3" data-number="7.5.2">
<h3 data-number="7.5.2" class="anchored" data-anchor-id="assumption-2-random-sample"><span class="header-section-number">7.5.2</span> Assumption 2: Random Sample</h3>
<p>We next presume that we’ve collected a random sample of our population. The name <em>random</em> sample is something of an antiquated, romantic name to denote the idea that the sample we’ve collected is representative of the broader population we wish to map on to.</p>
<div id="exm-randomsampvm" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.4 (Virtual Menus) </strong></span>Suppose we wish to investigate the relationship between introducing all virtual menus at a restaurant (the kind you scan on your phone) to see if it increases how much money they make for [<em>random marketing reasons</em>]. We take all the restaurants in Buckhead and Sandy Springs in Atlanta as a sample, comparing the ones that did this intervention to the ones that didn’t do the intervention. We get a coefficient of 10,000, suggesting that this intervention increased money made, on average, by 10,000 dollars compared to the places that didn’t do this.</p>
<p>The issue with this idea is that our sample is not a random sample of the broader restaurant population. Sandy Springs and Buckhead, in particular, are among the wealthiest areas in Atlanta. We can’t generalize the effect of this intervention to the population (restuarants in Atlanta, say) becuase our units of intertested are decidedly <em>not</em> representative of Atlanta’s entire culinary scence. They have very specific customers that make a generalization to the bigger population a bad idea. If we only did care about wealthier areas, then this may be fine.</p>
</div>
<p>Another example can come from sports. Say we posit a relationship between height and skill at basketball. We take a sample of NBA players, run a few regressions for relevant metrics and have our software spit out coefficients at us. Can we generalize to the population? No!! The NBA is one of the most selective sports leagues on the planet. The NBA selects for height and skill, among other things. The worst player on the NBA bench is like a god from Olympus compared to the average human, physically and in terms of skill. They are <strong>not</strong> representative of even a human 2 standard deviations above the mean on all metrics we can think of.</p>
<p>So, we cannot use NBA players generalize to the population, <em>unless</em> of course we are concerned only with NBA players.</p>
</section>
<section id="assumption-3-no-perfect-collinearity" class="level3" data-number="7.5.3">
<h3 data-number="7.5.3" class="anchored" data-anchor-id="assumption-3-no-perfect-collinearity"><span class="header-section-number">7.5.3</span> Assumption 3: No Perfect Collinearity</h3>
<p>The simple way to think about this one is we cannot include redundant variables in our regressions. Suppose we wish to predict the ticket sales of NBA teams. In our regression, we include the number of games won as well as the number of games lost (2 variables). Well, these are mirror images of each other. The number of games you won is a direct function of the total games minus the number you lost, and the number you lost is a direct and perfect function of the total minus the number you won. Including both is completely redundant.</p>
<p>By extension, suppose we wish to compare women to men (say we wish to test that men earn more/less than women on average). We take data on 500 respondents who we’ve sampled randomly across a company. We have one column that denotes the respondent as male and the other as female. We cannot include both male and female columns in our models, these are perfect linear functions of one another. A female is necessarily not coded as male, and male is necessarily not coded as female.</p>
<p>Practically, this means we must choose when we use a categorical variable in our models. Say our regression includes age and gender as a predictor. If category 1 of gender is female and category 0 is male, then if the beta for “gender” is -30, we would interpret the beta for gender as “compared to men of the same age group, female respondents earn about 30 dollars less than men.” By extension, the coefficient for male (if we decided to include this group as the group of interest) would just be 30, with a similar interpretation in the other direction.</p>
</section>
<section id="assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0" class="level3" data-number="7.5.4">
<h3 data-number="7.5.4" class="anchored" data-anchor-id="assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0"><span class="header-section-number">7.5.4</span> Assumption 4: Strict Exogeneity: <span class="math inline">\(\mathbb{E}[\epsilon_{i} | x_{i1}, \ldots, x_{iK}] = 0\)</span></h3>
<p>Next we presume strict exogeneity. Formally, this means the average of our errors, given the set of covariates we’ve controlled for, is 0. It means our predictor variables may not be correlated with the error term. Note that the error term is different from the residuals: the error term includes unobserved characteristics that also may affect the outcome. In other words, we cannot omit important variables from our model.</p>
<p>For example, say we wish to see how the number of firefighters sent to a call affects the amount of damage from that fire. We wish to measure the association of of different variables. We conclude that there’s a positive relationship between number of firefighters sent and damage. Say, we use the number of trucks sent to a call to predict the damage in dollars for a sample size of 10,000,000 calls. We find from the bivariate model that every truck you send increases damage by 30,000 dollars. So, we elect to send <em>less</em> people to future calls. Is this a good idea? No!!!! People will die like that.</p>
<p>Presumably, the firefighters are not pouring gasoline on the fire, so perhaps we’ve <em>omitted</em> things from our model that might influence <em>both</em> how many people we send as well as fire damage. What else should we control for? Maybe, building size, building type, neighborhood income status, local temperature, and other relevant predictors to ensure that we are not blaming the outcomes on a spurious relationship.</p>
<p>Indeed, on some level we would expect for the size of the fire to be correlated with the number of people sent to fight it. Thus, when we do not control for other relevant factors, our coefficients, no matter how precise, suffer from <em>omitted variable bias</em>. This is what it means for strict exogeneity to hold, it’s the idea that no other variable exists that can “predict” the errors of the model were we to omit it from the model. Strict exogeneity is pretty much <strong>never</strong> met in real life, but it basically posits that there’s no other critical variable missing from our regression model that may explain our outcome. This is also why it matters to critically think about the variables one will use in their regression model <em>before</em> they run regressions (any regression).</p>
</section>
</section>
<section id="interpreting-ols" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="interpreting-ols"><span class="header-section-number">7.6</span> Interpreting OLS</h2>
<p>Okay now that we’ve ran through the implementation of OLS, we can do some examples in Stata about how to interpret the coefficients. Suppose we posit that age and gender affect a person’s income. We can simulate a dataset in Stata like this (again, put this in a do-file and run this code)</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode stata code-with-copy"><code class="sourceCode stata"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">clear</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">set</span> <span class="kw">obs</span> 10  <span class="co">// Number of observations</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">set</span> <span class="dv">seed</span> 1466</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> gender = (runiform() &gt; 0.5)  <span class="co">// Randomly assign 0 for women, 1 for men</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">label</span> <span class="kw">define</span> genderlbl 0 <span class="st">"Female"</span> 1 <span class="st">"Male"</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">label</span> <span class="kw">values</span> gender genderlbl</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="kw">tempvar</span> base_income</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> age = 18 + <span class="fu">floor</span>(runiform() * 47)  <span class="co">// Randomly assign age between 18 and 65</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> <span class="ot">`base_income'</span> = 1000 + rnormal(200, 50)  <span class="co">// Base income with variation, at least 1000</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> income = <span class="ot">`base_income'</span> + 50 * gender + 10 * age  <span class="co">// Add $50 for men, $10 for each year of age</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>When we use Stata’s OLS command <code>regress</code>, we get</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode stata code-with-copy"><code class="sourceCode stata"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>. <span class="kw">reg</span> income i.gender age</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>      Source |       SS           df       MS      Number <span class="kw">of</span> <span class="kw">obs</span>   =        10</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>-------------+----------------------------------   <span class="fu">F</span>(2, 7)         =     13.25</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>       Model |  138196.299         2  69098.1495   Prob &gt; <span class="fu">F</span>        =    0.0042</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    Residual |  36498.9799         7  5214.13998   R-squared       =    0.7911</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>-------------+----------------------------------   Adj R-squared   =    0.7314</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>       Total |  174695.279         9  19410.5865   Root MSE        =    72.209</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------------</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>      income | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>-------------+----------------------------------------------------------------</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>      gender |</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>       Male  |   73.17233   46.61623     1.57   0.160    -37.05755    183.4022</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>         age |   14.00385   2.870617     4.88   0.002     7.215918    20.79178</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>       <span class="dt">_cons</span> |   1002.007    138.102     7.26   0.000     675.4474    1328.566</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------------</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Okay, we’ve estimated OLS for our dataset, where we use age and gender as predictors of income. Here, we find that men (compared to women of the same age) make 73 more dollars, and that for every year of age, compared to people of the opposite gender, you make about 14 dollars more. Our t-statistic for men (or, the measure of how extreme the coefficeint is relative to the standard error) is 1.57, suggesting that this relationship is not “statistically significant”. Age, however, is, since its t-statistic is greater than 1.96. Our 95% Confidence Intervals, or our level of uncertainty, is that men may make less than women by 37 dollars or 183 dollars more than women of the same age.</p>
<p>However, there’s a problem with this analysis. In this example, I’ve made the sample size be 10. This is an extremely small sample size. I skipped over asymptotics, but let’s just say our regression coefficients become more accurate as our sample size increases.</p>
<div class="goals">
<div class="goals-header">
<p>Why?</p>
</div>
<div class="goals-container">
<p>We can even prove this by looking at my code to create this dataset. Look at the code <code>gen income = base_income + 50 * gender + 10 * age</code>. The multiplication by 50 and 10 mean that the true coefficients for men should be 50 and 10, but they’re not because the sample size is very small, making our estimates imprecise and confidence intervals wide.</p>
</div>
</div>
<p>Let’s do this example once more, but this time we’ll increase the sample size.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode stata code-with-copy"><code class="sourceCode stata"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">clear</span> * <span class="co">// clears our dataset</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">set</span> <span class="kw">obs</span> 100000  <span class="co">// Number of observations</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">set</span> <span class="dv">seed</span> 1466</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> gender = (runiform() &gt; 0.5)  <span class="co">// Randomly assign 0 for women, 1 for men</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="kw">label</span> <span class="kw">define</span> genderlbl 0 <span class="st">"Female"</span> 1 <span class="st">"Male"</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="kw">label</span> <span class="kw">values</span> gender genderlbl <span class="co">// assigns ==Female and 1=Male to gender</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="kw">tempvar</span> base_income</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> age = 18 + <span class="fu">floor</span>(runiform() * 47)  <span class="co">// Randomly assign age between 18 and 65</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> <span class="ot">`base_income'</span> = 1000 + rnormal(200, 50)  <span class="co">// Base income with variation, at least 1000</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="kw">gen</span> income = <span class="ot">`base_income'</span> + 50 * gender + 10 * age  <span class="co">// Add $50 for men, $10 for each year of age</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>cls</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> income i.gender age</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>will return the table</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode stata code-with-copy"><code class="sourceCode stata"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>      Source |       SS           df       MS      Number <span class="kw">of</span> <span class="kw">obs</span>   =   100,000</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>-------------+----------------------------------   <span class="fu">F</span>(2, 99997)     &gt;  99999.00</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>       Model |  1.9123e+09         2   956158609   Prob &gt; <span class="fu">F</span>        =    0.0000</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    Residual |   251149387    99,997  2511.56922   R-squared       =    0.8839</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>-------------+----------------------------------   Adj R-squared   =    0.8839</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>       Total |  2.1635e+09    99,999  21634.8824   Root MSE        =    50.116</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------------</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>      income | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>-------------+----------------------------------------------------------------</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>      gender |</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>       Male  |   50.46279     .31696   159.21   0.000     49.84155    51.08403</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>         age |   10.00862   .0116609   858.30   0.000     9.985761    10.03147</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>       <span class="dt">_cons</span> |   1199.554   .5286505  2269.09   0.000     1198.518    1200.591</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------------</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>See how these coefficients change? Now, the coefficients are closer to their true values because I’ve increased the sample size. Now, we see that men make 50 dollars more compared to women of the same age, and that people of the same gender tend to make 10 dollars more for every year older they are. We can look at our 95% Confidence Intervals and see that they’re tighter too.</p>
<p>We can do another example too. As ususal, in your Stata do-file, opened via the <code>doedit</code> command in the Stata terminal, do</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode stata code-with-copy"><code class="sourceCode stata"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">clear</span> *</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>cls</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="kw">sysuse</span> auto, <span class="kw">clear</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="kw">tab</span> foreign</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="kw">reg</span> price <span class="kw">weight</span> i.foreign</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>where we predict the price of a bar based on it being a foreign car (compared to domestic cars) and how much the car weighs. Here, we see that compared to domestic cars of the same weight, foreign rides cost around 3640 dollars more. Compared to cars of the same import status, a one pound increase in car weight suggests a 3 dollar increase in price. The constant here, for reference, is the price of a domestic car that weighs 0 pounds. Of course, the constant is not very meaningful in this instance, since 0 pound cars do not exist.</p>
</section>
<section id="summary" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Summary</h1>
<p>This undoubtably is the most weighty chapter, both in terms of mathematics and in terms of practical understanding. Regression is one of the building blocks for policy analysis, in addition to solid theoretical background and contextual knowledge of the policy being studied. It is the tool we use to examine how the change in one or many variables affects the change we see in another variable, while holding constant other factors that affect the outcome of interest.</p>
<p>The reason I chose to cover this first, in the first few weeks of the class instead of waiting until the end, is because I believe that the only way to <em>truly</em> understand regression is by use in applied examples. This is what you’ll wrestle with in your papers, when you (likely) use regression to estimate Difference-in-Differences models.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-dof" class="csl-entry" role="doc-biblioentry">
Pandey, Shanta, and Charlotte Lyn Bright. 2008. <span>“<span class="nocase">What Are Degrees of Freedom?</span>”</span> <em>Social Work Research</em> 32 (2): 119–28. <a href="https://doi.org/10.1093/swr/32.2.119">https://doi.org/10.1093/swr/32.2.119</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./correlation.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Correlation and Association</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./treatmenteffects.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Causal Inference</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>