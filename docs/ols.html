<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Econometrics for Policy Analysis - 6&nbsp; OLS Explained</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./treatmenteffects.html" rel="next">
<link href="./correlation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">OLS Explained</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Econometrics for Policy Analysis</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Syllabus: PMAP 4041, Fall 2024</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./module1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Policy Studies</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Mathematics and Econometric Theory</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basicprob.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Basic Probability Theory</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotic.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Asymptotic Theory</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./correlation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Correlation and Association</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">OLS Explained</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./treatmenteffects.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Causal Inference</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">Applied Research Methods</span>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#a-primer-on-data-types" id="toc-a-primer-on-data-types" class="nav-link active" data-scroll-target="#a-primer-on-data-types"><span class="toc-section-number">6.1</span>  A Primer on Data Types</a>
  <ul class="collapse">
  <li><a href="#ratio-variable" id="toc-ratio-variable" class="nav-link" data-scroll-target="#ratio-variable"><span class="toc-section-number">6.1.1</span>  Ratio Variable</a></li>
  </ul></li>
  <li><a href="#review-of-lines-and-functions" id="toc-review-of-lines-and-functions" class="nav-link" data-scroll-target="#review-of-lines-and-functions"><span class="toc-section-number">6.2</span>  Review of Lines and Functions</a></li>
  <li><a href="#arrivederci-algebra-ciao-derrivatives." id="toc-arrivederci-algebra-ciao-derrivatives." class="nav-link" data-scroll-target="#arrivederci-algebra-ciao-derrivatives."><span class="toc-section-number">6.3</span>  Arrivederci, Algebra, Ciao Derrivatives.</a>
  <ul class="collapse">
  <li><a href="#power-rule" id="toc-power-rule" class="nav-link" data-scroll-target="#power-rule"><span class="toc-section-number">6.3.1</span>  Power Rule</a></li>
  <li><a href="#chain-rule" id="toc-chain-rule" class="nav-link" data-scroll-target="#chain-rule"><span class="toc-section-number">6.3.2</span>  Chain Rule</a></li>
  </ul></li>
  <li><a href="#an-extended-example" id="toc-an-extended-example" class="nav-link" data-scroll-target="#an-extended-example"><span class="toc-section-number">6.4</span>  An Extended Example</a>
  <ul class="collapse">
  <li><a href="#list-the-data" id="toc-list-the-data" class="nav-link" data-scroll-target="#list-the-data"><span class="toc-section-number">6.4.1</span>  List the Data</a></li>
  <li><a href="#define-our-econometric-model" id="toc-define-our-econometric-model" class="nav-link" data-scroll-target="#define-our-econometric-model"><span class="toc-section-number">6.4.2</span>  Define Our Econometric Model</a></li>
  <li><a href="#write-out-the-objective-function" id="toc-write-out-the-objective-function" class="nav-link" data-scroll-target="#write-out-the-objective-function"><span class="toc-section-number">6.4.3</span>  Write Out the Objective Function</a></li>
  <li><a href="#simplify-the-objective-function" id="toc-simplify-the-objective-function" class="nav-link" data-scroll-target="#simplify-the-objective-function"><span class="toc-section-number">6.4.4</span>  Simplify the Objective Function</a></li>
  <li><a href="#take-partial-derivatives" id="toc-take-partial-derivatives" class="nav-link" data-scroll-target="#take-partial-derivatives"><span class="toc-section-number">6.4.5</span>  Take Partial Derivatives</a></li>
  <li><a href="#get-the-betas" id="toc-get-the-betas" class="nav-link" data-scroll-target="#get-the-betas"><span class="toc-section-number">6.4.6</span>  Get the Betas</a></li>
  <li><a href="#our-ols-line-of-best-fit" id="toc-our-ols-line-of-best-fit" class="nav-link" data-scroll-target="#our-ols-line-of-best-fit"><span class="toc-section-number">6.4.7</span>  Our OLS Line of Best Fit</a></li>
  </ul></li>
  <li><a href="#inference-for-ols" id="toc-inference-for-ols" class="nav-link" data-scroll-target="#inference-for-ols"><span class="toc-section-number">6.5</span>  Inference For OLS</a>
  <ul class="collapse">
  <li><a href="#goodness-of-fit-measures-for-ols" id="toc-goodness-of-fit-measures-for-ols" class="nav-link" data-scroll-target="#goodness-of-fit-measures-for-ols"><span class="toc-section-number">6.5.1</span>  Goodness of Fit Measures for OLS</a></li>
  </ul></li>
  <li><a href="#assumptions-of-ols" id="toc-assumptions-of-ols" class="nav-link" data-scroll-target="#assumptions-of-ols"><span class="toc-section-number">6.6</span>  Assumptions of OLS</a>
  <ul class="collapse">
  <li><a href="#assumption-1-linear-in-parameters" id="toc-assumption-1-linear-in-parameters" class="nav-link" data-scroll-target="#assumption-1-linear-in-parameters"><span class="toc-section-number">6.6.1</span>  Assumption 1: Linear in Parameters</a></li>
  <li><a href="#assumption-2-random-sample" id="toc-assumption-2-random-sample" class="nav-link" data-scroll-target="#assumption-2-random-sample"><span class="toc-section-number">6.6.2</span>  Assumption 2: Random Sample</a></li>
  <li><a href="#assumption-3-no-perfect-collinearity" id="toc-assumption-3-no-perfect-collinearity" class="nav-link" data-scroll-target="#assumption-3-no-perfect-collinearity"><span class="toc-section-number">6.6.3</span>  Assumption 3: No Perfect Collinearity</a></li>
  <li><a href="#assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0" id="toc-assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0" class="nav-link" data-scroll-target="#assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0"><span class="toc-section-number">6.6.4</span>  Assumption 4: Strict Exogeneity: <span class="math inline">\(\mathbb{E}[\epsilon_{i} | x_{i1}, \ldots, x_{iK}] = 0\)</span></a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="toc-section-number">7</span>  Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">OLS Explained</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>This is the chapter on regression. We begin by covering data types. Then, we review the idea of a function and how it relates to a line. After a review of dertivtives, we finally cover the computation of regression coefficients, inference, fit, and assumptions.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Throughout these notes, I link to tutorials on the calculus concepts that we employ here. Consult these links, if you’d like.</p>
</div>
</div>
<section id="a-primer-on-data-types" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="a-primer-on-data-types"><span class="header-section-number">6.1</span> A Primer on Data Types</h2>
<p>For any dataset you ever work with, you’ll likely have different variables (columns). The predictors for regression must be numeric, naturally. These take a few different types.</p>
<section id="ratio-variable" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="ratio-variable"><span class="header-section-number">6.1.1</span> Ratio Variable</h3>
<p>The most common kind is a ratio variable (a value we may express as a fraction/continuous variable), such as the employment rate.</p>
<table class="table">
<thead>
<tr class="header">
<th>State</th>
<th>Year</th>
<th>Employment Rate (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alabama</td>
<td>1990</td>
<td>55.3</td>
</tr>
<tr class="even">
<td>Alabama</td>
<td>1991</td>
<td>56.1</td>
</tr>
<tr class="odd">
<td>California</td>
<td>1990</td>
<td>62.1</td>
</tr>
<tr class="even">
<td>California</td>
<td>1991</td>
<td>61.5</td>
</tr>
<tr class="odd">
<td>Georgia</td>
<td>1990</td>
<td>58.4</td>
</tr>
<tr class="even">
<td>Georgia</td>
<td>1991</td>
<td>59.2</td>
</tr>
</tbody>
</table>
<p>A dummy variable is a binary variable that indicates the presence or absence of a characteristic. A dummy variable (also called an <em>indicator</em> or <em>categorical</em> variable) is a variable that takes on the values 0 or 1. For example, a simple dummy indicates whether a respondent in a survey is a man or a woman.</p>
<table class="table">
<thead>
<tr class="header">
<th>Respondent ID</th>
<th>Gender (Male=1, Female=0)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>2</td>
<td>0</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1</td>
</tr>
<tr class="even">
<td>4</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Dummies can also be used to capture unobserved variation across groups. For instance, when predicting homicide rates across states like Alabama, California, and Georgia for 1990 and 1991, we can include dummy variables for each state. These dummies help account for unique, stable characteristics of each state, such as culture, that are hard to measure directly.</p>
<table class="table">
<thead>
<tr class="header">
<th>State</th>
<th>Year</th>
<th>Alabama (1/0)</th>
<th>California (1/0)</th>
<th>Georgia (1/0)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alabama</td>
<td>1990</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>Alabama</td>
<td>1991</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>California</td>
<td>1990</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>California</td>
<td>1991</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>Georgia</td>
<td>1990</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>Georgia</td>
<td>1991</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>There is also a notion of an <em>ordinal</em> variable, where the data at hand must obey a specific order. Suppose we ask people in a survey how religious they are on a scale from 1 to 10, where 1=Atheist and 10=Extremely Religious. Here, order matters, because 1 has a very different meaning from 10 in this instance. An ordinal variable has a clear, ordered ranking between its values.</p>
<table class="table">
<thead>
<tr class="header">
<th>Respondent ID</th>
<th>Religiosity (1-10)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>3</td>
</tr>
<tr class="even">
<td>2</td>
<td>7</td>
</tr>
<tr class="odd">
<td>3</td>
<td>5</td>
</tr>
<tr class="even">
<td>4</td>
<td>10</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="review-of-lines-and-functions" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="review-of-lines-and-functions"><span class="header-section-number">6.2</span> Review of Lines and Functions</h2>
<p>In middle school, we learn about <a href="https://www.youtube.com/watch?v=VhokQhjl5t0">the basics of functions</a> in that when we plug in a number, we get another number in return. For <span class="math inline">\(2x=y\)</span> for example, if we plug in 2, we get 4. If we plug in 5, we get 10. If you’re at the grocery store and grapes are 1 dollar and 50 cents per pound, we just weigh the grapes and multiply that number by 1.5. This could take the form of <span class="math inline">\((0,0), (1,1.5), (2,3)\)</span>, and so on. In fact, we can represent these data points in a table like this</p>
<table class="table">
<thead>
<tr class="header">
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>1.5</td>
</tr>
<tr class="odd">
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>These points form a line, <a href="https://www.youtube.com/watch?v=K_OI9LA54AA">the equation for which being</a> <span class="math inline">\(y=mx+b\)</span>. We can also think of this line as a function, where we get some value of <span class="math inline">\(y\)</span> given some values for the other variables. Here, <span class="math inline">\(y\)</span> is how much we pay in total, <span class="math inline">\(m\)</span> is the change in how much we pay for every 1 pound of grapes bought, and <span class="math inline">\(b\)</span> is our value we pay if we get no grapes.</p>
<div class="cell" data-engine="jupyter" data-execution_count="1">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The way we find the <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> for a straight line is <a href="https://www.youtube.com/watch?v=MeU-KzdCBps">the “rise over run” method</a>, in this case</p>
<p><span class="math display">\[
m = \frac{y_2 - y_1}{x_2 - x_1} = \frac{3 - 0}{2 - 0} = \frac{3}{2} = 1.5
\]</span></p>
<p>For this case, the function for the line is <span class="math inline">\(y=1.5x\)</span>. For here, <span class="math inline">\(b=0\)</span> because in this case, how much we pay is a function of pounds of grapes only. We could add a constant/<span class="math inline">\(b\)</span>, though. Suppose we’d already spent 10 dollars, and now how much we spend is a function of both some previous constant level of spending, and new amount of grapes bought. Now, our function is <span class="math inline">\(y=1.5x+10\)</span>. Either way, constant or not, notice here how the line explains how much we pay perfectly (in other words, the line exactly matches the data points, as the line intersects perfectly with the dots on the plot). This means our <em>residuals</em>, or the difference between the prediction of the function and what we pay are 0.</p>
<p>For the above, <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span> maps on to the <span class="math inline">\(i\text{-th}\)</span> real data point for each repsective variable. So for example, <span class="math inline">\(y_2\)</span> for the above table is 1.5, as it represents the second row of the y variable, and <span class="math inline">\(x_1\)</span> is just 0. <span class="math inline">\(\hat{y}_i\)</span> (which we call y-hat) is the <span class="math inline">\(i\text{-th}\)</span> prediction point for the line/function. Here, the letter epsilon (<span class="math inline">\(\hat{\epsilon}_i\)</span>) is just a variable for the (Euclidean) distance from the <span class="math inline">\(i\text{-th}\)</span> predicted point to the observed point. For example, if the observed value for our first data point is 10 but <span class="math inline">\(\hat{y}_1=11\)</span>, then <span class="math inline">\(\hat{\epsilon}_1=10-11=-1\)</span>. Going forward, I will use the words “outcome (variable” or “dependent variable” to refer to the thing that we are studying the change of, and “predictors”, “covariates”, or “independent variables” to refer to the variables we think affect our outcome.</p>
<p>With all this being said though, the example above is very simplistic. This is a case where all the necessary information is known (price and weight). The algebra is so simple we that we intuitively understand that this is how we calculate expenses. But…. what if the data we have at hand are not nice and neat in terms of a function?</p>
<div class="cell" data-engine="jupyter" data-execution_count="2">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Take the idea of predicting <a href="https://raw.githubusercontent.com/danilofreire/homicides-sp-synth/master/data/df.csv">crime rates in Brazilian states</a> in the year 1990 using the inequality level as a predictor, or <a href="https://data.cdc.gov/Policy/Table-of-Gross-Cigarette-Tax-Revenue-Per-State-Orz/rkpp-igza/about_data">data on</a> the consumption of cigarettes in American states in the year 1980 using price as a predictor. Now for these examples, how do we calculate the rise over run? We would presume some <em>function</em> exists that generates the crime rate for that Brazilian state in that year, or that consumption level for that American state in that year.</p>
<p>However, is quite obvious that no deterministic function exists here for either of these cases. Indeed, when I draw a line between these datapoints for both examples, we find that non-zero residuals still exists (in other words, the model <em>imperfectly</em> predicts homicide rates and cigarette consumption levels in both datasets.) The residuals are plotted below the scatterplots above. They are simply the difference between the line and the observed value. For the left graph, the predicted homicide rate is around 35, but we in fact observe around a homicide rate of 18. So, our residual here is maybe -17, since our observed homicide rate is 17 less than our predicted one.</p>
<p>Before, we simply would throw our hands up, in a sense, and say that there’s no solution. Algebra has failed us in that we cannot find a function which perfectly explains these data points, as we could with the grape example above. But, we should not dismay. After all, this is what the real world is like, right? This is what it means to think in a multivariate way. Homicide rates and cigarette sales are <em>random variables</em> in the sense that they are produced by some latent probability function that is determined by a wide variety of factors.</p>
<p>The homicide rate or cigarette consumption rate in any state anywhere is not guaranteed. Sometimes in some states, homicides (or crime in general) is high, other times its low. Why? Well, some states are wealthier and some are poorer. Some states vary by racial compositions, or will differ by factors like age composition, income inequality, alcohol use, and gun ownership. Thus… some cities have high homicide rates, others have low homicide rates.</p>
<p>We can reason accordingly for cigarette consumption of American states. American states will vary on cigarette consumption based on a variety of factors. Natrually, one factor would be the price of cigarettes, as one might expect, since people tend to not want to buy more of a good as the price increases (<a href="https://sites.lsa.umich.edu/mje/2022/01/10/veblen-goods-why-sports-cars-and-diamonds-dont-obey-the-law-of-demand/">well… usually</a>.) The number of young people in that state may mean that younger people are risk takers and may be more likely to smoke than adults (or alternatively, young people may perceive smoking as something for older adults and smoke less). Levels of alcohol taxation may matter as well, since alcohol may be a substitute for tobacco, so states with higher taxation may smoke more, on average. Also, plain and simple measures like culture (and otehr unobservable things) may play a role. After all this, Indeed, it would be very unreasonable to expect to find a singular function that perfectly explains the variation in either of these datasets. The real world is simple too complicated to be thought of like that.</p>
<p>So, what can we do? We can’t find a function for the line that perfectly explains these data… But, how about we instead seek the best <em>possible</em> straight line? As it turns out, this is not a fool’s errand.</p>
</section>
<section id="arrivederci-algebra-ciao-derrivatives." class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="arrivederci-algebra-ciao-derrivatives."><span class="header-section-number">6.3</span> Arrivederci, Algebra, Ciao Derrivatives.</h2>
<p>To do this though, we’ve now reached a point in the course where simple algebra is no longer our best guide. We now <em>must</em> use calculus, specifically the basics of <a href="https://www.youtube.com/watch?v=9vKqVkMQHKk">derivatives</a> and optimization.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Okay, so here I’m <em>kind of</em> lying. You actually <strong>don’t</strong> <em>need</em> to say farewell to algebra (completely) to derive regression estimates, but <a href="https://www.youtube.com/watch?v=mIx2Oj5y9Q8&amp;t=16s">that process</a> “requires a ton of algebraic manipulation”. For those brave of heart who know algebra well, you can <em>probably</em> just watch the series of videos I just linked to and skip to <a href="https://jgreathouse9.github.io/GSUmetricspolicy/ols.html#inference-for-ols">this section</a> of the notes, but I <strong>do not</strong> recommend this. I find the calculus way via optimization a lot more intuitive.</p>
</div>
</div>
<p>Before we do this though, a primer on derivatives. The derivative is the slope of a curve/line given a very small change in the value of the function. One very useful property about derivatives is that when we set the first derivative of a function to 0 and solve for the variable (I do an example below), we reach a maximum or minimum point on <em>the original function</em>, <a href="https://www.youtube.com/watch?v=8aAU4r_pUUU">usually</a>. We use derivatives in the context of an optimization problem to minimize the squared residuals. Optimization problems take the form of <span class="math display">\[
\min_{\theta \in \Theta } f(\theta) \: \mathrm{ s.t. \:}  g(\theta) = 0, h(\theta) \leq 0,
\]</span> where there’s some function <span class="math inline">\(f(\theta)\)</span> (called the <em>objective function</em>) that is minimized (or sometimes maximized) over a set of <span class="math inline">\(g(\theta)\)</span> equality constraints and <span class="math inline">\(h(\theta)\)</span> inequality constraints.</p>
<section id="power-rule" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="power-rule"><span class="header-section-number">6.3.1</span> Power Rule</h3>
<p>Let’s do a real example of an optimization problem. Suppose we shoot a basketball while we stand on a 2 foot plateau, which produces a trajectory function of <span class="math inline">\(h(t)= −5t^2 +20t+2\)</span>. Here <span class="math inline">\(h(t)\)</span> is a function representing the ball’s height over time in seconds and the 2 represents the fact that we are standing 2 feet above flat ground. We can find the <em>maximum</em> height of the ball by taking the derivative of the original quadratic function and solving it for 0.</p>
<div class="cell" data-engine="jupyter" data-execution_count="3">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In this case, we use <a href="https://www.youtube.com/watch?v=BYTfCnR9Sl0">the power rule</a> for derivatives. The power rule for dertivatives is where we subtract the exponent value of a function by 1 and place the original value to be multiplied by the base number. For example, the derivative of <span class="math inline">\(y=2x^3\)</span> is just <span class="math inline">\(6x^2\)</span>, since <span class="math inline">\(3-1=2\)</span> and <span class="math inline">\(2 \times 3 = 6\)</span>. Here, we can apply this exact principle to obtain the derivative for the function of the ball’s trajectory:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; h(t) = -5t^2 + 20t + 2 = \\
&amp; \frac{d}{dt}(-5t^2) + \frac{d}{dt}(20t) + \frac{d}{dt}(2)= \\
&amp; t(5\times 2) +1(20) = \overbrace{-10t + 20}^{\text{Derivative}}
\end{aligned}
\]</span></p>
<p>We can then solve the derivative for 0.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; -10t + 20=0 \Rightarrow  \\
&amp; -10t = -20 \Rightarrow \\
&amp; \frac{-10t}{10} = \frac{-20}{-10} \Rightarrow \\
&amp; \boxed{t=2}
\end{aligned}
\]</span></p>
<p>We may now plug in the value of 2 to get the maximum height of the ball:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; -5t^2 + 20t + 2 \Rightarrow \\
&amp; -5(2)^2 + 20(2) + 2 \Rightarrow \\
&amp; -20+40+2=22
\end{aligned}
\]</span></p>
<p>So, the ball at its maximum is at 22 feet after 2 seconds (<em>strictly speaking</em>, if we wanted to be sure that this was a maximum instead of a minimum, we could take <a href="https://www.youtube.com/watch?v=-cW5hCsc9Yc">the second derivative</a>). It is the highest point our ball reaches.</p>
</section>
<section id="chain-rule" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="chain-rule"><span class="header-section-number">6.3.2</span> Chain Rule</h3>
<p>The next rule of differentiation to know is something called the chain rule. The chain rule is called that because of the <em>chain</em> reaction of one function affecting the value of another function, in mathematics ebing reffered to as a <a href="https://youtu.be/wUNWjd4bMmw?si=J4GbqUBTLvQ9DV-B"><em>composite function</em></a>. A common example in economics is where we wish to maximize profits. Consider this example below.</p>
<p>Suppose we are the manager of shipping for a drug cartel, sometimes called the <em>connect</em>. Our job as connect is to ship cocaine to Miami so that a wholeseller, or a <em>distributor</em>, may sell to smaller gangs who will sell to customers. The distributor is willing to give us 69,000 dollars per kilo. And, as the connect, it is within the cartel’s best interest, therefore ours, to maximize the profits that we make from selling to our distributor. However, we are not a monopoly. We cannot set the price of a kilo, given that other connects are potentially available in the near term. Thus, we must come up with an amount of cocaine to produce such that we maxmize our profits, given some price of cocaine.</p>
<p>The revenue we make per ki is simply <span class="math inline">\(r(x) = 69x\)</span>. In math, it just means the distributor pays us 69,000 per ki we sell them. If we could run a business without any costs, we’d do just that. But, we do indeed have costs. We have a set of fixed costs that comprises baseline expenditures of doing business with the distributor. For example, we may have transportation costs to move our product from home to Miami, a certain amount of money for fuel, and other costs that we simply cannot avoid to <em>not</em> pay in order to do business. We have variable costs which are things that we as the producers, in the very short run, have direct control over, say, how many drivers we have or how much plastic we use, the number of workers on our farms, and so on. And finally, we have marginal costs, or the costs that we bear as the business for each new unit of product produced. Our cost function for our purposes is <span class="math inline">\(c(x)=-0.25x^2+1.5x - 200\)</span>. The variable costs are 1500 dollars, and the fixed costs, or those we pay no matter what, are 200,000 dollars.</p>
<p>As I say above, we wish to maximize profit. In microeconomic theory, we’d say the profit we make is the difference between the total revenue from sales and total costs of production, or <span class="math inline">\(\pi=r(x)-c(x)\)</span>. The point on this fuction where we maximize profit is the point at which we cannot produce any more cocaine without decreasing our profits. In other words, all production is not created equally. In other words, if we wish to produce only one ki, a few people could likely do this themselves. But, for every new ki we wish to sell, we need to expand upon things such as the number of workers, the size of our farms, and so on. We may gradually produce more cocaine going from 40 to 50 workers. We may see a big increase of production from 500 to 1000 people… but at some point, we simply can’t have any more people to do work because people wouldn’t be able to fit on the land or do work on it. If we had 500,000 workers, not 1000, this likely would harm profits since there’d be too many cooks in the proverbial kithchen. People wouldn’t be able to move or fit in the area, so this massive increase of humans would actually slow down business and thus decrease profits.</p>
<p>Anyways, our profit is affected by these <em>two functions</em>. To maximize profit in this case, we have to have to differentiate with respect to each function within the profit function, <span class="math inline">\(\pi(x) = r(x) - c(x)\)</span>. To aid in this (and to presage how I will use this in regression below), we can think of the profit we make as being composed of an “inner” and “outer” function, comprised of the two we’ve discussed so far. Our objective function in this case looks like</p>
<p><span class="math display">\[
\underset{x}{\text{argmin}} \left(\underbrace{69x}_{r(x)} - \underbrace{0.25x^2 - 1.5x - 20}_{c(x)} \right)
\]</span></p>
<p>First, we denote our inner functions <span class="math inline">\(r(c)\)</span> and <span class="math inline">\(c(x)\)</span> <span class="math display">\[
\begin{aligned}
f_1 &amp;= r(x) = 69x \\
f_2 &amp;= c(x)=1.5x - 0.25x^2 - 200.
\end{aligned}
\]</span></p>
<p>Here are our outer functions: <span class="math display">\[
\pi(x) = f_{1} + f_{2}.
\]</span></p>
<p>We begin by differentiating the inner functions. <span class="math display">\[
\begin{aligned}
&amp;r'(x) = 69 \\
&amp;c'(x) = 1.5 - 0.50x.
\end{aligned}
\]</span></p>
<p>Why these results? Well, <span class="math inline">\(f_{1}\)</span> is just a linear function. It is the change of the function given a one unit increase in <span class="math inline">\(x\)</span>. Since it is linear, the slope is the same everywhere! So, it is just 69. The 200 goes away because it is fixed. We pay those costs anyhow. The derivative for <span class="math inline">\(f_{2}\)</span> comes from the power rule. We get <span class="math inline">\(1.5\)</span> for the first term for the reasons I just stated, then we differentiate the marginal costs term <span class="math inline">\(0.25 \times 2\)</span>, and so we get <span class="math inline">\(0.50\)</span>. The outer functions have no variables or exponents attached to them, so we leave them alone. The chain rule for <span class="math inline">\(x\)</span>, then, is the difference (in this case) of the derivatives of the individual inner functions themselves, or:</p>
<p><span class="math display">\[
\dfrac{\mathrm{d}\pi}{\mathrm{d}x} = \dfrac{\mathrm{d}r}{\mathrm{d}x}-\dfrac{\mathrm{d}c}{\mathrm{d}x}.
\]</span> This returns <span class="math inline">\(\pi'(x) = 67.5 - 0.50x.\)</span> In order to find the value of <span class="math inline">\(x\)</span> that profit is maximized at, we solve for 0, or <span class="math inline">\(67.5 - 0.50x = 0\)</span>. We compute:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \text{add to RHS} \\
&amp;67.5 = 0.5x \\
&amp; \text{and now we divide by the RHS} \\
&amp;x = \frac{67.5}{0.5} \\
&amp; \text{which returns our solution} \\
&amp;x \approx 135
\end{aligned}
\]</span></p>
<p>So, in order to maximize profits that we will earn from this distributor, we must sell 135 kis. We can prove this to be true by using optimization libraries in Python.</p>
<div class="cell" data-engine="jupyter" data-execution_count="4">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We will use these to solve for the value which minimizes the sum of residuals squared. This is known as <em>ordinary least squares</em> regression (OLS), also called <em>linear regression</em>, or simply just “regression”. OLS is the main estimator you’ll use for this class, and it is the main foundation of econometric analysis for public policy research.</p>
</section>
</section>
<section id="an-extended-example" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="an-extended-example"><span class="header-section-number">6.4</span> An Extended Example</h2>
<p>To introduce OLS, we can return to the equation of a line (<span class="math inline">\(y=mx+b\)</span>) where <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> are variables. Unlike the above examples where <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> were once known variables, now they are unknown quantities we must solve for. Below, <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> will take on the values of <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_0\)</span> respectively. With OLS, we have multiple predictors (typically), each of which affect the output of the function differently.</p>
<p>In the multivariable case, we take the <em>partial derivative</em> with resepct to each variable (that is, assuming that the other variables do not change). If this seems at all abstract to you, I will provide a detailed, clear derivation of the betas. Note that for all of the steps below, Stata, R, or Python does (and optimizes!) this process for you. I only provide this derivation so you have a source to refer to when you wish to know how and <em>why</em>, exactly, the machine returns the numbers that it returns to you.</p>
<p>Before we continue, let’s fix ideas. Suppose we wish to attend a nightclub and we wish to express how much we pay for that evening as a function (our outcome variable, a ratio level variable). At this nightclub, our outcome is a function of two things. We pay <em>some</em> one-time cost of money to enter, and then we pay <em>some</em> amount of money per new drink we buy (where number of drinks is also a ratio level variable). I say “<em>some</em>” because unlike the real world where we would know the price and entry fees by reading the sign, in this case we wish to estimate these values with only two known variables: how many drinks we bought and how much we paid.</p>
<div class="cell" data-engine="jupyter" data-execution_count="5">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="list-the-data" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="list-the-data"><span class="header-section-number">6.4.1</span> List the Data</h3>
<p>Say that we have data that looks like <span class="math inline">\((0, 30), (1, 35), (2, 40)\)</span>, where <span class="math inline">\(x\)</span>= number of drinks we buy (0,1,2) and <span class="math inline">\(y\)</span>=amount of money we spend that evening (30,35,40). In spreadsheet format, this looks like:</p>
<table class="table">
<thead>
<tr class="header">
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>30</td>
</tr>
<tr class="even">
<td>1</td>
<td>35</td>
</tr>
<tr class="odd">
<td>2</td>
<td>40</td>
</tr>
</tbody>
</table>
<p>If you want to, calculate the rise-over-run of these data points to derive <span class="math inline">\(m\)</span> and see what the answer might be in the end. Below though, we proceed by deriving what <span class="math inline">\(m\)</span> <em>must be</em>.</p>
<p><span class="math display">\[
m = \frac{35 - 30}{1 - 0} = \ldots
\]</span></p>
</section>
<section id="define-our-econometric-model" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="define-our-econometric-model"><span class="header-section-number">6.4.2</span> Define Our Econometric Model</h3>
<p>We begin by defining our model. That is, we specify our outcome and the variables which affect our outcome (the values we’re solving for, entry price and drink cost). Our model of how much we pay given some entry fees and additional drink costs looks like:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1x
\]</span></p>
<p>Here, <span class="math inline">\(y_i\)</span> is the total amount of money we spend that evening in dollars given the <span class="math inline">\(i\text{-th}\)</span> drink, <span class="math inline">\(\hat{\beta}_{0}\)</span> is how much we pay (also in dollars) to enter, <span class="math inline">\(\hat{\beta}_{1}\)</span> is how much we pay for the <span class="math inline">\(i\text{-th}\)</span> drink, and <span class="math inline">\(x\)</span> is the total number of drinks we get. Nothing <strong>at all</strong> about this is different, so far, from anything we’ve discussed above. I’ve simply substituted <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> with the Greek letter <span class="math inline">\(\beta\)</span> (“beta”) into the already familiar $y=mx+b y_i = _1x+_0 $.</p>
</section>
<section id="write-out-the-objective-function" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="write-out-the-objective-function"><span class="header-section-number">6.4.3</span> Write Out the Objective Function</h3>
<p>Now that we have our model, next let’s think about what the objective function would be. We already know that we wish to minimize the residuals of the OLS line. So, we can represent the objective function for OLS as <span class="math display">\[
\begin{aligned}
&amp; S = {\text{argmin}} \sum_{i=1}^{n} \hat{\epsilon}^{2}  \\
&amp; S = {\text{argmin}} \sum_{i=1}^{n} \left( y_i - \hat{y} \right)^{2} \\
&amp; \text{where } \hat{y} \text{ is defined as} \\
&amp; S = \underset{\hat{\beta}_0,\hat{\beta}_1}{\text{argmin}} \sum_{i=1}^{n} (y_i - (\overbrace{\hat{\beta}_0 + \hat{\beta}_1x}^{\text{Predictions}}))^2.
\end{aligned}
\]</span></p>
<p>Let’s explain what this means. The word “<span class="math inline">\(\text{argmin}\)</span>” here means “argument of the minimum”. It means that we are minimizing a function and seeking the values, or <em>arguments</em>, which minimize the ourtput of that function. The symbols underneath <span class="math inline">\(S\)</span>, <span class="math inline">\(\hat{\beta}_0,\hat{\beta}_1\)</span>, are the coefficients/inputs we are solving for. I call the objective function “<span class="math inline">\(S\)</span>” for “spending”, but we can call it any letter we’d like. We call the solutions <span class="math inline">\(\hat{\beta}_0,\hat{\beta}_1\)</span> <em>optimal</em> if they return the lowest possible values the function <span class="math inline">\(S\)</span> can produce. What values can <span class="math inline">\(S\)</span> produce? The sum of the squared residuals. The sigma symbol <span class="math inline">\(\sum_{i=1}^{n}\)</span> means we are adding up the <span class="math inline">\(i\text{-th}\)</span> squared residual to the <span class="math inline">\(n\text{-th}\)</span> data point/number of observations (in this case 3). This means that the line we compute will be as close as it can be to the observed data at every single data point.</p>
<p>Now that we’ve defined our terms, we can think about what the mathematics of the objective function is actually saying. <span class="math inline">\(\hat{\epsilon}_i\)</span> is our predicted residuals, as we’ve mentioned above. The middle formula re-expresses the residuals. This expression should look familiar. <a href="https://jgreathouse9.github.io/GSUmetricspolicy/basicprob.html#variance">Recall</a> the formula for the variance of a variable <span class="math inline">\(\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]\)</span> ?. Here, we are taking our real observations <span class="math inline">\(y\)</span> and subtracting some <em>expected value</em> <span class="math inline">\(\hat{y}\)</span> (y-hat) from it. Because we are minimizing the residuals, another way of thinking about OLS is that is the line that maximizes the explained variance given our independent variables.</p>
<p>As with the variance, one may ask <em>why</em> we are squaring the summed <span class="math inline">\(\hat{\epsilon}_i\)</span>, instead of say <a href="https://learningds.org/ch/04/modeling_loss_functions.html#mean-absolute-error">the absolute error</a>. <a href="https://www.youtube.com/watch?v=U46D7oEijlI">First of all</a>, minimizing the raw sum of the predicted residuals (that is, without squaring them) is a non-differentiable function. <a href="https://medium.com/@polanitzer/the-minimum-mean-absolute-error-mae-challenge-928dc081f031">We could</a> use the raw sum of the errors as an objective function (called the <em>mean absolute error</em> instead of the <em>mean squared error</em>), but <a href="https://github.com/chenxingwei/machine_learning_from_scratch/blob/master/algorithm/2.linearRegressionGradientDescent.md#gradient-descent-to-solve-linear-regression-with-mean-absolute-error-mae-loss-function">have fun doing that</a>, as due to the non-differentiable nature of the aboslute value function, we would need to use numerical methods to compute its solution such as gradient descent. By no means impossible… just computationally less tractable.</p>
<p>Using the squared residuals means that we are dealing a quadratic function which, as we did above, is easily differentiable. The squaring of residuals also penalizes worse predictions. Indeed, just as with the variance, all residuals <em>should</em> not be created equally. If the observed value is 20 but we predict 25, the residual is -5. Its squared residual is 25. But if the observed value is 40, and we predict 80, the “absolute” error is -40 and the squared error of is 1600. If we did not square them, we would be treating a residual of 5 as the same weight as a residual of 40. For proof, we can plot these</p>
<div class="cell" data-engine="jupyter" data-execution_count="6">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="simplify-the-objective-function" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="simplify-the-objective-function"><span class="header-section-number">6.4.4</span> Simplify the Objective Function</h3>
<p>First, we can substitute the real values as well as our model for prediction into the objective function. We already know the values x-takes. You either buy no drinks, 1 drink, or 2. So with this information, we can now find the amount of money we pay up front (<span class="math inline">\(\hat{\beta}_0\)</span>) and how much it costs for each drink (<span class="math inline">\(\hat{\beta}_1\)</span>) <span class="math display">\[
\begin{aligned}
S = &amp;\underbrace{(30 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 0))^2}_{\text{Term 1}} + \\
&amp;\underbrace{(35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1))^2}_{\text{Term 2}} + \\
&amp; \underbrace{(40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2))^2}_{\text{Term 3}}
\end{aligned}
\]</span></p>
</section>
<section id="take-partial-derivatives" class="level3" data-number="6.4.5">
<h3 data-number="6.4.5" class="anchored" data-anchor-id="take-partial-derivatives"><span class="header-section-number">6.4.5</span> Take Partial Derivatives</h3>
<p>To find the values of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, we take the partial derivatives of <span class="math inline">\(S\)</span> with respect to (w.r.t.) <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>. Here is a short sketch of how we do this: For simplicity, I break this into two sections, one section per coefficient. In this case, the chain rule and power rules for differentiation are our friends here. To hear more about combining the power rule and chain rule, <a href="https://www.youtube.com/watch?v=TI-j-fr6c4A">see here</a>. I define one set of inner and outer functions, beginning with the power rule for the outer function, and then taking the derivative for the inner function. First, we differentiate w.r.t. <span class="math inline">\(\hat{\beta}_0\)</span> (entry fees), then we do the same for <span class="math inline">\(\hat{\beta}_1\)</span> (drink fees).</p>
<ol type="1">
<li><strong>Partial derivative w.r.t. <span class="math inline">\(\hat{\beta}_0\)</span>:</strong></li>
</ol>
<p>First, we denote our inner functions. <span class="math display">\[
\begin{aligned}
f_1 &amp;= 30 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 0) \\
f_2 &amp;= 35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1) \\
f_3 &amp;= 40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2)
\end{aligned}
\]</span></p>
<p>Here are our outer functions: <span class="math display">\[
S = f_{1}^{2} + f_{2}^{2} + f_{3}^{2}.
\]</span></p>
<p>Now, following the chain rule, we have:</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_0} = \frac{\partial S}{\partial f_1} \cdot \frac{\partial f_1}{\partial \hat{\beta}_0} + \frac{\partial S}{\partial f_2} \cdot \frac{\partial f_2}{\partial \hat{\beta}_0} + \frac{\partial S}{\partial f_3} \cdot \frac{\partial f_3}{\partial \hat{\beta}_0.}
\]</span> The first step of the chain rule is using the power rule for the outer functions: <span class="math display">\[
\begin{aligned}
&amp;\frac{\partial S}{\partial f_1} = 2f_1 \\
&amp;\frac{\partial S}{\partial f_2} = 2f_2 \\
&amp;\frac{\partial S}{\partial f_3} = 2f_3.
\end{aligned}
\]</span> All we’ve done here is used the power rule, taking the 2 from the exponent and putting it on the outside of each term’s outer function. For the next step of the chain rule, we take the derivatives of each term’s inner function w.r.t. <span class="math inline">\(\hat{\beta}_0\)</span>: <span class="math display">\[
\begin{aligned}
\frac{\partial f_1}{\partial \hat{\beta}_0} &amp;= -1 \\
\frac{\partial f_2}{\partial \hat{\beta}_0} &amp;= -1 \\
\frac{\partial f_3}{\partial \hat{\beta}_0} &amp;= -1.
\end{aligned}
\]</span> Since the coefficient for <span class="math inline">\(-\hat{\beta}_0\)</span> is negative 1, each inner function’s derivative for <span class="math inline">\(\hat{\beta}_0\)</span> is just -1. Now, we substitute these inner derivatives back into the <span class="math inline">\(f_i\)</span> functions: <span class="math display">\[
\begin{aligned}
&amp;\frac{\partial S}{\partial \hat{\beta}_0} = 2f_1 \cdot (-1) + 2f_2 \cdot (-1) + 2f_3 \cdot (-1) \\
&amp;= -2f_1 - 2f_2 - 2f_3 \\
&amp;= -2(30 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 0)) \\
&amp; - 2(35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1)) \\
&amp;-2(40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2))
\end{aligned}
\]</span></p>
<p>Now, we simplify this expression. Let’s first expand each term by distributing the 2:</p>
<p><span class="math display">\[
= -60 + 2\hat{\beta}_0 - 70 + 2\hat{\beta}_0 + 2\hat{\beta}_1 - 80 + 2\hat{\beta}_0 + 4\hat{\beta}_1.
\]</span></p>
<p>Next I rearrange. I add parentheses for readability:</p>
<p><span class="math display">\[
= (-60 - 70 - 80) + (2\hat{\beta}_0 + 2\hat{\beta}_0 + 2\hat{\beta}_0) + (2\hat{\beta}_1 + 4\hat{\beta}_1).
\]</span> Finally, when we combine like terms, we end with <span class="math display">\[
= \boxed{-210 + 6\hat{\beta}_0 + 6\hat{\beta}_1}.
\]</span></p>
<ol start="2" type="1">
<li><strong>Partial derivative w.r.t. <span class="math inline">\(\hat{\beta}_1\)</span>:</strong></li>
</ol>
<p>Following from the above, here are our inner functions <span class="math display">\[
\begin{aligned}
f_1 &amp;= 30 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 0) \\
f_2 &amp;= 35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1) \\
f_3 &amp;= 40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2).
\end{aligned}
\]</span></p>
<p>Then the outer functions:</p>
<p><span class="math display">\[
S = f_1^2 + f_2^2 + f_3^2.
\]</span></p>
<p>Just as before, here’s the chain rule to get the partial derivative <span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_1} = \frac{\partial S}{\partial f_1} \cdot \frac{\partial f_1}{\partial \hat{\beta}_1} + \frac{\partial S}{\partial f_2} \cdot \frac{\partial f_2}{\partial \hat{\beta}_1} + \frac{\partial S}{\partial f_3} \cdot \frac{\partial f_3}{\partial \hat{\beta}_1}.
\]</span></p>
<p>First I apply the power rule to the the outer functions:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial S}{\partial f_1} &amp;= 2f_1 \\
\frac{\partial S}{\partial f_2} &amp;= 2f_2 \\
\frac{\partial S}{\partial f_3} &amp;= 2f_3.
\end{aligned}
\]</span></p>
<p>Next, we’ll do the inner functions:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial f_1}{\partial \hat{\beta}_1} &amp;= 0 \\
\frac{\partial f_2}{\partial \hat{\beta}_1} &amp;= -1 \\
\frac{\partial f_3}{\partial \hat{\beta}_1} &amp;= -2
\end{aligned}
\]</span> Note how <span class="math inline">\(\hat{\beta}_1\)</span> <strong>isn’t</strong> in term 1 since it is multiplied by 0 <span class="math display">\[
\underbrace{(30 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 0))^2}_{\text{Term 1}},
\]</span> its derivative must be 0. This means the first outer function goes away because <span class="math inline">\(f_1 \times 0=0\)</span>. So, I omit this from my derivation. Next, we’ll substitute these remaining two derivatives back into the expression for the outer function:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\frac{\partial S}{\partial \hat{\beta}_1} = 2f_2 \cdot (-1) + 2f_3 \cdot (-2) \\
&amp;= -2f_2 - 4f_3 \\
&amp;= 2(35 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 1)) - 4(40 - (\hat{\beta}_0 + \hat{\beta}_1 \cdot 2)).
\end{aligned}
\]</span> Now, we combine like terms: <span class="math display">\[
\frac{\partial S}{\partial \hat{\beta}_1} = -2(35 - \hat{\beta}_0 - \hat{\beta}_1) - 4(40 - \hat{\beta}_0 - 2\hat{\beta}_1).
\]</span> To simplify this expression, first we’ll expand each term by distributing the -2 and -4. Here is term 1:</p>
<p><span class="math display">\[
-2(35 - \hat{\beta}_0 - \hat{\beta}_1) = -70 + 2\hat{\beta}_0 + 2\hat{\beta}_1.
\]</span> Now we expand term 2: <span class="math display">\[
-4(40 - \hat{\beta}_0 - 2\hat{\beta}_1) = -160 + 4\hat{\beta}_0 + 8\hat{\beta}_1.
\]</span> Now, combine like terms: <span class="math display">\[
= (-70 + 2\hat{\beta}_0 + 2\hat{\beta}_1) + (-160 + 4\hat{\beta}_0 + 8\hat{\beta}_1).
\]</span> I move the constants outside of the parentheses. I add adding parentheses for readability:</p>
<p><span class="math display">\[
= (-70 - 160) + (2\hat{\beta}_0 + 4\hat{\beta}_0) + (2\hat{\beta}_1 + 8\hat{\beta}_1).
\]</span></p>
<p>After subtracting the constants and combining like terms once more, we’re left with<br>
<span class="math display">\[
\boxed{-230 + 6\hat{\beta}_0 + 10\hat{\beta}_1}.
\]</span></p>
</section>
<section id="get-the-betas" class="level3" data-number="6.4.6">
<h3 data-number="6.4.6" class="anchored" data-anchor-id="get-the-betas"><span class="header-section-number">6.4.6</span> Get the Betas</h3>
<p>Now we’ve set up our partial derivatives, so we can now return to normal scalar algebra to get our betas (in practice this is done with <em>matrix</em> algebra). Either way, we must solve a system of equations. The reason we must solve them as a system of equations is because we need for BOTH values to be the ones which minimize the distance between the model’s predictions and the observed data. Here are the equations we must solve for (note, we have two equations because we have two variables, but this can readily be extended to more variables): <span class="math display">\[
\begin{aligned}
&amp;\hat{\beta}_0 + \hat{\beta}_1 = 35 \\
&amp;6\hat{\beta}_0 + 10\hat{\beta}_1 = 230.
\end{aligned}
\]</span></p>
<p>Here I use <a href="https://youtu.be/V7H1oUHXPkg?si=evU3ldtIMa27uJvW">a method called substitution</a> to solve the system, but there are many such ways we can solve this. I choose to solve the first parital first since it is by far the easiest.</p>
<section id="solve-for-hatbeta_0" class="level4" data-number="6.4.6.1">
<h4 data-number="6.4.6.1" class="anchored" data-anchor-id="solve-for-hatbeta_0"><span class="header-section-number">6.4.6.1</span> Solve for <span class="math inline">\(\hat{\beta}_0\)</span>:</h4>
<p>The first step of substitution is to solve for one of our variables. <span class="math display">\[
\begin{aligned}
&amp;\hat{\beta}_0 + \hat{\beta}_1 = 35 \Rightarrow \\
&amp;\hat{\beta}_0 + (\hat{\beta}_1-\hat{\beta}_1) = 35-\hat{\beta}_1 \Rightarrow \\
&amp;\hat{\beta}_0 = 35 - \hat{\beta}_1
\end{aligned}
\]</span> Before I continue, notice a few interesting things about this: we know, by construction of the problem, that the entry fee <span class="math inline">\(\hat{\beta}_0\)</span> is some positive number. We also know <span class="math inline">\(\hat{\beta}_1\)</span> has to be less than 35. Imagine a <span class="math inline">\(\hat{\beta}_1 \geq 35\)</span>. This would mean <span class="math inline">\(\hat{\beta}_0\)</span> (our entry costs) would be kind of unusual. In this setting we at least get in for free or, even sillier, are <em>paid</em> to enter when <span class="math inline">\(\hat{\beta}_1 &gt; 35\)</span>, which does not make sense given the problem at hand. We also know that our entry fees <span class="math inline">\(\hat{\beta}_0\)</span> can’t be 35, because then this means that <span class="math inline">\(\hat{\beta}_1\)</span> would have to be 0, or a scenario where drinks are free (or, being paid to drink if <span class="math inline">\(\hat{\beta}_0 &gt; 35\)</span>).</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The reason I’m making such a fuss about this and carrying on with the example in this manner is because I want you to think of regression as a way of actually explaining the world, not just a set of arcane mathematical equations. This maps nicely on to the example we’ve developed.</p>
</div>
</div>
<p>Now, we continue to solve.</p>
</section>
<section id="substitute-hatbeta_0" class="level4" data-number="6.4.6.2">
<h4 data-number="6.4.6.2" class="anchored" data-anchor-id="substitute-hatbeta_0"><span class="header-section-number">6.4.6.2</span> Substitute <span class="math inline">\(\hat{\beta}_0\)</span>:</h4>
<p>Since we know the expression for the constant (the entry fee), we can plug it into the partial for <span class="math inline">\(\hat{\beta}_1\)</span>, <span class="math display">\[
\begin{aligned}
&amp;6\hat{\beta}_0 + 10\hat{\beta}_1 = 230. \\
&amp;6(35 - \hat{\beta}_1) + 10\hat{\beta}_1 = 230.
\end{aligned}
\]</span> Next, we distribute the 6 <span class="math display">\[
210 - 6\hat{\beta}_1 + 10\hat{\beta}_1 = 230
\]</span> and combine the terms <span class="math inline">\(- 6\hat{\beta}_1 + 10\hat{\beta}_1\)</span> together <span class="math display">\[
210 + 4\hat{\beta}_1 = 230.
\]</span> Next, we subtract 210 <span class="math display">\[
4\hat{\beta}_1 = 20.
\]</span> Finally, we divide by 4 <span class="math display">\[
\boxed{\hat{\beta}_1 = 5}.
\]</span></p>
<p>Now, we know our value for <span class="math inline">\(\hat{\beta}_1\)</span>!!! We know that for each drink we get, we pay 5 more dollars. Since we now know <em>this</em>, we substitute 5 into <span class="math inline">\(\hat{\beta}_0 + \hat{\beta}_1 = 35\)</span> where <span class="math inline">\(\hat{\beta}_1\)</span> is. Then, we have one equation to solve for, with our goal being to get the value of <span class="math inline">\(\hat{\beta}_0\)</span>. <span class="math display">\[
\hat{\beta}_0 + 5 = 35.
\]</span></p>
<p>Now, we simply subtract 5 from the RHS <span class="math display">\[
\boxed{\hat{\beta}_0 = 30}.
\]</span></p>
<p>The entry fee is 30 dollars.</p>
</section>
</section>
<section id="our-ols-line-of-best-fit" class="level3" data-number="6.4.7">
<h3 data-number="6.4.7" class="anchored" data-anchor-id="our-ols-line-of-best-fit"><span class="header-section-number">6.4.7</span> Our OLS Line of Best Fit</h3>
<p>So, our line of best fit is <span class="math inline">\(\hat{y} = 30 + 5x\)</span>. In social science, you’ll hear people throw around terms like “when we controlled for <em>this</em>” or “<em>adjusted for</em>” another variable, or “when we <em>hold constant</em> these set of variables”. This is what they mean by it. They, in the plainest language possible, mean that the dependent variable changes by some amount for every unitary increase in an independent variable, assuming the other values of the function don’t change. So here, assuming the club has a flat entry fee that does not change on a person to person basis, the amount the function changes by for every new drink is an increase of 5 dollars. Or, <em>compared</em> to the scenario where you only wanted to get in the club (and not drink at all, where <span class="math inline">\(x=0\)</span>), you spend 5 more dollars per each new drink you get.</p>
<p>At the outset, one may ask why we did this at all. Why bother with the partial derivative approach and the messy system of equations? The reason is firstly pedagogical. OLS was never derived for me in quite this manner in undergrad, so I believe you should see it done with a simple, tractable, familiar example. Additionally, when we understand how the partial derivative allows us to look at the change of an overall function given the change of just <em>one</em> variable in that function, we can quickly see how we can attempt to isolate the causal impact of <em>multiple</em> variables. Strictly speaking, we did just that right here in this derivation! We considered the impact of BOTH entry fees and the number of drinks we bought too (after all, suppose entry was free, then we have no constant).</p>
<div class="cell" data-engine="jupyter" data-execution_count="7">
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ols_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="inference-for-ols" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="inference-for-ols"><span class="header-section-number">6.5</span> Inference For OLS</h2>
<p>Now that we’ve conducted estimation, we can now conduct inference with these statistics we’ve generated. Indeed, this is the primary point of this at all, in a sense. We <em>want</em> to know if these estimates are different from some null hypothesis. To begin, recall the notation of <span class="math inline">\(\hat{\epsilon}_i\)</span> which denotes our residuals for the regression predictions. We can use this to generate the standard error/the uncertainty statistic associated with the respective regression estimate. We can begin with the residual sum of squares, calcualted like <span class="math inline">\(\text{RSS} = \sum (\hat{\epsilon}_{i})^2\)</span>. Put another way, it is all the variation <em>not</em> explained by our model. If <span class="math inline">\(RSS=0\)</span>, as was the case in the above example, then we have no need for inference since there’s nothing our model does <em>not</em> explain. We then can estimate the variance of the error like <span class="math inline">\(\hat{\sigma}^2 = \frac{\text{RSS}}{n - p}\)</span>, where <span class="math inline">\(n\)</span> is our number of observations and <span class="math inline">\(p\)</span> is our number of predictors (including the constant). We divide by <span class="math inline">\(n-p\)</span> because this takes into account our model’s residual degrees of freedom, or our model’s freedom to vary. Note as well that when <span class="math inline">\(n=p\)</span>, the error variance is not defined, meaning for OLS to be valid we need less predictors than observations. For a more detailed explanation of degrees of freedom, see <span class="citation" data-cites="dof">Pandey and Bright (<a href="#ref-dof" role="doc-biblioref">2008</a>)</span>.</p>
<p>With this, we can estimate the variance-covariance matrix as <span class="math display">\[
\text{Var}(\hat{\beta}) = \hat{\sigma}^2 (X^T X)^{-1}
\]</span> This allows us to compute our standard error: <span class="math inline">\(\text{SE}(\hat{\beta}_j) = \sqrt{\text{Var}(\hat{\beta})_{jj}}\)</span>.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Again, I will never ever ask you to calculate the standard error or t-statistic without the necessary coefficients to calculate it algebraically. The reason I’m showing you this is just so you have a very clear sense of where these numbers are coming from,</p>
</div>
</div>
<p>As we’ve discussed with normal descriptive statistics/classic hypothesis testing, we can also compute confidence intervals for these regression estimates. The formula for this should appear quite familiar <span class="math display">\[
\beta_j \pm t_{\alpha/2, \text{df}} \cdot \text{SE}(\beta_j)
\]</span></p>
<p>Here, <span class="math inline">\(\beta_j\)</span> is the coefficient of our model, <span class="math inline">\(t\)</span> is our test statistic (1.96 ususally), <span class="math inline">\(\alpha\)</span> is our acceptance region (0.05 in most cases if we want a 95% confidence interval), SE is our standard error as we’ve computed it above and df is our degrees of freedom. Then, we can interpret our coefficients accordingly. Suppose a confidence interval for a coefficient 6.7 lies within 6.06 and 16.44. We interpret this as follows: given the dataset and input values (and assuming the OLS assumptions are valid, which we will detail below), we are 95% confident that the true value for that coefficient lies within the range of 6.06 and 16.44. In other words, there’s a “width” of this confidence interval by about 10.</p>
<p>Just as we discussed in the preceding chapters, OLS’s estimates (conditional on the assumptions being met, which we will cover after this) is only justified, asymptotically, based on the law of large numbers and CLT. In other words, as n tends to infinity, <span class="math inline">\(\lim\limits_{n\to \infty}\)</span>, our betas will converege to the true population value and the standard errors will shrink. Ergo, as these shrink, the confidence intervals will tighten, meaning our estimates will be more precise. A practical consequence of this is that as a very general rule, having more observations in your dataset makes your OLS estimates more precise and less biased.</p>
<section id="goodness-of-fit-measures-for-ols" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="goodness-of-fit-measures-for-ols"><span class="header-section-number">6.5.1</span> Goodness of Fit Measures for OLS</h3>
<p>We typically thhink of two goodness of fit statistics when using OLS, the R-squared statistics and the Root Mean Squared Error</p>
<p><span class="math display">\[
\begin{aligned}
R^2 &amp;= 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}} &amp; \text{RMSE} &amp;= \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}.
\end{aligned}
\]</span> Here, for R-squared, we have two terms: first, <span class="math inline">\(\text{SS}_{\text{res}} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)</span> is the sum of squared residuals. <span class="math inline">\(\text{SS}_{\text{res}}\)</span> quantifies the amount of variance unexplained by the independent variables in the model, and <span class="math inline">\(\text{SS}_{\text{tot}}\)</span> is just the total amount of variance of <span class="math inline">\(y\)</span>. Think of R-squared as a ratio/percentage of how well the model explains our outcomes. An R-squared of 1 reflects the simple example we derived above, where the model perfectly explains the variation of our outcomes. R-squared usually scales with the amount of predictors in the model (that is, as we add more variables, the R-squared will increase). However, I think the best way to think about R-squared is a measure of how well our model does, compared to the average of our outcomes. In the nightclub example, if we just took the average of our outcomes, we’d guess for that evening, you’d spend 28.3 dollars. But, in this case, the model significantly outperforms this since it explains the variation perfectly. Note, that it is possible to have a negative R-squared. It is very rare, and it basically means that your model does a worse job than the simple average of the outcomes. I’ve seen this in my work, but I’ve only encountered it in the wild maybe twice.</p>
<p>The Root Mean Squared error, or RMSE, is exactly as it sounds: it is the square root of our average of our <span class="math inline">\(\text{SS}_{\text{res}}\)</span>. For the tobacco dataset for example that I include above, our sum of residuals squared is 26015.2655. When we divide this by our residual degrees of freedom, or <span class="math inline">\(38-1\)</span>, we get 703.115283. When we take the square root, we get 26.516. In English, this simply means that when we use price to explain consumption for the year 1980, the model is off, on average, by about 27 packs. Considering that the average cigarette consumption in 1980 was 137.6308, being off by 26 packs isn’t bad, and it suggests, as one would expect, that we’ve explained our dataset fairly well. As RMSE approaches 0, we explain our variation better, with an RMSE of 0 being perfection. Note that other goodness of fit metrics do exist; however, these are the most common ones you’ll encounter.</p>
</section>
</section>
<section id="assumptions-of-ols" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="assumptions-of-ols"><span class="header-section-number">6.6</span> Assumptions of OLS</h2>
<p>Keep in mind, despite all the detail we’ve discussed so far, do not lose sight of the larger picture: OLS is simply a mathematical estimation method. Its <em>validity</em> for explaining the external world (aside from having quality data) relies on a few assumptions (collectively called the Gauss-Markov assumptions) being defensible. I say defensible instead of true because practically they are never true. After all, this is statistics: almost all of statistics is true. All statistical research (pretty much, outside of simulations) is at least partly wrong because we live in a probalistic world where we don’t have all the answers. In other words, the assumptions are only as valid as we can defend for them. Below, I detail them.</p>
<section id="assumption-1-linear-in-parameters" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="assumption-1-linear-in-parameters"><span class="header-section-number">6.6.1</span> Assumption 1: Linear in Parameters</h3>
<p>The very first assumption is that the parameters for our model are linear. The classic regression model for OLS is</p>
<p><span class="math display">\[
y_{i} = \beta_{0} + \beta_{1} x_{i1} + \cdots + \beta_{K} x_{iK} + \epsilon_{i}.
\]</span></p>
<p>We call this a linear model. Why? How do we know if it’s a linear relationship, and what might violations of this look like? Let’s say we’re buying weed. Say the price per quarter ounce is 80 dollars. The impact of <span class="math inline">\(\beta_{1}\)</span> is the same everywhere in the function, <span class="math inline">\(y=80x\)</span>. But step back and ask ourselves, from the seller’s persepctive, if this makes sense: does it make sense for weed to cost the same for every weight amount? No! Why not? Well, for one, let’s say you’re selling a full gram or pound of weed. That’s <strong><em>so much</em></strong> weed that weed(wo)men/people will charge much much more for lone individuals who wish to buy this much. So while it may be 80 for a quarter ounce, it’ll now be, say, 900 per pound. In fact, we could express this as a piecewise function</p>
<p><span class="math display">\[
\beta_{j} =
\begin{cases}
    80 \text{ if } x &lt; 1 \\
    900 \text{ if } x &gt; 1. \\
\end{cases}
\]</span></p>
<p>Why might this be done? Firstly, that’s so much more product than the average person could smoke or use. So, anyone interested in this would need to pay premium prices for such an outlandish amount. Also, it allows the dealer to get the pound of weed off their hands– relative to ounces, pounds of weed are much more likely to be noticed by police and therefore punished by the law harsher. So, the quicker they sell, the quicker they may re-up. So, for the normal crowd of people who do not but pounds, they pay one price. For those who are abnormal in how much they demand (say, the distributor for the connect for cocaine markets), they pay another price altogether. We see price discrimination in legal markets too, such as Sams Club. We can see that a regression model here IS NOT linear in parameters, since the slope of the line will change at different values of the function.</p>
<p>People often confuse this assumption with non-linear values of our independent variables as they relate to our outcome. They conflate nonlinear regression <span class="math display">\[
y_{i} = \beta_{0}^{2} + \beta_{1}^{2} x_{i1} + \cdots + \beta_{K}^{2} x_{iK} + \epsilon_{i}
\]</span> with <span class="math display">\[
y_{i} = \beta_{0} + \beta_{1} x_{i1}^{2} + \cdots + \beta_{K} x_{iK}^{2} + \epsilon_{i},
\]</span> or an OLS model with non-linear relations between the inputs and the outcomes. Let me explain why this is wrong, because as it turns out, we can indeed model curvature. I’ve already given an example of when we’d have a nonlinear realtionship in terms of our betas. Now I discuss non-linearities in terms of our predictors. Let’s say we wish to model how much someone can dead lift given some input of age. Let’s say the true population parameter for the OLS model is 6 (we ignore the constant for exposition purposes) <span class="math display">\[
y_{i} = 6x_{i}
\]</span> What is our value for 0? 0, since you’re not yet born. For age 10? 60. For age 30? 180. For age 80? 480. I think we can already see why this relationship being modeled would be silly: it presumes that the older you get, the stronger one is. Which, generally speaking, is true… but we also know that at some point, as with all things, glory fades. Someone that was once strong and in shape will not (in general) always be that way because the body declines with the passage of time. How do we take this into account for our regression model, though?</p>
<p><span class="math display">\[
y_{i} = \beta_{1}(x_{i1} \times x_{i}) +  x_{i} \equiv y_{i} = \beta_{1}x^{2}_{i}
\]</span> We simply square the original value of age, keeping its linear form in the regression model. That way, when age 4 is input in the model, the number our regression model reads in the computer is 16. When age 10 is put into the model, it reads 100. Of course, as one would expect, there’s likely some critical point for this function, where people begin to be able to lift less given some values of age.</p>
<p>Another example of being able to account for non-linearities from economics is the idea of modeling how much produce one may produce given a set of labor inputs. Suppose we’re cooking cocaine. With just two people, you can get work done, but it won’t be a lot. With three people, you can do more, and more with each additional person. However, there’s an idea in economics called diminishing marginal returns for the factors of production (in this case labor). You may be able to cook a lot with 10 or 20 people, but when you have 40 or 50 people, at some point we end up producing less because there’s too many proverbial cooks in the kitchen. So, if we wished to model output of cocaine as a function of labor, we’d likely wish to square the “number of workers” part of our model since it does not make sense to expect production to increase perfectly linearly with every new human working with you. So you see, the linear in parameters assumption deals with our betas impact on our predictor variables, not the input values of our predictor variables.</p>
</section>
<section id="assumption-2-random-sample" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="assumption-2-random-sample"><span class="header-section-number">6.6.2</span> Assumption 2: Random Sample</h3>
<p>We next presume that we’ve collected a random sample of our population. The name <em>random</em> sample is something of an antiquated, romantic name to denote the idea that the sample we’ve collected is representative of the population we wish to map on to. Suppose we wish to investigate the relationship between introducing all virtual menus at a restaurant (the kind you scan on your phone) to see if it increases how much money they make for [<em>random marketing reasons</em>]. We take all the restaurants in Buckhead and Sandy Springs in Atlanta as a sample, comparing the ones that did this intervention to the ones that didn’t do the intervention. We get a coefficient of 10,000, suggesting that this intervention increased money made, on average, by 10,000 dollars compared to the places that didn’t do this. The issue with this idea is that our sample is not a random sample of the population. Sandy Springs and Buckhead, in particular, are among the wealthiest areas in Atlanta. We can’t generalize the effet of this intervention to the population (restuarants in Atlanta, say) becuase our units of intertested are decidedly <em>not</em> representative of Atlanta’s entire culinary scence. They have very specific customers that make a generalization a bad idea.</p>
<p>Another example can come from sports. Say we posit a relationship between height and skill at basketball. We take a sample of NBA players, run a few regressions for relevant metrics and have our software spit out coefficients at us. Can we generalize to the population? No!! The NBA is one of the most selective sports leagues on the planet. The NBA selectd for height and skill, among other things. The worst player on the NBA bench is like a god from Olympus compared to the average human, physically and in terms of skill. They are not representative of even a human 2 standard deviations above the mean. So, we cannot use NBA players generalize to the population, <em>unless</em> of course we are concerned only with NBA players. The same would apply to the first example: if we care only about the high-income restaurants in the city, then that’s great, but assuming we wish to generalize more broadly, we will need more data from other, more diverse units that have more information encoded in their outcome about the sample.</p>
</section>
<section id="assumption-3-no-perfect-collinearity" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="assumption-3-no-perfect-collinearity"><span class="header-section-number">6.6.3</span> Assumption 3: No Perfect Collinearity</h3>
<p>The simple way to think about this one is we cannot include redundant variables in our regressions. Suppose we wish to predict the ticket sales of NBA teams. In our regression, we include the number of games won as well as the number of games lost (2 variables). Well, these are mirror images of each other. The number of games you won is a direct function of the total games minus the number you lost, and the number you lost is a direct and perfect function of the total minus the number you won.</p>
<p>By extension, suppose we wish to compare women to men (say we wish to test that men earn more/less than women on average). We take data on 500 respondents who we’ve sampled randomly across a company. We have one column that denotes the respondent as male and the other as female. We cannot include both male and female columns in our models, these are perfect linear functions of one another. A female is necessarily not coded as male, and male is necessarily not coded as female. Practically, this means we must choose when we use a categorical variable in our models. Say our regression includes age ang gender as a predictor. If category 1 of gender is female and category 0 is male, then if the beta for “gender” is -30, we would interpret the beta for gender as “holding constant age/compared to men of the same age group, female respondents earn about 30 dollars less than men.” By extension, the coefficient for male (if we decided to include this group as the group of interest) would just be 30, with a similar interpretation in the other direction.</p>
</section>
<section id="assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0" class="level3" data-number="6.6.4">
<h3 data-number="6.6.4" class="anchored" data-anchor-id="assumption-4-strict-exogeneity-mathbbeepsilon_i-x_i1-ldots-x_ik-0"><span class="header-section-number">6.6.4</span> Assumption 4: Strict Exogeneity: <span class="math inline">\(\mathbb{E}[\epsilon_{i} | x_{i1}, \ldots, x_{iK}] = 0\)</span></h3>
<p>Next we presume strict exogeneity. Formally, this means the average of our errors, given the set of covariates we’ve controlled for, is 0. It means our predictor variables may not be correlated with the error term. In other words, we cannot omit important variables from our model. For example, say we wish to see how the number of firefighters sent to a call affects the amount of damage from that fire. We conclude that there’s a positive relationship between number of firefighters sent and damage, so we elect to send less people to future calls, using a simple bivariate model as justification. Is this a good idea? No!!!! People will die like that. Presumably, the firefighters are not pouring gasoline on the fire, so perhaps we’ve <em>omitted</em> things from our model that might influence both how many people we send as well as fire damage. What else should we control for? Maybe, building size, building type, neighborhood income status, temperature, and other relevant predictors to ensure that we are not blaming the outcomes on a spurious relationship. Indeed, on some level we would expect for the size of the fire to be correlated with the number of people sent to fight it. Thus, when we do not control for other factors, our coefficients, no matter how precise, suffer from <em>omitted variable bias</em>. Strict exogeneity is pretty much never met in real life, but it basically posits that there’s no other critical variable missing from our regression model that may explain our outcome. This is also why it matters to critically think about the variables one will use in their regression model <em>before</em> they run regressions.</p>
</section>
</section>
<section id="summary" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Summary</h1>
<p>This undoubtably is the most weighty chapter, both in terms of mathematics and in terms of practical understanding. Regression is one of the building blocks for policy analysis, in addition to solid theoretical background and contextual knowledge of the policy being studied. The reason I chose to cover this first, in the first few weeks of the class instead of waiting until the end, is because I believe that the only way to truly understand regression is by use in applied examples. This is what you’ll explore more of in your papers.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-dof" class="csl-entry" role="doc-biblioentry">
Pandey, Shanta, and Charlotte Lyn Bright. 2008. <span>“<span class="nocase">What Are Degrees of Freedom?</span>”</span> <em>Social Work Research</em> 32 (2): 119–28. <a href="https://doi.org/10.1093/swr/32.2.119">https://doi.org/10.1093/swr/32.2.119</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./correlation.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Correlation and Association</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./treatmenteffects.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Causal Inference</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>