[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics for Policy Analysis",
    "section": "",
    "text": "1 Syllabus: PMAP 4041, Fall 2024\nEvery day, governments pass laws/public policy to affect some outcome of interest. Policy usually touches thousands if not millions of people. From traffic-circles to pop/sugar sweetened beverge taxes, vaccine mandates and universal pre-k programs, cannabis legalization to minimum wages, public policy impacts us all from birth to death.\nPolicy is never self justifying. It demands evaluation. If California bans tobacco smoking in public, or if New York City implements gun control, presumably we would agree these likely impact outcomes like tobacco use or homicide rates, ideally decreasing both of them.\nIf California’s anti-tobacco policy didn’t affect smoking rates at all (or worse, if more people began to smoke) or if gun control has 0 impact on homicide rates (or increased them, paradoxically), then surely these could not be justified in the very first place. Before we continue, understand fundamentally these outcomes being affected are the point. The only reason that we, as a society, do policy is precisely because we think policy affects (or should affect) people somehow. If political science studies “who gets what where”, one summation of policy studies might be “what works?” But what policies should we care about? How can we know if they work? This is the starting point for empirical policy analysis. This class discusses the theory and process for how statistical analysis of data may be used to answer policy questions."
  },
  {
    "objectID": "index.html#course-philosophy-and-structure",
    "href": "index.html#course-philosophy-and-structure",
    "title": "Econometrics for Policy Analysis",
    "section": "1.1 Course Philosophy and Structure",
    "text": "1.1 Course Philosophy and Structure\nI believe the best way to demonstrate knowledge of policy analysis is through writing. As such, there will be no quizzes or in-class exams. Why? It is unrealistic. In real life, rarely do we have an hour and 30 minutes or a ten minute quiz window on icollege to write a full summation of our ideas or think through a question. Typically, we have much more time and resources to help us. In fact, proper use of resources, instead of memory, is what makes a good analyst. Good analysts do not need to remember everything, but they do need to be good at finding answers. The class is broken up into two sections: in the first section, we go over basic probability, correlation, and regression, as these are the tools you’ll work with to actually be able to write your paper. The remainder of the class covers other topics in research for policy analysis.\n\n1.1.1 Paper Breakdown\nIn this spirit, I give to you one assignment. Specifically, you will write a paper where you will apply the statistical concepts we cover to answer questions about a real, existing policy. You will turn in each sub-section of the paper to me well before the first draft is due. I will give you feedback on each section, which I expect you to improve upon with subsequent iterations. So then, when it is time to turn in the full first draft, you will be able to incorporate my previous suggestions. I expect you to send me your section/paper (or at least ask to meet with me about it) as often as you need. To facilitate this, I have three potential datasets you may use.\n\nThe first dataset deals with the impact of Texas’s abortion restrictions on the number of births.\nThe second dataset deals with the impact of a tax on pop/soda on the employment rate in San Francisco.\nThe third dataset deals with the impact of terrorism on the GDP per Capita of the Basque Country,Spain from 1975-1997.\n\nShould there be another policy you wish to study, or another research question you have in mind, you may do this instead, subject to public data on that question existing and my permission. Here is an example paper of how yours should look, more or less. The text file versions of these three datasets are now online (as of August 29, 2024)\nThe paper consists of 5 sections: The intro, data, methods, results, and discussion sections. Each section is worth 4% each, meaning the sections themselves are worth 20%. The full first draft, then, is 20%. Here is what each section must look like:\n\nIntro: Describes the general problem of interest. Should answer “Who did the policy/intervention, what policy/intervention was done, when did the policy happen, where did the policy happen, and how is the policy exepected to affect whatever outcome we care about?” I.e., if you are doing option 3, you will need to do some reading on why we would expect terrorism to affect a local economy. If you are doing option 2, you will need to read on why a tax on pop might affect local levels of employment. Also, say why having answers to how this policy affects things matters– why does the (wo)man on the street care about how abortion bans affect birth rates?\nData: This section describes the dataset you’re using. Say how many units of analysis there are (i.e., how many unique cities, countries, states, counties are in your dataset). Say the time period the data was collected from, if applicable. Say when the treatment began and for which state/entity. Say how each variable was measured (i.e., if the main variable is education, is it a 0 1 variable for college educated or not, or is it 1 … 16 to represent the years of education).\nMethods: Describe your statistical model. For most people, this presumably will be difference-in-differences.\nResults: Summarize your results. Did the policy work or not? If so, by how much? was the effect size small or large? How uncertain are we about the estimates?\nDiscussion/Conclusion: talk about what your results mean and why they matter.\n\n\n\n1.1.2 Class Grade Breakdown\n\n40% of your grade comes from the best first version of the paper. The 5 sections themselves are 4% as I say above, and the full best first version is 20%.\n55% for the final paper (a revision of the above, not broken into sections), and\n5% for attendence.\nI will drop the first submission if your final paper is an improvement."
  },
  {
    "objectID": "index.html#additional-details",
    "href": "index.html#additional-details",
    "title": "Econometrics for Policy Analysis",
    "section": "1.2 Additional Details",
    "text": "1.2 Additional Details\n\nIf I feel the concept is important, it’ll be in the lecture notes or we will discuss it. I will also sometimes assign external readings to be done before class.\nThere is no required textbook (aside from this one!) for this course. Various free textbooks exist that go over the statistical material that we do such as Introductory Econometrics with R, Introductory Statistics, Intro to Modern Statistics, Regression and Other Stories, Intro to Econometrics, Intro to Political Science Research Methods, and many others. The Policy Department at Georgia State also recommends Introduction to Research Methods or Research Methods for the Social Sciences. The corresponding lecture will focus on the content that each respective chapter covers. Note that these books cover different aspects of the course in different levels of depth (Gelman’s book Regression and Other Stories is obviously mainly about regression, one of the last math topics we cover, whereas the others are more rudimentary).\nThe same is true for software– I don’t care which of these you use, but the only ones I know well are Stata, Python, and (to a lesser degree) R. For this classroom, there’s already Stata on the computer, and I’ll provide code blocks frequently to illustrate concepts and ideas. For Stata users, Statalist is a great resource for Stata. R also is backed by a vast statistician community."
  },
  {
    "objectID": "index.html#helpful-notes-from-me",
    "href": "index.html#helpful-notes-from-me",
    "title": "Econometrics for Policy Analysis",
    "section": "1.3 Helpful Notes from Me",
    "text": "1.3 Helpful Notes from Me\n\nSun Tzu said every battle is won before it is fought. To reverse the perspective, as Ben Franklin said, if you fail to prepare, prepare to fail. The fact that the paper is the primary assignment you have, in effect, means that I expect quality analyses written in a professional manner. I do not expect perfection, or anything at a level beyond what we cover, but preparation for the paper is your best friend in this course. Choosing your paper topic as early as possible, asking me for feedback on current draft iterations will help me, help you.\nAs corollary to the preceding points, please do contact me if you have questions. Policy data analysis is what I do in my research every day. I love what I do, and I love discussing this topic with others. If you have any questions about the ideas we cover in class or have any difficulties, you may always meet with me or contact me otherwise. However, I can only help you if you reach out to me.\nWith this said, do not simply communicate with me. Feel free to communicate with your classmates as well. This is something I only really learned the value of as a PHD student, so I figured I would advise the same to you."
  },
  {
    "objectID": "index.html#class-schedule",
    "href": "index.html#class-schedule",
    "title": "Econometrics for Policy Analysis",
    "section": "1.4 Class Schedule",
    "text": "1.4 Class Schedule\nBelow is the schedule. All readings for Econometrics for Policy Analysis (EPA) should be done before class. I will specify if anything else must be read before class.\n\n1.4.1 Week 1\n\n08-26-2024 (Monday)\n\nIntroductions\n\n08-28-2024 (Wednesday)\n\nPaper Discussion\n\n\n1.4.2 Week 2\n\n09-02-2024 (Monday)\n\nUniversity holiday. No class.\n\n09-04-2024 (Wednesday)\n\nRequired: EPA, C2 and EPA C3.\nCompletely Optional: IS C2, IS C3 (skim), IDS C2, IDS C3, especially “Discrete Probability” and “Random Variables”.\nBriefly covers the use of data for policy analysis. A refresher on averages. Also covers t-tests, standard errors, and confidence intervals.\n\n09-06-2024 (Friday) The introduction section for your paper is due.\n\n\n\n1.4.3 Week 3\n\n09-09-2024 (Monday) - No Class, Jared’s Out of Town. Read the chapter on basic Asymptotic Theory (mainly the Law of Large Numbers and the Central Limit Theorem), EPA C4.\n09-11-2024 (Wednesday)\n\nStata Day.\n\n\n1.4.4 Week 4\n\n09-16-2024 (Monday)\n\nCorrelation, Coeffcients, and Association (EPA, C6)\nHere we cover basic correlation in 2 Dimensions, mainly using scatterplots.\n\n09-18-2024 (Wednesday)\n\nRequired: EPA C6, OLS Explained, estimation (here through (not including this section) here)\nOptional: (ROS, C7), IS, C10.\n\n\n1.4.5 Week 5\n\n09-23-2024 (Monday)\n\nInference for OLS. Gauss-Markov Assumptions of OLS. The data section for your paper is due.\n\n09-25-2024 (Wednesday)\n\nIntro to Treatment Effects (Sections 7.1-7.3)\n\n\n1.4.6 Week 6\n\n09-30-2024 (Monday)\n\nTreatment Effects Continued\n\n10-02-2024 (Wednesday)\n\nDifference-in-Differences (7.4 onward)\n\n\n1.4.7 Week 7\n\n10-07-2024 (Monday)\n\nDID Continued\n\n10-09-2024 (Wednesday)\n\nReplicable Research: The Importance of Script\n\n\n1.4.8 Week 8\n\n10-14-2024 (Monday)\n\nMidpoint (Last Day to Withdraw). Presenting Results: Tables and Graphs. Methods and Results section is due today.\n\n10-16-2024 (Wednesday)\n\nTables and Graphs Continued\n\n\n1.4.9 Week 9\n\n10-21-2024 (Monday) Ethics in Research: P-Values\n10-23-2024 (Wednesday) P-values continued.\n\n\n\n1.4.10 Week 10\n\n10-28-2024 (Monday) Data Measurement\n10-30-2024 (Wednesday) Data Measurement continued\n\n\n\n1.4.11 Week 11\n\n11-04-2024 (Monday) Sampling\n11-06-2024 (Wednesday) First submission due today.\n\n\n\n1.4.12 Week 12\n\n11-11-2024 (Monday)\n11-13-2024 (Wednesday)\n\n\n\n1.4.13 Week 13\n\n11-18-2024 (Monday)\n11-20-2024 (Wednesday)\n\n\n\n1.4.14 Week 14\n\n12-02-2024 (Monday)\n12-04-2024 (Wednesday)\n\n\n\n1.4.15 Week 15\n\n12-09-2024 (Monday) Final Paper Submission Due\n12-11-2024 (Wednesday)\n\n\n\n1.4.16 Week 16\n\n12-16-2024 (Monday)"
  },
  {
    "objectID": "index.html#course-policy",
    "href": "index.html#course-policy",
    "title": "Econometrics for Policy Analysis",
    "section": "1.5 Course Policy",
    "text": "1.5 Course Policy\n\n1.5.1 Assignments\n\nAssignments are due the day it’s listed. Late work will not be graded.\n\n\n\n1.5.2 Attendance\n\nUp to 2 unexcused absences are allowed.\nTo obtain approval for an excused absence from class (including religious holidays, family emergencies, or illness), a written notice needs to be sent to the instructor via email in advance.\n\n\n\n1.5.3 Changes to Schedule\n\nSyllabus and course schedule are subject to change.\n\n\n\n1.5.4 Type of Course\n\nThis is an in-person course at Classroom South, Room 200.\n\n\n\n1.5.5 Technology\n\nThis course will primarily use Stata, however I don’t care if you already know another software (e..g, R) and wish to use that instead.\nGSU computers are available in libraries and computer labs across campus (including in the AYSPS Building) for easy access.\nPlease reach out to me immediately if you have any challenges accessing or utilizing the technology for this course.\n\n\n\n1.5.6 Support Statements\n\n1.5.6.1 Inclusivity Statement\nWe understand that students in our program come from a variety of backgrounds and perspectives. AYSPS is committed to providing a learning environment that respects diversity. To build this community, we ask all members to: - Share their unique experiences, values, and beliefs - Be open to the views of others - Honor the uniqueness of their colleagues - Appreciate the opportunity that we have to learn from each other in this community - Value each other’s opinions and communicate in a respectful manner - Keep confidential discussions that the community has of a personal (or professional) nature\n\n\n1.5.6.2 Students with Disabilities\nStudents who wish to request accommodation for a disability may do so by registering with the GSU Access & Accommodations Center (AACE). Students may only be accommodated upon issuance of a signed Accommodation Plan by AAACE. Students are responsible for providing a copy of that plan to instructors of all classes in which accommodation is sought. To register for accommodations, please follow this link: GSU Access & Accommodations Center.\nFor more information, contact AACE located at Student Center East, Suite 205, 55 Gilmer Street, Atlanta, GA 30303.\nPhone: 404-413-1560\nEmail: access@gsu.edu\n\n\n1.5.6.3 Remote Academic Coaching\nThe Office of Disability Services also offers free remote academic coaching. To learn more about these services, go to GSU Disability Services or watch a Coaching Video.\n\n\n1.5.6.4 Veterans & Serving Military\nGeorgia State honors its military and veteran men and women returning to pursue their education. Students who are veterans serving in the military and their dependents are encouraged to avail themselves of a full range of college services and activities through the Military Outreach Center (MOC).\nFor assistance or guidance while attending GSU on campus or online, contact the Atlanta Campus Military Student Advocate, Randy Barrone, at 404-413-2331. Please be sure and let me know ASAP if or when there is any possibility of you being activated and deployed. Thank you for your service!\nFor more information, contact the GSU Military Outreach Center\nPhone: 404-413-2331\nEmail: rbarrone@gsu.edu\nWebsite: GSU Military Outreach\nAddress: Dahlberg Hall, 30 Courtland Street, Suite 217, Atlanta, GA 30303\n\n\n1.5.6.5 Online Course Evaluations – Student Surveys\nYour constructive assessment of this course plays an indispensable role in shaping improvements of all courses within this program and your educational experiences at Georgia State. Please take the time to fill out the online course evaluations. We appreciate honest and open feedback.\n\n\n1.5.6.6 GSU Policy Prohibiting Students from Posting Instructor-Generated Materials on External Sites\nThe selling, sharing, publishing, presenting, or distributing of instructor-prepared course lecture notes, videos, audio recordings, or any other instructor-produced materials from any course for any commercial purpose is strictly prohibited unless explicit written permission is granted in advance by the course instructor. This includes posting any materials on Chegg, Course Hero, OneClass, Stuvia, StuDocu, and other similar sites. Unauthorized sale or commercial distribution of such material is a violation of the instructor’s intellectual property and the privacy rights of students attending the class and is prohibited. The GSU Faculty Senate approved this policy on August 21, 2020."
  },
  {
    "objectID": "module1.html",
    "href": "module1.html",
    "title": "2  Data and Policy Studies",
    "section": "",
    "text": "3 Summary\nAt this point, it’s clear that data and data analysis are critical to public policy. It allows us to visualize trends, identify the effects of interventions, and reach conclusions on the basis of this evidence. However, the “how” we reach conclusions part matters, since the methodology we use to reach conclusions fundamentally affects what we can conclude in the very first place. The next two lectures cover probability and asymptotic theory; these form the foundations of quantitative public policy analysis."
  },
  {
    "objectID": "module1.html#what-is-this-thing-called-science",
    "href": "module1.html#what-is-this-thing-called-science",
    "title": "2  Data and Policy Studies",
    "section": "2.1 What is This Thing Called Science?",
    "text": "2.1 What is This Thing Called Science?\nScience at its core is a process we use to understand observable phenonmena. It is based on using logic and observations of the senses to form coherent understandings about the world. Data, or a collection of observations, is fundamental to being able to conduct scientific research. We use data in our daily lives to draw conclusions. Note here that data is not a living, breathing concept: it requires construction and interpretation by us. We use principles of science to analyze data. As we learn in middle and high school, science typically begins with asking questions or defining a problem.\nSuppose our current problem involves commute time to school or work, and we don’t wish to walk. In this case, that is our question: “What’s the ideal way to get to school/work?” We then gather information. Chances are we may use Google Maps or Waze to guide us. In this context, these tools provide us with the information we need, namely, estimates of how long our commute will be. And, assuming we wish to get to our destination as fast as possible, we make inferences or conclusions about the ideal way to take based on the GPS’ options. If GPS says the highway takes 15 minutes but the backstreets which avoid highways take 35 minutes, we will typically elect to use the highway since that takes us to our destination the quickest.\nThere’s still two more steps to do, though: test our hypothesis (derived from the GPS) and draw conclusions about the actual observed facts. This means that we must, in real life, leave home and take one way to school. When we get to our destination, we form conclusions about how actually taking the highway went. Of course, we repeat this idea multiple times to get a better sense of which say is best; eventually, we take a certain direction to work or school precisely because we have the expectation the highway will, on average, be preferable to alternative ways. This is a simple example, yet it illustrates the central point: in scientific inquiry, we ask questions, draw on existing information at our disposal, act upon that information, and draw conclusions or plan accordingly based on testing the validity of that observed information. We don’t call this science in daily life, but that’s exactly what it is. The steps I’ve outlined so far are present in every field from public policy to physics, albeit with a little more sophisitcation.\nIn quantitative public policy analysis, data is central to all that we do. The simple reason for this using data allows us to try to resolve disagreements. While it is certainly true that people may conduct different data analyses based on different assumptions and obtain different results/conclusions, the main idea is that we all agree that we can look into the real world as a metric for decisionmaking. In particular, we build constructs that map on to metrics we care about. After all, everyone can have opinions or views on things, but the useful part is testing out our expectations against reality. That way, we can have a better sense of what’s more likely to be true if a certain policy happens/is passed.\nTraditionally, data analysis in the policy space has two goals in mind. The first is descriptive analysis of a phenomenon or topic. In this setting, we simply use raw or lightly transformed data to visualize relationships between variables. For example, suppose we wish to visualize the trends of the U.S. GDP before and after the Berlin wall fell, beginning in 1960 to 2003.\n\nclear *\nu \"http://fmwww.bc.edu/repec/bocode/s/scul_Reunification.dta\", clear\nloc int_time = 1990\nqui xtset\nlocal lbl: value label `r(panelvar)'\n\n\nloc unit =\"USA\":`lbl'\n\ncls\n\nlevelsof code, loc(units)\nloc a: word count `units'\nforval i=1/`a'{\n    local lcolors \" `lcolors' plot`i'opts(lcol(gs10))\"\n}\n\nxtline gdp,  overlay ///\n    `lcolors' ///\n    addplot((connected gdp year if code ==`unit', ///\n        connect(L) lcolor(black) mcol(none) lwidth(thick))) ///\n    legend(off) ///\n    yti(\"GDP per Capita\") ///\n    tline(`int_time')\nWe can use the above script to load into Stata a dataset that speaks to this phenomenon. I’ll explain what I did step by step. I first open Stata on my computer. Then, I type in doedit into the command line terminal for Stata, opening what’s called a do-file, or a text file that Stata uses to execute script. I clear everything from the terminal’s memory by using clear *. I then store the year of the Berlin Wall’s collapse into what’s called a local macro, 1990 in this case. A macro is simply something that takes the place of another number or letter. The data here are sorted by country code and year – here, code is a numeric variable that has words indexed to each number (in this case, the USA is unit 1). I then use the levelsof command to get an ordere list of all values for code (again here, each country is indexed to a particular number), storing this list in the macro units. I wish to highlight the United States in particular. Therefore, I count the length of the macro units, storing it in its own macro called a. Now, I make the plot. I use what’ called a loop to build a macro of line colors (a light grey in this case), where we have one grey per number of nations aside from the United States. Then, using xtline, I build the plot. We can see that the GDP per capita of the United States consistently trended upwards from 1960 to the early 2000s. This has no meaning by itself; however, imagine this were enemployment rates in a local community, and we detected a spike in one particular unit. Descriptive analysis tells us that a problem exists, and we may use line graphs or other graphics to visualize this. By the way, this is an example of what we call panel data, where many units are observed over more than one point in time.\nA second goal of policy analysis is estimating the impact of some policy or intervention on some outcomes, or perhaps some measure of association. Association is quite simple. Say we wish to visualize the relationship between cigarette prices and sales in the year 1980. We can do this with a scatterplot. We can load in a dataset that speaks to this and make a plot.\nclear *\nimport delim \"https://raw.githubusercontent.com/OscarEngelbrektson/SyntheticControlMethods/master/examples/datasets/smoking_data.csv\", clear\n\nkeep if year ==1980\nreplace retprice = retprice/100\ncls\nscatter cigsale retprice, ///\nti(\"Scatterplot: Cigarette Packs Sold per Capita versus Price\") ///\nxti(Price) yti(Sales)\nHere we create a cross sectional dataset (or, where we observe many units at one point in time, 1980 in this case). We can then make a scatterplot between the price of cigarettes in each state and the amount of packs sold in each state. We can see, as we’d expect, that sales tend to decrease with an increase in price.\nWhat about impacrt estimation? We can use the same dataset we just imported to do this too. In 1989 California wished to reduce tobacco smoking by its implementation of Proposition 99. This intervention raises an immediate question for policy analysis: namely, “what was the effect of this intervention on the actual smoking rates we see?” Of course, we may collect more comprehensive tobacco sales data on this matter. After data collection (or even prior, in this case), we can form hypotheses. A hyopothesis is an interrogative, testable statement about the world. It is like a hypothetical in the sense that we try to imagine the effect of a policy (or relationship between variables). Here, we can hypothesize that Proposition 99 has a negative impact on tobacco smoking. Negative here is not intended in the normative sense; presumably most people reading this do not smoke (tobacco, anyways) or think that smoking is wonrg or immoral. Instead, here “negative” means that the policy might decrease the tobacco sales per capita compared to what they would have been otherwise. To test this, we can use statistical analysis to compare California to other states that didn’t do the policy.\n\n\n\n\n\n\n\n\n\nThe plot shows the cigarette pack sales per 100,000 for California from the years 1970 to 2000 (our dependent variable). The thick black line denotes the observed values for California, and the vertical black reference line shows the year that Proposition 99 (the independent variable/treatment) was passed. In our case, we wish to produce an estimate of California’s cigarette consumption in the years following 1989 had Proposition 99 never been passed. This line is denoted by the blue dashed line. After we do our analyses/estimations, we can discuss what the implications are. In other words, was the policy effective by some appreciable margin? Are there other outcomes concerns to consider? We can see at least from the plot I present here that it seems like Prop. 99 decreased California’s tobacco sales compared to what they would have been otherwise."
  },
  {
    "objectID": "module1.html#steps-of-data-analysis",
    "href": "module1.html#steps-of-data-analysis",
    "title": "2  Data and Policy Studies",
    "section": "2.2 4 Steps of Data Analysis",
    "text": "2.2 4 Steps of Data Analysis\nBroadly speaking, we can think of data analysis being broken into 5 distinct concepts. I summarize them below.\n\n2.2.1 Identifying Policy Problems\nAs we’ve discussed above, the first step in this process is simply asking questions. What kind of questions? Policy questions. Knowing what specific questions to ask though can be tricky. Policy is a giant field. Of the thousands of questions we could ask, how do we know which ones will be the most pressing or timely? In other words, how do we know that this is a problem that policy needs to be enacted for? How can we identify programs whose analysis benefits the citizenry or other interested parties? Put simpler, who cares? Why do we want to do this study or answer this question? Who stands to benefit?\n\n\n2.2.2 Gathering Data\nEven once we’ve identified the problem, how do we go about gathering real data to answer questions? If we can’t get data that speaks to the issues that we’re concerned about, we can’t obtain answers that are useful.\n\n\n2.2.3 Cleansing Data\nIn real life, datasets do not come to us wrapped in a pretty bow ready for use. Cleaning data (or organizing it) can be a very messy affair in the best of times. In order for us to answer our questions, the data we obtain must be organized in a coherent way such that we can answer questions at all. If you wish to plot the trend lines of maternal mortality in Romania compaed to 15 other nations and your data are not sorted by nation and time, trust me, the plot you’ll get will not just look terrible, but you can’t glean any trends or patterns from it. What’s worse, you may not even know improper sorting is the casue of the problem until you bother to look at your dataset again. So, it is best to have good habits developed early.\n\n\n2.2.4 Analyzing Data\nFor analysis, we apply statistical analysis in order to answer the questions we’re asking, using the dataset we’ve now cleaned. Such techniques can range from simply descriptive statistical analysis to complex regression models. From such models, we sometimes wish to make inferences to a bigger population, but sometimes more specific statistics (e.g., the average treatment effect on the treated units) are of interest.\n\n\n2.2.5 Presenting the Results\nNow that we’ve done analysis, we can finally interpret what the findings mean. We attempt to draw conclusions based on our results and come up with avenues for future research or other relevant aspects of interest. In this section, we typically try and say why our findings are relevant."
  },
  {
    "objectID": "module1.html#identifying-policy-problems-1",
    "href": "module1.html#identifying-policy-problems-1",
    "title": "2  Data and Policy Studies",
    "section": "2.3 Identifying Policy Problems",
    "text": "2.3 Identifying Policy Problems\n\n2.3.1 Justifications For Policy\nBefore we can do any analysis though, we have to take a step back. We have to ask ourselves how we know a problem exists in the first place. There are two broad justifications that policy is based on: negative externalities and social good, but the main point of both justifucations is “harm”.\n\n\n2.3.2 Externalities\nThe idea of externalities comes from microeconomic theory, which says that efficient markets will affect only those parties who willingly participate in transactions. Particularly in the case of negative extrnalities, or externalities which harm others, we could use public policy to rectify this.\nConsider a very simple example: seatbelts. In physics, any force that is not stopped by an equal, oppostite force will keep going. So, if you’re in a car crash while driving at 60 miles per hour while unbuckled, the car stops. You, however, don’t stop: you keep going, 60 miles per hour through the windshield. No public policy is needed just yet. So far, any cost that comes from a transaction has been borne by you, the driver. By the way, I’m not kidding: one of the arguments against seatbelts was literally that using seatbelts should be a personal decision if it does not put others at risk. Additionally, industry also argued against mandatory seatbelt laws on the grounds that it was the government interfering between the transactions of a consumer and the seller.\nHowever, there are a few issues with the externality argument. Firstly, being unbuckled turns you into a human projectile. You can hit your passengers or even others outside your vehicle if you’re unbuckled. Your market exchange (you buying the car and driving it) is now potentially having second-order effects on others by you not using a seatbelt. So, the government may wish to mandate seatbelts while driving in order to prevent these negative externalities which come in the form of medical bills or death. To address the argument of indsutry above, that seatbelt laws would raise costs of production, this raises an important moral dilemma: does the harm caused to the business of having to install seatbelts matter more than the human harm caused by a society where seatbelts are optional? Also, we are human beings. We have imperfect knowledge. We know for fact that we don’t have all the answers, to paraphrase Socrates. We also don’t know if the actions we do will ultimately hurt someone else. We live in a probabalistic world (which we will return to later). Indeed, we could argue against laws banning DUI in precisely this manner, saying that we don’t know if the intoxicated driver will harm someone until they do. But, as with seatbelts, we never know if there will be another passenger on the road or a child playing in the street. So, we rarely know if we’re actually putting peoples’ lives in danger by driving drunk or unbuckled. We can’t know if an externality will occur until it does, usually. Thus, the next view (social good) adopts a different form of reasoning.\n\n\n2.3.3 Social Good\nMoreover, the externality justification isn’t typically the way we think about things from a public policy perspective. Usually, we have social welfare goals in mind. This can come in the form of harm reduction or prevention measures. When we argue for public education, for example, we typically don’t do so because we think that the private schools won’t educate citizens enough (even though they won’t), and that public school will be to decrease inefficient education markets. In fact, we typically don’t think of education (in our formative years anyways) as a market at all. We usually argue for public education because we think that education has inherent benefits, and that being denied a certain level of education necessitates an inherent harm. Imagine for a moment how the literacy rate of the United States would look if school was completely optional. We likely would not complain about GDP loss, we’d likely complain about a society where lots of people can’t read the cereal box or function within society in a decent manner. In other words, society has a vested interest in keeping people safe, educated, and healthy to some degree. So we mandate seatbelt laws, basic schooling, and other laws/regulations in service of these ends. Importantly, “these ends” does not have a right or wrong answer. The goals of policy are ultimately decided by people within the society. However, knowing the goals of a policy and reasons for its existence helps us ask meaningful questions about it. Following the above discussion, a natural research question that follows is “How did seatbelt laws affect the rate of car accident injuries and deaths?”\n\n\n2.3.4 Why Is Tobacco a Problem?\nAs we’ve discussed above, harm or necessity is typically a standard we look to in order to determine if policy is needed. As I’ve mentioned, California passed Proposition 99 in 1989 to reduce smoking rates. But, how did we know there was a problem to begin with? To do this, we can grab data on lung cancer morality rates from 1950 until today. Presumably, of course, we view lung cancer as harmful and somehthing we wish to prevent.\n\n\n\n\n\n\n\n\n\nThe shaded area represents the period before any state-wide anti-tobacco legislation was passed in the United States. We can see quite clearly the age-standarized lung cancer mortality rates rose in a fairly linear manner in the United States. However, the curve is parabolic; mortality rates were rising every single year until the zenith in 1990. Mortality began to fall when the first large scale anti-tobacco laws were passed. Of course, the degree to which these laws were the cause of this decrease is an empirical question (especially since lung cancer develops over time, the decrease after 1990 suggests other thing may have also contributed to the decline in behaviors that led to the decreease in mortality). However, given the clear increase in lung cancer rates and other obvious harms of tobacco smoking in the preceding decades, policymakers in California and the voters, in fact, became increasingly hostile to tobacco smoking in public and in other crowded areas. So, California passed legislation in 1988 (as did at least a dozen other states from 1988 to 2000) to decrease smoking rates.\nHad I not plotted this trend line, people (from the tobacco industry in 1970, for example) could simply say “Well, nobody knows if lung cancer mortality is a problem. How do we know if there’s a problem here? I don’t think one exists.” This plot makes a powerful case that lung cancer is indeed a problem which must be addressed due to the persistent rise in mortality. Data in other words provides intellectual self-defense; if you posit that a problem exists, then this should be demonstrable using datasets that speak to the issue at hand. As a consequence of this, if a problem does exist (be it tobacco smoking or the impact of racial incarceration/arrest disparities), we can then look for policies that attempt to mitigate or solve the problem. That way, we can go about doing analysis to see which policies are the most effective."
  },
  {
    "objectID": "basicprob.html#descriptive-statistics",
    "href": "basicprob.html#descriptive-statistics",
    "title": "3  Basic Probability Theory",
    "section": "3.1 Descriptive Statistics",
    "text": "3.1 Descriptive Statistics\nProbability is rarely used in a vacuum, though. We typically, in the policy sciences, wish to take a given outcome from a set of outcomes and draw conclusions from it. To do this, we use descriptive statistics (also called moments) to summarize probabilities. These moments map on to variables aside from coin flips- however to truly understand this, we’d need to introduce integration and other topics in math which we can’t cover here.\n\n3.1.1 Means: Arithmetic and Median\nThe first moment is called the average/arithmetic mean. The formula for the mean, also called the expected value (denoted by \\(\\mathbb E\\)) is \\[\n\\bar{x} = \\mathbb{E}[X]=\\frac{1}{N}\\sum_{i=1}^{N}x_i\n\\] where \\(\\bar{x}\\) is the mean, \\(N\\) is the number of values, and \\(x_i\\) represents the \\(i\\text{-th}\\) value in the sequence. The uppercase Greek letter “sigma” means summation, or \\(\\sum_{i=1}^{N} x_i\\). It adds the values from \\(i = 1\\) to \\(N.\\) For a discrete random variable, the expected value is\n\\[\n\\mathbb{E}[X] = \\sum_{i=1}^{N} x_i \\cdot P(x_i).\n\\] So for our die, the expected value is \\[\n\\mathbb{E}[X] = 1 \\cdot \\frac{1}{3} + 2 \\cdot \\frac{1}{3} + 3 \\cdot \\frac{1}{3} = \\frac{1 + 2 + 3}{3} = 2\n\\] For a coin, the expected value (assuming it is fair) is 0.5. Suppose we have a room of 10 men and 40 women, where women take the value of 1 and men the value of 0. The average number of women in the room is \\(\\frac{40}{50}\\). This means the expected value of women in the room is .8. In other words, if we randomly selected a person from the room 10 times, we’d expect about 8 of them to be women. We can take averages with things aside from die and coins too. Naturally, if the amount of water in one giant jug was 5 liters and in another jug there is 7 liters, the average liters of water in the sample is \\(\\frac{1}{N}\\sum_{i=1}^{N} x_i = \\frac{1}{2}\\sum_{i=1}^{2} 5+7=6\\) liters. Now we should distinguish between the population and sample statistics: the sample is that subset of the population that we can get. We can rarely sample every single American in the country (the population), but a random (or representative) sample of 3000 Americans, say, is just fine. This difference is important: outside of simulations, we never can get every single datapoint for all our interventions of interest. So, we collect a sample which approximiates that population we are truly interested in.\nThe median, or the middle number, is also a type of average. It is less influenced by outliers than the average is. Suppose we have a dataset of years of education across a group of people in a neighborhood, \\(A=\\{5,6,7,9,18\\}\\). The middle number here is 7 (since two numbers lie to the let and right of 7). But let’s consider the issue deeper: suppose we were to use the average years of education at the average. For us, we have \\[\n\\frac{1}{5} \\times \\sum_{i=1}^{5} 5 + 6 + 7 + 9 + 18 =\\frac{45}{5}=9\n\\] The mean and median produce differing values. If we were to use the mean, we’d conclude the average person in this sample is in high school. When in fact, as a raw number, the modal respondent is a middle schooler with one elementary schooler. Thus, we can see that the average is influenced by outliers (in this case, somebody in graduate school). The classic joke is that when Bill Gates walks into a bar, everyone, on average, is a billionaire.\n\n\n3.1.2 Variance\nThe variance is the second moment. It represents the average squared differences from a point. For a random variable \\(X\\), the sample variance is denoted as \\(s^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\bar{x})^2\\). For an intuitive example, suppose we have two middle schoolers in a room, one who reads at 6th grade level and the other at 8th grade level, \\(A=\\{6,8\\}\\). The sum of squared differences of each of these datapoints from the mean is (7) is 2, since 6 is 1 less than 7, and 8 is 1 more than 7. So, our sample variance is 2. The variance simply reflects the average distance of each data point from the center/mean of our observations. In practice however, we must correct for uncertainty about our sample estimate. So we subtract by 1 in the denominator. This is called Bessel’s correction. Subtracting 1 factors in uncertainty, since practically we are unsure about the “true” average in a population. When we do this, we get \\(\\frac{2}{2-1}\\), making the new bias corrected variance be equal to 2.\nThe square root of our sample variance is what’s called the standard deviation from the mean. Why standard? The raw variance is not in the same units as our original data. When we take the square root, we may interpret this as the “standardized” distance from the mean. For this simple example of the students, we can round the standard deviation down to 1, since \\(\\sqrt{2}=1.41\\). When we think about it, it makes sense. If you’re at a middle school where the average reading level is 7th grade, people who read at 6th grade level are simply 1 year below the average, and those at 8th grade 1 year more than the average.\nSome might wonder why we’re squaring these differences from the mean. The squared differences gives more weight to outliers, or datapoints that are very far from the mean. Suppose \\(A=\\{6, 8, 18\\}\\). The average of this is 10.6. Person 1 and 2 are only 4.6 and 2.6 years less than the mean. But someone in middle school with a graduate in college reading level at 18 years is very, very, very far from the mean (practically speaking). The squared differences themselves are 21.79, 7.13, and 53.77, and the sample standard deviation is roughly 6.43. Had we not squared the differences, we’d get a standard deviation of 4.89. So, we square the larger differences to assign more weight to large outliers, since not doing so would basically treat the middle schooler with a college graduate reading level as roughly equal to those who are much closer to the average of 10. The variance is also understood in other contexts like weather."
  },
  {
    "objectID": "basicprob.html#hypothesis-testing",
    "href": "basicprob.html#hypothesis-testing",
    "title": "3  Basic Probability Theory",
    "section": "3.2 Hypothesis Testing",
    "text": "3.2 Hypothesis Testing\nIn public policy we oftentimes wish to test hypotheses. A hypothesis is a statement about the world that we wish to determine the validity of. For example, we could hypothesize that the average math score for a scool is 86, or we can hypothesize that black people use welfare less than white people. We are always testing our hypothesis (which we call the research hypothesis, \\(H_R\\)) against a scenario where this hypothesis is wrong (the null hypothesis, \\(H_{0}\\)). That is, we start off by assuming the math score is not different from 86 or that blacks and whites use welfare at the same rates. We only change our minds in light of compelling evidence. If this confuses you, imagine we had a courtroom where the burden of proof is now shifted on the defense to prove their client innocent. We would never be okay with presuming guilt. No, we’d presume the null hypothesis is true (the person is innocent until proven guilty), saying that the people making the positive claim of guilt are the ones who must supply enough evidence to convince us otherwise. In research, there are many ways to test a hypothesis, but the first way we will go over for this is by employing a t-test. The t-test produces a t-statistic, or a measure of how extreme our estimated sample mean is relative to a hypothesized mean or comparison group mean. The t-statistic is a measure of how many standard errors away from the population/hypothesized mean our observed mean is.\n\n3.2.1 One Group T-Test\nFirst we cover the one-sample t-test, where we compare our research hypothesis against some known/predefined statistic. If I ask you what you think the average literacy rate is in the population of Americans, you may give different answers like “I think it’s at the 9th grade level”, “I think the average literacy level is less than the 6th grade level” or “I think the average literacy rate is different from 0”. Each of these forms sets of testable hypotheses. In the first case, \\(H_R\\) is “The literacy rate is equal to 9th grade.” In the second case, \\(H_R\\) is “The literacy rate is less than the 6th grade level.” Finally, we’d say \\(H_R\\) for case 3 is “I think the literacy rate in America is not 0” (or, some significant portion of the population can read). Formally, we denote these hypotheses as\n\\[\n\\begin{aligned}\n& H_{R}: L=9 \\\\\n&H_{R}: L < 6 \\\\\n&H_{R}: L \\neq 0.\n\\end{aligned}\n\\]\nOur corresponding null hypotheses (the one we first assume) is\n\\[\n\\begin{aligned}\n& H_{0}: L \\neq 9 \\\\\n&H_{0}: L \\geq 6 \\\\\n&H_{0}: L = 0.\n\\end{aligned}\n\\]\nwe simply take a of sample the population somehow (which is usually taken care of for us in census data/compiled statistics). We then calculate the sample average of the grade level of our sample (ranging from 0 being illiterate and 16+ which means postgraduate). Let’s visualize this.\n\n\n\n\n\n\n\n\n\nThis is a histogram. It shows a distribution of data. To better conceptualize it, imagine the height of the histogram (the y axis) represents the number of people in the data who take on the values on the x-axis. So, roughly 70 people have a literacy level of 8. In this case I generated a sample of 500 people with a small amount of variance. The average literacy rate in this population is 8 (for 8th grade). We now wish to see if our population mean (of 8th grade) is different from the researcher mean (9, 6, and 0). The formula for the one-sample t-statistic is\n\\[\nt = \\frac{\\bar{x} - \\mu_R}{\\frac{s}{\\sqrt{n}}}.\n\\] Let’s parse these terms. In the numerator we take the difference of our sample mean (\\(\\bar x\\), the mean we in fact observe in our dataset) versus our hypothetical mean that we are testing our sample mean against, denoted as \\(\\mu_R\\) (“myoo-sub R”). The denominator is the standard error, which is the standard deviation divided by the square root of our sample size. For example, here’s how we’d do this with the null hypothesis for 0 (that is, our sample mean is different from 0).\n\\[\nt = \\frac{\\bar{x} - \\mu_R}{\\frac{s}{\\sqrt{n}}}=\\frac{7.949 - 0}{\\frac{1.9983}{\\sqrt{500}}}=88.95.\n\\] In other words, the sample mean is 88.95 standard errors away from the hypothesized mean (0 in this case). We can do the analogous thing for reading level 6 and 9, where we substitute 0 in the above formula for those respective numbers. We can do this in Stata like\n\nttesti 500 7.949 1.9983 0 // sample size, average, standard deviation, hypothesized mean\n\nOne-sample t test\n------------------------------------------------------------------------------\n         |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\n       x |     500       7.949    .0893667      1.9983    7.773419    8.124581\n------------------------------------------------------------------------------\n    mean = mean(x)                                                t =  88.9481\nH0: mean = 0                                     Degrees of freedom =      499\n\n    Ha: mean < 0                 Ha: mean != 0                 Ha: mean > 0\n Pr(T < t) = 1.0000         Pr(|T| > |t|) = 0.0000          Pr(T > t) = 0.0000\n\n\n\n3.2.2 Two-Group T-Test\nWe can also do a 2-group t-test, where we wish to compare average group differences. We can compare men and women, one city to another city, one city to many cities, and so on. For our purposes though, we’ll just compare one city to another one. I generate a sample of 10000 minutes (the unit of time) where one city has a mean production rate per minute of 6 and another of 14 with respective variances of 1.5 and 3. Say these means represent the average kilowatt usage for electricity, and the variances are how much production is spread out by the minute for each city (where city 2 clearly has a higher variance).\n\n\n\n\n\n\n\n\n\nHere is how we’d calculate the t-statistic in this case.\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\nLet’s parse these terms. Here \\(\\bar{x}_i\\) is the averge production rate of a city and \\(s\\) denotes the variance for each city. The numerator represents the raw differences in means, and the denominator represents the pooled standard errors for both cities (recall how we calculated standard error above). Note however here we use the variance, not the standard deviation to compute t. Why? Well, each group may have different levels of spread in its measurements. We need to account for the variability in each sample to see if the means are truly different when we account for this spread in each sample because higher variance increases error. And, in a situation where there’s substantial variability in the data, it makes it much harder to say if the means differ in an appreciable manner. To compute the t-statistic, we plug in the values. For the denominator:\n\\[\n\\sqrt{\\frac{1.5}{10000} + \\frac{3}{10000}} = \\sqrt{\\frac{1.5 + 3}{10000}} = \\sqrt{\\frac{4.5}{10000}} = \\sqrt{0.00045}\n\\] which yields \\[\n\\sqrt{0.00045} \\approx 0.0212.\n\\]\nNow, calculate the t-statistic:\n\\[\nt = \\frac{6 - 14}{0.0212} = \\frac{-8}{0.0212} \\approx -377.36\n\\]\nThis means that the sample mean difference is 377 standard errors lower than what we’d expect (in this case assuming no difference). We can also infer that City B produces a lot more electricity than City 1.\nBy the way, we can also do this for other things too. Take the Proposition 99 example. We could take the mean differences in tobacco consumption after 1988, comparing California to other states that did not do the intervention. Here is some sample Stata code (you can use this for your paper, by the way)\n\nclear *\n\ncls\n\nu \"https://github.com/jgreathouse9/FDIDTutorial/raw/main/smoking.dta\"\n\n\nttest cigsale if year > 1988, by(treat) reverse unequal\n\nTwo-sample t test with unequal variances\n------------------------------------------------------------------------------\n   Group |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\n       1 |      12       60.35    3.487999    12.08278    52.67297    68.02703\n       0 |     456    102.0581     1.10465    23.58887    99.88726     104.229\n---------+--------------------------------------------------------------------\nCombined |     468    100.9887    1.121973    24.27199    98.78393    103.1934\n---------+--------------------------------------------------------------------\n    diff |           -41.70811    3.658742               -49.59344   -33.82279\n------------------------------------------------------------------------------\n    diff = mean(1) - mean(0)                                      t = -11.3996\nH0: diff = 0                     Satterthwaite's degrees of freedom =   13.314\n\n    Ha: diff < 0                 Ha: diff != 0                 Ha: diff > 0\n Pr(T < t) = 0.0000         Pr(|T| > |t|) = 0.0000          Pr(T > t) = 1.0000\nFrom this, we’d suggest that the average effect of the intervention was a decrease of 41 packs per capita compared to other states that didn’t do the policy. The t-statistic in this case means that our estimate of the mean difference is 11 standard errors below that of the that of the average of states who did not do the policy."
  },
  {
    "objectID": "basicprob.html#uncertainty-around-the-mean",
    "href": "basicprob.html#uncertainty-around-the-mean",
    "title": "3  Basic Probability Theory",
    "section": "3.3 Uncertainty Around the Mean",
    "text": "3.3 Uncertainty Around the Mean\nTypically we are concerend with the uncertainty of our estimates. Uncertainty around the mean is typically expressed through confidence intervals. A confidence interval provides a range of values that, under certain conditions, contains the true population mean.\n\n3.3.1 Confidence Intervals and the Normal Distribution\nTo understand confidence intervals, it’s essential to first grasp the role of a normal/Gaussian distribution. The normal distribution is a continuous probability distribution characterized by its bell-shaped curve. We call it a continuous distribution because unlike coin flips, other data points can take on many values such as homicide rates, COVID-19 rates, and other metrics that can’t be broken into simple, countable groups. Most real-world phenomena, when measured, tend to follow a normal distribution (we will return to this in the lecture on asymptotic theory). A normal distribution is defined by its mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The great thing about a normal distribuition is that we can prove that 68 and 95% of the data lie within 1 and 2 standard deviations of the mean. We will exploit this fact to construct a confidence interval for a statistic of interest.\n\n\n3.3.2 Constructing a Confidence Interval\nThe most common confidence interval is the 95% confidence interval. This means that if we were to take many samples and repeat our analysis many times, the confidence interval contains the true average. Typically, we have only one sample to work with (and we rely on large-sample asymptotics to argue for the validity of our confidence intervals), but methods such as bootstrapping (where we simulate many such samples) may be employed to do this too. For our current purposes though, we construct a confidence interval for the population mean \\(\\mu\\), we use the sample mean \\(\\bar{x}\\) and the standard error of the sample mean as we’ve defined them above.\nFor a 95% confidence interval, we use the critical value from the standard normal distribution, typically denoted as \\(t^*\\). For a 95% confidence level, \\(t^* \\approx 1.96\\) (since approximately 95% of the data lie within 1.96 standard deviations from the mean in a standard normal distribution). As ones sample size increases, the t-distribution converges to a standard normal distribution. This means 1.96 is a good approximation of a 95% confidence interval for all sample sizes greater than 30; otherwise, a different t-statistic would be used. In the old days, t-tables were used to do this, but now software handles much of this for us. By the way, we can construct other confidence intervals too. We can construct a 90, 99, or even 80% confidence intervals; however in science, we typically use the 95% CI. We usually interpret confidence intervals that contain 0 (say, [-1,1]) as being statistically insignificant. If the CI does not conatain 0 (say, [3,5]), we say it is significantly different from 0 (or, that the means are much different from one another).\n\n3.3.2.1 One Group T-Test CI\nI generated a 10,000 person sample of incomes (in 1000s). The true average is 50. We think the average is 60. The variance is 2. To estimate the confidence interval, we compute \\[\n0.0277= 1.96 \\times \\frac{\\sqrt{2}}{\\sqrt{10000}}\n\\] to get our standard error of the mean. Here, we see that we multiply our critical value of 1.96 by the standard error. This entire expression is our margin of error. Now, in order to characterize the range that the mean falls within, we simply do \\[\n\\text{CI} = \\mu \\pm \\text{Margin of Error}=50 \\pm 0.0277=(49.986,50.014).\n\\]\nWe interpret this as “Our sample mean is 50 thousand dollars. We are 95% confident that given the data, the real mean lies between 49.986 and 50.014 thousand dollars.” Since both these numbers are less than 60, our research hypothesis (\\(H_R=60\\)) is likely incorrect, as 60 does not fall within these estimates. Therefore, we fail… to reject, the null hypothesis (the hypothesis of no difference).\n\n\n3.3.2.2 Two-Group T-Test CI\nNow, we can revisit the city example from the above and see if the means significantly differ, using the two-group t-test. We typically use this kind of t-test in situations where we wish to compare one group to another. The formula for the CI for the difference between two means is given by: \\[\nCI = (\\bar{x}_1 - \\bar{x}_2) \\pm t \\times \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\]\nWe know the values from the above, so we plug them in. We also use the critical value of 1.96, since this is the value we use for a 95% CI. First we compute the standard error using the variances and sample sizes for both groups: \\[\nSE = \\sqrt{\\frac{1.5}{10000} + \\frac{3}{10000}} = \\sqrt{\\frac{4.5}{10000}} = \\sqrt{0.00045} \\approx 0.0212.\n\\] We now have a margin of error (using the critical value 1.96 as above) of:\n\\[\n\\text{Margin of Error} = t  \\times SE = 1.96 \\times 0.0212 \\approx 0.0413\n\\] We already know the mean difference is -8, so now we just plug that in and solve:\n\\[\nCI = (-8) \\pm 0.0413.\n\\]\nSo, the 95% confidence interval for the difference in means is: \\[\n(-8 - 0.0413, -8 + 0.0413) = (-8.0413, -7.9587).\n\\]\nThis interval suggests that City 1 consumes significantly less, on average, than City 2. By the way, for those who are curious, if we just reversed the order of the numerator, we’d get the same result but it would be \\((7.9587,8.0413)\\), where we’d say that City 2 consumes significantly more than City 1. Using the Stata code block above, we’d say the true effect of the intervention lies between a reduction of 49.59344 to 33.82279 packs per capita, under quite heroic assumptions (but that’s for later)."
  },
  {
    "objectID": "basicprob.html#a-brief-word-on-practical-significance",
    "href": "basicprob.html#a-brief-word-on-practical-significance",
    "title": "3  Basic Probability Theory",
    "section": "3.4 A Brief Word on Practical Significance",
    "text": "3.4 A Brief Word on Practical Significance\nTo conclude, a word of caution: as we can see, the magnitude of the t-statistic and tightness of the CIs tend to scale with sample size. Consider the two group case: \\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}.\n\\]\nWe can see that an increase in the sample size leads to a decrease in the overall denominator. Suppose for the denominator \\(s^2=4\\) for both groups. If both groups have the size of 40, then we just have \\(\\sqrt{.1+.1}\\). But if both groups have a sample size of 400, the then we have \\(\\sqrt{.01+.01}\\). The reason this matters is because researchers oftentimes interpret the t-statistic and whether it’s greater than 1.96 as a measure of practical importance. But this is wrong! Since our t-statistic is guaranteed to increase with sample size, per the formulae above, at certain sample sizes it would be hard for our confidence intervals to contain 0 at all.\nWhat this means as a matter of practicality is to always keep in mind your sample size and what would matter practically to people in real life. If you estimate that the price of one brand of bottled water, for example, costs 0.05 dollars more than another brand across all 50 states, and your t-statistic is 70 and your CI is [0.01,0.06], then do not claim (in isolation anyways) that this difference is very meaningful or earth-shattering, since they make either a penny more or 6 cents more. I say this because I do not want for you, in real life or in your papers, to apply these ideas mechanically. I want you to always keep in mind how statistics maps on to the real world. The t-statistic for a correlation coefficient or regression coefficient can be statistically significant but practically uninformative."
  },
  {
    "objectID": "basicprob.html#summary",
    "href": "basicprob.html#summary",
    "title": "3  Basic Probability Theory",
    "section": "3.5 Summary",
    "text": "3.5 Summary\nProbability theory is the stepping stone into using statistical methods for policy analysis. It is the foundation of decision-making in business, economics, and policy studies. It allows us to, in a principled way, approach the understanding of problems in a data driven, informative manner by taking simplified models and applying them to real world ideas. For many, the concepts covered here will be new material– indeed, the term “statistics” or “data analysis” can be intimidating to people at first glance. I believe the best way to introduce these topics is to keep a balanced perspective between technical mathematics and application. However, this course only scratches the very surface; the world of quantitative methods in policy analysis is a big one. For those of you interested in graduate school or who wish to use statistics for your future job, your mastery of this very essential material will not be in vain."
  },
  {
    "objectID": "stataintro.html",
    "href": "stataintro.html",
    "title": "4  Introduction to Stata",
    "section": "",
    "text": "5 Summary\nAt present, these are the basics of Stata that you need to be able to write about the dataset for your chosen question. I went over how to import data, how to use the browse command to check visually check through certain elements of your data, how to do t-tests as we discussed in the first lecture, and other general principles. We will refine these ideas more and more as we discuss correlation, regression, and the essentials of causal inference. Below is the do file I used to run most of this code."
  },
  {
    "objectID": "stataintro.html#script",
    "href": "stataintro.html#script",
    "title": "4  Introduction to Stata",
    "section": "4.1 Script",
    "text": "4.1 Script\nBefore we get to any of that though, we begin with the lifeblood of science: documentation and script. In this course, your analysis is only valid if what you do can be reproduced and shown to others. The way we do this is with script. The reason I opted away from Excel for this course (aside from the fact that we are a professional policy analysis department and Excel isn’t used in real life for proper data analysis) is that Stata allows for script such that we can replicate exactly what we did every single time. Excel, by contrast, does not have a reproducible documentation system which allows us to say exactly what we did and how we did it.\nIn Stata, we do this (mainly) with what we call .do files. Once you’ve opened Stata, type the word doedit into the terminal. What will open is an untitled do file. Alternatively, you can type doedit and a file name, say doedit file1, and a corresponding do file will open named file1.do. This is where all of our work will live for the purposes of class.\nI present script in code blocks. These code blocks may be copied and pasted directly into Stata so that you may run them. Each do file should begin with\nclear *\ncls\nThis clears the Stata terminal to be completely empty, as well as clears all of the current output on the screen. I do this for two reasons: firstly, it keeps everything clean in the sense that the screen is not cluttered with needless output. But, it also means that everything should be able to run from the very first step to the very last one without error.\nBy the way, clear and cls are Stata commands. We can see how to use these commands by consulting the help files, by typing in the terminal help and the command we’re interested in. For example, for help with the clear command, we can just do help clear or h clear where h is the short version of help in Stata. If you wanted to know how to summarize a variable using descriptive statistics, you’d do h summarize. If you wanted to learn how to take the correlation between two variables, you’d do help corr.\nAll Stata command names are lowercase. Typically, your variable of interest comes first after the command name, with all options being followed up after a comma. For example, I suppose I wanted to estimate the causal impact of drought in California on the violent crime rate. To do this, we can use data from this paper. In a Stata do file, we’d put this into a do file and then type CTRL+d.\nclear *\nimport delim \"https://ndownloader.figstatic.com/files/9466315\"\nkeep state year violentcrimerate murderrate vcrimerate id\norder id state year vcrimerate, first\ng drought = cond(id==4 & year >=2011,1,0)\nnet from \"https://raw.githubusercontent.com/jgreathouse9/FDIDTutorial/main\"\nnet install fdid, replace\nxtset id year, y\nfdid vcrimerate, tr(drought) unitnames(state) gr1opts(scheme(sj))\nSee how all commands are lowercase? In particular, after the fdid command, we see that the main variable is vcrimerate and that the options for the command follow the comma. We can see the same thing for the order command, where I rearrange the way the dataset looks."
  },
  {
    "objectID": "stataintro.html#importing-data",
    "href": "stataintro.html#importing-data",
    "title": "4  Introduction to Stata",
    "section": "4.2 Importing Data",
    "text": "4.2 Importing Data\nNow that that’s partly out of the way, we cover how to import data. Data are presented in different file types, but for undergraduate study the most you will need is the way to import Stata’s native datasets. Let’s import the terrorism dataset. Note that we can do this whether the data is on our local machine or if it is on the internet.\n\nclear * // We can leave comments in our code like this\ncls\n\nuse /// or we can leave comments like\n\"https://github.com/jgreathouse9/FDIDTutorial/raw/main/basque.dta\" // this\nbrowse\n\n\n/*\nOr, here's a longer \n\n\n\n\nblock comment\n\n*/\nThis imports the Basque dataset using the use command (we may also use u as an abbreviation). Note that the use command may ONLY be used with Stata datasets, datasets whose file extension ends with .dta. To view the dataset, we use the browse command (which we may abbreviate with br). Here, we see the entire dataset. The first column is the variable id, and the second one is year, and the third column is the outcome of interest gdpcap. When we browse the dataset, we may notice that some cells of the spreadsheet have periods in them. This means that the value is missing.\n\n4.2.1 Data Types\nNow, a word on variable types. The first column is what we call a categorical variable. A categorical variable is a variable that is numeric in nature, but it has no specific order attached to it. For example, consider the output of the list command, which simply prints the dataset.\nlist id if year ==1955\n\n     +------------------------------+\n     |                           id |\n     |------------------------------|\n  1. |                    Andalucia |\n 44. |                       Aragon |\n 87. |                     Asturias |\n130. |             Baleares (Islas) |\n173. |  Basque Country (Pais Vasco) |\n     |------------------------------|\n216. |                     Canarias |\n259. |                    Cantabria |\n302. |              Castilla Y Leon |\n345. |           Castilla-La Mancha |\n388. |                     Cataluna |\n     |------------------------------|\n431. |         Comunidad Valenciana |\n474. |                  Extremadura |\n517. |                      Galicia |\n560. |        Madrid (Comunidad De) |\n603. |           Murcia (Region de) |\n     |------------------------------|\n646. | Navarra (Comunidad Foral De) |\n689. |                   Rioja (La) |\n     +------------------------------+\nAs I said, this simply lists the dataset, in this case in the year 1955. We could just as easily do list id treat if year ==1955 to see the values of the id and the treatment varaible in 1955. What makes id categorical though? Well, imagine the state names indexed to the numbers 1…17, where 1 means Andalucia, 2 means the state of Aragon, and so on and so forth. However, there’s no inherent order for these beyond what we assign these states. We could just as easily have the Basque Country’s number be 1, instead of the current number (which if we do display id[173] in the Stat terminal we see is number 5). You may wonder about why, when we browse the dataset, the letters are blue. This is because Stata knows it is a categorical variable. We can also create what’s called a string variable, or letters by doing\ndecode id, generate(statestring)\n\nbrowse id statestring if year ==1970\nWe can now see that the next variable, statestring is colored red. In Stata, this means that it is what we call a string variable (i.e., something we cannot do math with)."
  },
  {
    "objectID": "stataintro.html#inspecting-data",
    "href": "stataintro.html#inspecting-data",
    "title": "4  Introduction to Stata",
    "section": "4.3 Inspecting Data",
    "text": "4.3 Inspecting Data\nAll other variables in the dataset are numeric, in that we may do math with them. For example, suppose we wish do see the average of all Spanish gdpcap in the year 1955, the first year of the dataset. We can do\nsummarize gdpcap if year ==1955\nwhich returns the output table\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      gdpcap |         17     2.42521    .9244023    1.24343   4.594473\nWe can do the same with other variables too, such as year. When we do summarize year, we get\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        year |        731        1976    12.41817       1955       1997\nNotice how the number of observations is now 731. Why? Well, I did not place any restrictions on the range of years (naturally) by using the if qualifier. So, when you don’t place restrictions on any command you write, Stata will presume you desire the full dataset. Anyways, we can see that the minimum year is 1955 and the maximum year is 1987 (so, nobody who is doing their paper on the Basque Country should get wrong when their study period is when the data section is due!!!)\nWe can use the if qualifier to do other things too. Suppose I ask you when the treatment happened, denoted by the treatment variable treat. We can do summarize year if treat==1.\nSuppose I ask you which unit was treated. How can we solve this? Well, we already know that by construction that only the Basque Country was exposed to the terrorist intervention, and the other 16 control units were not exposed. So, this means that across all units, the treat variable should only ever equal 1 for one unit. Because of this, this means that the average id (which is a number, as we’ve seen) shoould be a constant number. Let’s use summarize to get this result.\nsummarize id if treat ==1\nwill return\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          id |         23           5           0          5          5\nThis means the treated unit’s numeric id id 5. Note, for the sake of interpretation, the standard deviation is 0. Why? Well, as we discussed, only one unit is treated. Only one unit’s row for treat ever takes on the value of 1. So, the standard deviation should be 0 because there IS no spread or variation among the values of the unit id variable that is treated, it’s all the same number. Similarly, the maximum and minimum for the unit id variable id is the same because only one unit is treated.\nTo get a list of all the variables in ones dataset, one may do\n\nclear *\n\ncls\n\nuse \"https://github.com/jgreathouse9/FDIDTutorial/raw/main/basque.dta\", clear\n\nds"
  },
  {
    "objectID": "stataintro.html#doing-basic-analysis",
    "href": "stataintro.html#doing-basic-analysis",
    "title": "4  Introduction to Stata",
    "section": "4.4 Doing Basic Analysis",
    "text": "4.4 Doing Basic Analysis\nLet’s suppose we wish to test the hypothesis that the Basque Country’s GDP per Capita was different from the average of all other 16 control units before 1975. We can do this with a t-test. What kind of t-test? A Two Group T Test, where groups in this instance are defined by whether or not the unit is the Basque Country or not. To do this, we use Stata’s ttest command by doing\n\nclear *\n\ncls\n\nuse \"https://github.com/jgreathouse9/FDIDTutorial/raw/main/basque.dta\", clear\n\ng basque = cond(id==5,1,0)\n\nttest gdpcap if year < 1975, by(basque) reverse unequal\nwhich returns\n\nTwo-sample t test with unequal variances\n------------------------------------------------------------------------------\n   Group |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]\n---------+--------------------------------------------------------------------\n       1 |      20    5.282476     .232162     1.03826    4.796556    5.768397\n       0 |     320    3.662215     .080167    1.434072    3.504492    3.819938\n---------+--------------------------------------------------------------------\nCombined |     340    3.757524    .0793618    1.463359    3.601421    3.913628\n---------+--------------------------------------------------------------------\n    diff |            1.620262    .2456134                1.113093     2.12743\n------------------------------------------------------------------------------\n    diff = mean(1) - mean(0)                                      t =   6.5968\nH0: diff = 0                     Satterthwaite's degrees of freedom =   23.781\n\n    Ha: diff < 0                 Ha: diff != 0                 Ha: diff > 0\n Pr(T < t) = 1.0000         Pr(|T| > |t|) = 0.0000          Pr(T > t) = 0.0000\n\nTo do this, we need a variable that is equal to 1 if the unit id belongs to the Basque Country, else 0. I do this with the cond() function, which obeys the logic of “If condition 1, do A, if not condition 1, do B”. In this case, condition 1 is id==5, or the id variable being 5 (since this is the number for the Basque Country). So, the variable basque will be for allidthat are 5, else 0. We then restrict thettest` to be done for all years before 1975. This is the exact same t-test as we discussed in the second chapter. We can use the formula to compute t\n\\[\nt = \\frac{5.282476 - 3.662215}{\\sqrt{\\frac{1.078}{20} + \\frac{2.0576}{320}}}=6.59\n\\]\nThe term diff refers to the simple difference in averages, in this case before 1975. The confidence interval for this difference is \\([1.113, 2.127]\\). This means that the Basque Country’s GDP per Capita is 1.6 dollars higher than the rest of the states in Spain, and the lower bound for this estimate is 1.113 dollars higher and the upper bound is 2.127 dollars higher. From a practical standpoint, we can conclude that Basque GDP was much higher than the rest of Spain in the pre-1975 period. This suggests that a comparison between the raw means of Basque Country and Spain is invalid, since they are quite different from each other in the pre-intervention period."
  },
  {
    "objectID": "stataintro.html#graphs-and-plots",
    "href": "stataintro.html#graphs-and-plots",
    "title": "4  Introduction to Stata",
    "section": "4.5 Graphs and Plots",
    "text": "4.5 Graphs and Plots\n\n\n\n\n\n\nNote\n\n\n\nPart of being a policy analyst is using data to tell a story, but the only way this can be done graphically is by the graph being informative and well constructed. These are the principles you should follow when making graphs, making them informative and clearly drawn. This will require you playing with Stata’s options, as the defaults for plots are rarely adequate.\n\n\nWe are primarily working with what we call panel data, where we observe many units over time. In this case, we observe 17 states across 43 time periods, hence the number of observations being \\(17 \\times 43=731\\), or one row per state per time period. One useful way of plotting data that we see over time, is by using a line plot. Let’s plot the difference between the Basque Country and the 16 control states.\n\nclear *\n\ncls\n\nuse \"https://github.com/jgreathouse9/FDIDTutorial/raw/main/basque.dta\", clear\n\nkeep year gdpcap id\n\nreshape wide gdpcap, i(year) j(id)\n\nbrowse\nFirst to do this we need to reshape our dataset from long to wide. In long datasets, we have one observation (time period) per unit per row. In a wide dataset, we have one observation per row, and a column for each unit. When we browse the above code block, we see that year is the first variable, and the values of GDP per Capita are populated by the values for each state. In other words, these line plots produce the same results\nclear *\n\ncls\n\nuse \"https://github.com/jgreathouse9/FDIDTutorial/raw/main/basque.dta\", clear\n\nkeep year gdpcap id\n\nline gdp year if id ==5, name(plot1, replace) //plot number 1, long dataset\nreshape wide gdpcap, i(year) j(id)\nline gdpcap5 year, name(plot2, replace) // plot number 2, wide dataset\nNow, we need to calculate the mean of the control units. To do this, we need the egen command.\n\nclear *\n\ncls\n\nuse \"https://github.com/jgreathouse9/FDIDTutorial/raw/main/basque.dta\", clear\n\nkeep year gdpcap id\n\nreshape wide gdpcap, i(year) j(id)\norder gdpcap5, after(year)\n\negen controlmean = rowmean(gdpcap1-gdpcap17)\n\ndrop gdpcap1-gdpcap17\nline gdpcap controlmean  year, xli(1975) // creates a reference line for 1975, where the last variable is the x axis\nWe can see that the Basque Country is much richer than the rest of Spain here. Its trends of GDP per Capita are much greater than the national average. So for example, this is one plot you might make if you wanted to argue that some additional statistical test would need to be done to see what the true effect of a policy was. After all, if the trend of Spain is not sufficiently similar to the Basque Country in the pre-intervention period, the rest of Spain may not offer a suitable idea for how the Basque Country’s trends would have evolved, absent treatment. And as policy analysts, the way an outcome would look absent an intervention of interest is a key thing we care about, which we define as the counterfactual.\nNote that this is just a line plot, mostly useful for when observations happen over a set time frame. Other plots are useful for visualizing the distribution of data, say boxplots. If we wished to see the distribution of gdp per capita and levels of local investment of all Spanish states in 1970, we’d do\n\nclear *\n\ncls\n\nuse \"https://github.com/jgreathouse9/FDIDTutorial/raw/main/basque.dta\", clear\n\ngraph box gdpcap if year ==1970, name(distgdp, replace) nodraw\ngraph box invest if year ==1970, name(distinvest, replace) nodraw\ngraph combine distinvest distgdp"
  },
  {
    "objectID": "asymptotic.html",
    "href": "asymptotic.html",
    "title": "5  Asymptotic Theory",
    "section": "",
    "text": "6 Summary\nAsymptotic theory simplifies statistical analysis by encouraging us to think about the “true” population of interest. It provides us with tools to derive more accurate and precise estimates from. But, as a more general rule for policy analysis (and life), it demands us to think with an infinte mind instead of a limited one. In other words, we should always ask ourselves as researchers “if we could sample everyone, would this statistic I just calculated be close to reliable?” One practical implication of this is having a sufficiently large sample size in order to increase the probability of being closer to the true mean. However, as the previous section discusses, these asymptotics are only as justified as the quality of the sample. We need, in other words, our sample to be as representative as possible of the broader population of interest. In the next lecture on correlation, aside from sampling, we will discuss the basics of statistical design in order to have valid results from statistical analysis."
  },
  {
    "objectID": "asymptotic.html#law-of-iterated-expectations",
    "href": "asymptotic.html#law-of-iterated-expectations",
    "title": "5  Asymptotic Theory",
    "section": "5.1 Law of Iterated Expectations",
    "text": "5.1 Law of Iterated Expectations\nSuppose we wish to commute to and from Georgia State University from Marietta, Georgia. As we’ve discussed in the previous chapter, we can think of some variable \\(c\\) (commute time) as a random variable, as its value is not guaranteed until we actually leave home and arrive. Suppose we are now interested in the average time it takes us to arrive using I-75 South, conditional on us taking the South or North depending on our starting point. We think I-75 South will take 15 minutes, compared to I-75 North which we think will take 20. We can formalize this also as \\(x =\\{1,0\\}\\), where we take North being coded as 1, else as 0 if we take North. So, we leave home (or school) using either highway, and record how long it took to get to the destination. Say, we record the value of 30 minutes in the morning and 40 in the afternoon. Do we conclude that this is how long it takes to get to school on averge, and that we should use some other interstate? No. Why not? This is only one estimate from one day. There are all kinds of things that could have been going on in the morning or afternoon that might influence your travel time, most notably traffic, construction, or other random events for any given path we choose. So, what are we left to do? The only thing we can do, is collect more data.\nSo suppose we take this same highway for one month, using only 75 South and North (unlike say I-285). We record the amount of time it took us to get to school and home that day using either way (that is, we record our commute time to and from school each day for both ways, taking the average separately for each week). Mathematically, we express this as the expected commute time conditional on the chosen route \\(x\\): \\(\\mathbb{E}[c|x=1]\\) (expected commute time given we take North) and \\(\\mathbb{E}[c|x=0]\\) (expected commute time given we take South). The Law of Iterated Expectations (LIE) states that the overall expected commute time for I-75 (to Marietta, anyways) is the average of these conditional expectations, weighted by how often each route was taken. Formally: \\(\\mathbb{E}[c] = \\mathbb{E}[\\mathbb{E}[c|x]]\\). This means that the overall average commute time \\(\\mathbb{E}[c]\\) is the expected value of the conditional expectations \\(\\mathbb{E}[c|x]\\)\nSo, if after a month of data collection, we find that the average commute time is 17 minutes, this is the unconditional expectation \\(\\mathbb{E}[c]\\), calculated by taking the average of the commute times across both routes across all days. This approach, where we repeatedly take the average of an outcome/variable given some condition (here, the route we take), reflects the Law of Iterated Expectations."
  },
  {
    "objectID": "asymptotic.html#law-of-large-numbers",
    "href": "asymptotic.html#law-of-large-numbers",
    "title": "5  Asymptotic Theory",
    "section": "5.2 Law of Large Numbers",
    "text": "5.2 Law of Large Numbers\nWhy might this be true, though? Why, after taking all of the averages across a whole month do we arrive at 17, which is a lot closer to 15 than we first thought? Surely, this number is much quicker than the 30 minutes we took the first day? The reason for us getting this value is because of what we call the Law of Large Numbers, or \\[\n\\lim_{N \\to \\infty} P\\left(\\left|\\frac{1}{N} \\sum_{i=1}^{N} x_i - \\mu\\right| < \\epsilon\\right) = 1.\n\\] Here is what the math says: as our sample size \\(N\\) increases without bound (that is, as we take the highway more and more and more and more and more… to an infinite amount of times), the probability that the average of our individual empirical daily commute times \\(x_i\\) approaches the true population value is 1. In other words, the more estimates we take, the closer, in probability, we come to our population estimate. You see, the first day of 30 minutes was simply a sample of 1. We had nothing else to base our ideas off of aside from whatever GPS tells us. Maybe there was traffic or some other unforeseen thing. However, as we take the highway more times, we tend to get a better sense of how long it’ll take to get places, what lanes to use, and so on and so forth. To further prove this, the expected value of a three sided die numbered 1 to 3 is 2. If we cast the die three times and then take the average of what we get, we should see the cumulative empirical average converge to the theoretical average.\n\n\n\n\n\n\n\n\n\nAs it turns out, that’s exactly what we do see. As researchers, what this means is that drawing from a large sample tends to be better than a small one for empirical. For example, if someone has a sample size of 20, we likely would not be okay with generalizing one particular aspect of this sample (say, weight or political affiliation) to everyone in the same city, as we’d need more data points to average over. Ideally, if we’re trying to sample American public opinion, we don’t want to use only 2 American states, ideally we’d have all the state data available to use."
  },
  {
    "objectID": "asymptotic.html#central-limit-theorem",
    "href": "asymptotic.html#central-limit-theorem",
    "title": "5  Asymptotic Theory",
    "section": "5.3 Central Limit Theorem",
    "text": "5.3 Central Limit Theorem\nBy understanding the Law of Iterated Expectations (LIE) and the Law of Large Numbers (LLN), we can now delve into the Central Limit Theorem (CLT), which helps us characterize the overall distribution of commute times. That way, we can do things like calculate the confidence interval of our commute times, hoping it will approximate the true one. The CLT says: \\[\n\\frac{\\bar{x_i} - \\mu}{\\sigma / \\sqrt{N}} \\xrightarrow{d} N(0, 1)\n\\] or that as the sample size increases, the probability of our empirical distribution approaches[a normal distribution. Aside from the distribution of our measurements, our sample mean, by LLN, approaches the population mean. If this seems at all abstract to you, we may simulate this. Here, I define the average commute time to be 15 with a standard deviation of about 3 minutes (between 12 and 18 minutes). I then simulate the commute time across 20000 commutes (since I didn’t wish to drive down Interstate 75 20,000 times!). We can clearly see from the GIF that the empirical distribution (that is, the commute times we experience) quickly approaches the true population average time as we commute more and more.\n\nThis convergence supports the LLN, which tells us that our sample mean will approach the true mean \\(\\mu=15\\) as we collect more data. Additionally, notice how our confidence intervals get tighter and tighter given an increase in sample size. Note that this result is intuitive: as we collect more data, it would make sense that we have a better picture about the underlying data (such as, the sample mean and variance). So, with this in mind, it makes sense that we have a better sense of our uncertainty of our estimates, as the confidence interval shows. So, with the commuting example, the first day took us a half hour. We suspected that the commute would be 15 and 20 minutes respectively (the expected value of which is 17.5 minutes). So, in terms of minutes, we could hypothesize that the true travel time takes between 15 minutes or 45 minutes. In other words, we’re quite uncertain. But, as we collect more data, the uncertainty decreases, tightening to be around the true value.\n\nAdditionally, the CLT allows us to visualize how even if the original distribution of the variable is not normal (say, a coin toss where we only have two outcomes, heads or tails), the distribution, given enough samples, converges to a normal distribution.The plot above flips a coin 1000 times, plotting the empirircal distribution after every 100 flips. We can see that the distribution, despite having only two outcomes, converges to a normal one. Thus, CLT allows us to construct confidence intervals and make inferences about a population, given a large enough sample."
  },
  {
    "objectID": "asymptotic.html#sampling",
    "href": "asymptotic.html#sampling",
    "title": "5  Asymptotic Theory",
    "section": "5.4 Sampling",
    "text": "5.4 Sampling\nBefore we continue however, we need to say a few words about sampling, and the idea of collecting a random, probibalistic sample versus a non-random sample. The reason this matters is because no matter how we calculate our statistics, we need to be sure that our statistics can actually map on to the actual constructs we want them to. A random sample, in principle, is the idea that everyone from a given population has the same chance to be included in the sample for some study. For example, if we wanted to survey the public opinion of Georgia State students, one potential way of doing this would be to get every single active GSU email from the University and put it into a spreadsheet. We then could generate a variable in the spreadsheet called a “Bernoulli” variable, or a variable that takes on the values 0 or 1. We can then, after setting the number of observations (defined as the number of emails we have), define the probability with which a given variable takes on 0 or 1 to be 0.5. In other words, everyone has an equal chance of being included in the survey, 1 means youre included, else 0. Note, there are other, much more sophisticated ways to do this, but this is one way of taking a random sample.\nWhat would a non-random sample look like, then? Well, in principle it’s where everyone in the population does not have an equal chance of being included in the sample. But, this is a simple definition. It does not explain why it’s bad or why it’s harmful to researchers. So, let’s consider a few kinds of non-random sampling. The most obvious one is something called convenience sampling. As the name suggests, we take a sample based on whoever is around us. This can be in our class, on the street, or in our friend group. For example, I have two coworkers, one goes to MIT and the other went to Columbia, both for econ degrees. If I wanted to know how good the average person in the United States is at calculus or math, then I cannot simply assess their skillset and reach a conclusion. Why not? Well, MIT and Columbia are incredibly selective schools that select for math skills. Furthermore, the fact that I know them or work with them is likely correlated to my research interests which demand a good background in statistics/applied mathematics. Similarly, even if we personally randomly asked people on the street, this is still not a random sample. Why? Well, there are likely underlying reasons to people being in certain locations. If you’re in the biology department, it’s likely that most people you meet will have an uncanny knowledge about germ theory or anatomy. And we’d expect this, we’d say something’s wrong if these were not true.\nAt this point, you’re likely asking what any of this has to do with the previous discussion of asymptotic theory or policy analysis for that matter. Well, the sample we collect is directly related to the results that we obtain. If our goal is to approximate math knowledge in America, it’s inapprorpiate to only sample students from the engineering department at Georgia Tech or the economics students at Chicago. To say it differently: even if we did collect data from every single student at every single engineering department in the United States, this would map on to the population of engineering students, at best, not the United States as a whole. In statistics language, our estimate of the “true” mean would be biased, in this case significantly upwards. Even though our confidence intervals would be more precise as our sample size increases (that is, the more engineering students we ask), we would still never converge to the true parameter because our sample is so dissimilar to the population of people we care about, effectively giving us the right answer to the wrong question. When we discuss regression, the importance of sampling will become even more apparent, but for now suffice to say that it’s important that the sample be as representative of the underlying population as possible."
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "6  Correlation and Association",
    "section": "",
    "text": "7 Summary\nCorrelation and causation is a very delicate topic in statistics. Whenever we’re doing research, we must always be careful to structure our studies in such as way that our findings are not corrupted by other factors, even if our software tells us we’ve found a “statistically significant” correlation. I’m sure if we took the t-statistic of music programs and graudation (or funding of music programs, as my scatterplot does), we’d find a very high t-statistic for the correlation coefficient. Equally, we’d find a high t-statistic for countries reporting increases in loneliness and low fertility rates. But I don’t care, and neither should you, since there are other factors which contribute to graduation aside from music, and a lot more factors driving fertility rates than simple loneliness or lack of opportunity to meet people. Thinking causally can be a challenging thing. After multiple years of torment, you will learn to think like this as if by muscle memory since it’ll be so routine to you. We will cover this in more detail in our chapter on treatment effect estimation."
  },
  {
    "objectID": "correlation.html#a-prelude-to-regression",
    "href": "correlation.html#a-prelude-to-regression",
    "title": "6  Correlation and Association",
    "section": "6.1 A Prelude To Regression",
    "text": "6.1 A Prelude To Regression\nTypically, when we discuss correlation, we use scatterplots to visualize the association between variables. A scatterplot is simply a chart which depicts the co-movement of variables. On the x-axis we plot our independent variables/predictors (or, the things we think explain a given outcome) and the y axis plots the variable we think is being affected by the one on the x-axis. Below, I plot in the left panel the homicide rate per 100,000 versus the state-specific Gini coefficient for 27 Brazilian states in the year 1990. The right panel plots the average retail price of cigarettes versus cigarette consumption per capita for 39 American states in the year 1980.\n\n\n\n\n\n\n\n\n\nWell now, what do we see here? We see the plotted datapoints along with the Pearson’s \\(r\\). We can see a negative correlation coefficient reported, where a one unit increase in the Gini coefficient leads to a decrease in the homicide rate in Brazil for this year. We also observe a negative relationship between the retail price of cigarettes and the consumption of cigarettes, where an increase in price leads to a decrease in the amount of cigarettes consumed. This result especially should be pretty intuitive: all else equal, as the price of a good increases, the demand for said good generally decreases. However, what about the leftmost plot? The Gini coefficient is a measure for inequality, where 1 denotes one person has all the money and everyone else nada/nothing. A Gini coefficient of 0 means everyone has the same amount of money, and a measure of anything in between is some intermittent level of inequality. Well, this result is puzzling in a bivariate lens: typically, we associate income inequality and poverty with an increase in things like higher rates of homicide and other kinds of crime, but that’s not what this result implies. While correlation is a useful measure sometimes, it alone is inadequate for serious policy analysis. Below, I explain why."
  },
  {
    "objectID": "correlation.html#the-first-exercise-of-the-statistical-mind",
    "href": "correlation.html#the-first-exercise-of-the-statistical-mind",
    "title": "6  Correlation and Association",
    "section": "6.2 The First Exercise of the Statistical Mind",
    "text": "6.2 The First Exercise of the Statistical Mind\nEarlier this morning, I was on Facebook and one of my friends posted a picture that cited a story of a public school music teacher named Annie Ray winning a Grammy. In the article, they say [caps theirs]\n\nTHE FACTS ABOUT THE IMPORTANCE OF MUSIC EDUCATION DON’T LIE… Schools that have music programs have significantly higher graduation rates than do those without music programs (90.2 percent as compared to 72.9 percent).\n\nFor visualization, let’s do some graphing shall we? The plot on the left plots the rates from the quote, the right plot is simulated data.\n\n\n\n\n\n\n\n\n\nThis quote caused fire alarms to sound in my head. Why? Because the article (and reporting on music education more broadly) misleadingly discusses these statistics. This does NOT mean these statistics are wrong in terms of their computation. Presumably whoever did this used statistical software to get these numbers, and I trust that the numbers are accurate.\nMy criticism is about practical implication. The heavy suggestion from the block quotation is the music programs are causing this 17.3 percentage point difference in graduation rates. When we read things like “The facts don’t lie”, there’s this air of certainty that these statistics are being reported with.\nAnd in fairness, this is not a completely crazy idea: music does indeed help people learn languages. It likely helped me learn Spanish and made me a better overall reader. It’s also correlated with spatial skills. In fact, in another life, I was a music theorist who could do harmonic analysis of chord progressions and tell you what I thought the artist was trying to communicate. And as a matter of fact, music studies likely made me an even better econometrician, because one thing music analysis teaches you about is context. And in larger context, these statistics seem misleading.\nThe suggestion is that if more schools just had music programs, we’d see higher graduation rates by 17.3 percentage points, on average. And yes, to some degree, if a school now has a music program and did not have one before, it is true students now have the opportunity to learn music. They may even go on to study and succeed in music now that they have the opportunity to do so. But how could we estimate this? How many people would this even affect, exactly?\nMost people do not want to be musicians. Learning music, like any other art form or professional skillset, is a non-trivial investment of time and money. Everyone doesn’t have the means or honestly dedication to pursue it, esepcially in light of other potential desires or opportunities. Those who do decide to become musicians (not just in classically trained in school, but generally) may have personal qualities that differ from other students in ways that affect whether they graduate or other aggregate metrics of success. So if 20 more students in a school of 5000 and a graduating class of 300 reap the benfits of music, is this really enough to move the proverbial needle on the graduation rate for an entire school or county? Not likely.\nMore to the point, not every school has the means to have a music program, nevermind a well funded music program. According to statistics reported by Yamaha, 8% of all public schools in the United States don’t have any arts programs at all (music, theatre, or dance).\nIn the U.S., schools are funded by property taxes in public schools and by extremely wealthy donors at private schools. This means that the wealthier public schools will be located in wealthier districts which naturally has a bigger tax base. What do those districts have more of? Money! Status, class, opportunity. Instruments do not grow on trees, they cost money; not every school has 80,000 dollars for a Steinway piano. By the way (no, I did not look this up before writing it), the same study, according to Yamaha’s reporting, said\n\nThe study also noted that a disproportionate number of these students without access to arts education are concentrated in major urban or very rural areas where the majority of students are Black, Hispanic or Native American, and at schools with the highest percentage of students eligible for free/reduced-price meals.\n\nI have to emphasize again, I didn’t suspect this because I looked it up before, I suspected it because this is what it means to think like a statistician. It means you have to think in a multivariate way, where more than one thing can affect something else. Here, this idea is pretty obvious: how do we know that these graduation rates are not explained in part (if not entirely) by baseline differences in socioeconomic status of communities (poverty, low employment rates, larger contextual factors), opportunities of individual families (say, personal connections individual families may have that others don’t), as well as the effects those factors have on individual students? When we think of it like this, simply suggesting that music programs would be a great solution is not so convincing.\nThese aside, there may also be what we call “selection bias” here, where some students are able to select into/decide to go to a given school. For example, some schools have magnet programs. The one I was in was for the International Baccalaureate Program; other high schools have STEM magnet programs or music/arts programs where some schools literally recruit middle school students who want to pursue these things. And when they select for these qualities, it becomes hard to isolate the impact of a music program on a graduattion rate, becasue they’re already selecting for highly motivated students (who mostly but not exclusively are from wealthier districts or better-to-do families). These are the kinds of students who would perform well anyways, even if music wasn’t there.\nThis again does not discount the real benefits of music education or the arts more generally! But when we read statistics, as with harmonic analysis of music chords, we also need to understand the context they exist within, and when we do this we begin to see that the relationship is not as clear cut as simple descriptive statistics might seem. As we see from the simulated plot, some schools have a graduation rate of almost nobody, which also happen to have low levels of music funding. And while I couldn’t find specific examples of this when I looked up data on it, numbers this low do happen anecdotally. And when we see numbers like this, we must ask ourselves “How are these sets of schools different from these sets of schools?”\nAs we will see when we cover regression analysis, the world rarely works off of pretty, linear functions. Measures of simple association do not mean that something is causing another thing, the world is much too complex for that. As researchers and as human beings, we must constantly be skeptical of simplistic claims and investigate them when they seem too good to be true."
  },
  {
    "objectID": "correlation.html#correlation-in-stata",
    "href": "correlation.html#correlation-in-stata",
    "title": "6  Correlation and Association",
    "section": "6.3 Correlation in Stata",
    "text": "6.3 Correlation in Stata\nBecuause you’ll never actuallly use the correlation coefficient for 99% of the analyses you ever do, I’ll only show how to use Stata to calculate correlation coefficients and scatterplots. Hopefully, the previous discussion is sufficient to show how it is not causation. Here, I take data from a paper that investigates the causal impact of crime reduction strategies on homicide rates (the same thing I did above). In 1999, the government of Sao Paulo “created or expanded a number of policies that have arguably contributed to the decrease in criminality”. One thing that may affect the homicide rate in a Brazilian state is the Gini Index, or a measure of inequality in a region. For the Gini Coefficient, -1 denotes perfect equality and 1 denotes one person having all of the money. We can produce a table and a plot in Stata which shows this relationship.\nclear *\n\ncls\n\n//!!!!!!!!! Note my use of \"import delimited\", \n// since this is the way to import a csv file\n\nimport delimited \"https://raw.githubusercontent.com/danilofreire/homicides-sp-synth/master/data/df.csv\", clear\n\nkeep if year == 1990\n\n// restricting our sample size to 1990\n\nkeep code state year homiciderates giniimp\n\n// keeping only these COLUMNS (the above keep, keeps only some observations)\n\ncorr homiciderates giniimp\n\ntwoway (scatter homiciderates giniimp, mcolor(pink) msymbol(smsquare)), ///\n    ytitle(Homicide Rate) /// Y Axis Title\n    xtitle(Gini Coefficient) /// X Axis Title\n    title(Correlation Between Inequality and Homicide) /// Plot Title\n    note(From: https://doi.org/10.25222/larr.334) ///\n    scheme(sj) // Stata Journal Scheme\nThe table we get is\n\n             | homici~s  giniimp\n-------------+------------------\nhomicidera~s |   1.0000\n     giniimp |  -0.2848   1.0000\nThe way to interpret this, is that there’s a weak negative relationship between the Gini Coefficient and the homicide rate. Or, as the Gini Coefficient/inequality increases, the homicide rate decreases"
  },
  {
    "objectID": "correlation.html#implications",
    "href": "correlation.html#implications",
    "title": "6  Correlation and Association",
    "section": "6.4 Implications",
    "text": "6.4 Implications\nThe reason this matters for policy analysts is because when taken to its conclusion, mistaken correlation for causation or not thinking about things in a systemic, multivariate way could result in pumping more money into music programs in order to fix failing schools, a much wider and more sophisticated problem.\nIt leads to things like pumping more money into police deparmtents to decrease crime rates, even though crime has been falling in general for decades in the U.S. and there’s no real link between militant policing measures and crime reduction.\nIt also leads to things like mass like many modern governemnts doing things like making dating apps and sponsoring mass dating events (yes, really!) in order to increase birth and marriage rates.\nThe idea of course is that people aren’t having kids or getting married because they’re not meeting one another. And if more people met, the more kids they’d have. And to a degree this is true due to things like social media, so the underlying logic makes plenty of sense. But then, once we agree to this probelm, we have to ask why are people not meeting one another and having kids or getting married? The real problem is a lot more systemic. Especially in South Korea, Japan, and China, the reasons are mostly having to do with labor issues, changes in gender norms, and broader social factors which lead to people not wanting to have kids."
  },
  {
    "objectID": "ols.html",
    "href": "ols.html",
    "title": "7  OLS Explained",
    "section": "",
    "text": "8 Summary\nThis undoubtably is the most weighty chapter, both in terms of mathematics and in terms of practical understanding. Regression is one of the building blocks for policy analysis, in addition to solid theoretical background and contextual knowledge of the policy being studied. The reason I chose to cover this first, in the first few weeks of the class instead of waiting until the end, is because I believe that the only way to truly understand regression is by use in applied examples. This is what you’ll wrestle with in your papers."
  },
  {
    "objectID": "ols.html#math-preliminaries",
    "href": "ols.html#math-preliminaries",
    "title": "7  OLS Explained",
    "section": "7.1 Math Preliminaries",
    "text": "7.1 Math Preliminaries\n\n\n\n\n\n\nImportant\n\n\n\nThis is a statistics course, not a math course. However, math is the language which underlies statistics. This is the only part of the course where we use any calculus to explain ideas. Furthermore, you’ll never be expected (from me) to use calculus for any of your assignments.\nWith this said, I do use the calculus to explain what is going on in regression. The calculus explains the why, which I think is always important to know if you’re implementing or interpreting regression. I presume however that you are like myself when I was in undergrad, in that you’ve either never taken calculus (me) or had little exposure to it. So, I link to tutorials on the calculus concepts that we employ here. Consult these links, if you’d like (they sure helped me in preparation of this chapter). However, they are completely optional. I try to explain everything as best I can, so they are provided as a supplement to the curious who want a better understanding.\n\n\n\n7.1.1 A Primer on Data Types\nFor any dataset you ever work with, you’ll likely have different variables (columns). Regression is no exception. The predictors for regression must be numeric, naturally. These take a few different types. Note that if it is not numeric, we call it a “string”.\nThe most common kind is a ratio variable (a value we may express as a fraction/continuous variable), such as the employment rate.\n\n\n\nState\nYear\nEmployment Rate (%)\n\n\n\n\nAlabama\n1990\n55.3\n\n\nAlabama\n1991\n56.1\n\n\nCalifornia\n1990\n62.1\n\n\nCalifornia\n1991\n61.5\n\n\nGeorgia\n1990\n58.4\n\n\nGeorgia\n1991\n59.2\n\n\n\nA dummy variable is a binary variable that indicates the presence or absence of a characteristic. A dummy variable (also called an indicator or categorical variable) is a variable that takes on the values 0 or 1. For example, a simple dummy indicates whether a respondent in a survey is a male or a female. In this case, the number 1 “maps” on to the value for male, and 0 for female. Note that in this case, the simple average of these respective columns returns the proportion of our observations that are male or female.\n\n\n\nRespondent ID\nGender (Male=1, Female=0)\n\n\n\n\n1\n1\n\n\n2\n0\n\n\n3\n1\n\n\n4\n0\n\n\n\nDummies can also be used to capture unobserved variation across groups. For instance, when predicting homicide rates across states like Alabama, California, and Georgia for 1990 and 1991, we can include dummy variables for each state. These dummies help account for unique, stable characteristics of each state, such as culture, that are hard to measure directly. In other words, if we think something “makes” Alabama, Alabama, compared to California or Georgia, we can include these kinds of variables to capture that unobserved variation. When including dummies in regression, we must always omit one category from the regression (for reasons we will explain below). So, for example, we could include Alabama/Georgia dummies or Georgia/California dummies, where California and Alabama would be what we call the reference group.\n\n\n\nState\nYear\nAlabama (1/0)\nCalifornia (1/0)\nGeorgia (1/0)\n\n\n\n\nAlabama\n1990\n1\n0\n0\n\n\nAlabama\n1991\n1\n0\n0\n\n\nCalifornia\n1990\n0\n1\n0\n\n\nCalifornia\n1991\n0\n1\n0\n\n\nGeorgia\n1990\n0\n0\n1\n\n\nGeorgia\n1991\n0\n0\n1\n\n\n\nThere is also a notion of an ordinal variable, where the data at hand must obey a specific order. Suppose we ask people in a survey how religious they are on a scale from 1 to 10, where 1=Atheist and 10=Extremely Religious. Here, order matters, because 1 has a very different meaning from 10 in this instance. An ordinal variable has a clear, ordered ranking between its values.\n\n\n\nRespondent ID\nReligiosity (1-10)\n\n\n\n\n1\n3\n\n\n2\n7\n\n\n3\n5\n\n\n4\n10\n\n\n\nThese are the data types you will generally work with. When we move on to real datasets, their meaning will become much more apparent.\n\n\n7.1.2 Review of Lines and Functions\nIn middle school, we learn about the basics of functions in that when we plug in a number, we get another number in return. If you’re at the grocery store and grapes are 1 dollar and 50 cents per pound, we just weigh the grapes and multiply that number by 1.5. This could take the form of \\((0,0), (1,1.5), (2,3)\\), and so on. In fact, we can represent these data points in a table like this\n\n\n\nx\ny\n\n\n\n\n0\n0\n\n\n1\n1.5\n\n\n2\n3\n\n\n\nThese points form a line, the equation for which being \\(y=mx+b\\). We can also think of this line as a function. It returns a value of \\(y\\) given some values for previous expenditures and pounds of grapes. Here, \\(y\\) is how much we pay in total, \\(m\\) is the rate of change in how much we pay for every 1 pound of grapes bought, and \\(b\\) is our value we pay if we get no grapes.\n\n\n\n\n\n\n\n\n\nFor this case, the function for the line is \\(y=1.5x\\). For here, \\(b=0\\) because in this case, how much we pay is a function of pounds of grapes only. We could add a constant/\\(b\\), though. Suppose we’d already spent 10 dollars, and now how much we spend is a function of both some previous constant level of spending, and new amount of grapes bought. Now, our function is \\(y=1.5x+10\\). The way we find the \\(m\\) and \\(b\\) for a straight line is the “rise over run” method, in this case\n\\[\nm = \\frac{y_2 - y_1}{x_2 - x_1} = \\frac{3 - 0}{2 - 0} = \\frac{3}{2} = 1.5\n\\]\nLines fit to points sometimes have discrepancies between the line and the data we see. We call this the residual, or \\(\\hat \\epsilon = y-(mx+b)\\). Also notice how the rate of change, or \\(1.5x\\), is the same at every point on the line: in this instance, you pay a dollar and fifty cents for any amount of grapes we get.\nRegression, fundamentally, is about fitting a line (or a plane) to a set of data given some inputs. Going forward, I will use the words “outcome variable” or “dependent variable” to refer to the thing that we are studying the change of, and “predictors”, “covariates”, or “independent variables” to refer to the variables we think affect our outcome. In the grape example, our outcome is the total amount of money we spend, and the singular predictor we use is how many pounds of grapes we get. With all this being said though, this example is very simplistic. After all, the necessary information is known to us up front (price and weight). But… what if the data we have at hand are not nice and neat in terms of a function? Suppose we consider a more challenging example.\nTake the idea of predicting crime rates in Brazilian states in the year 1990 using the inequality level as a predictor, or data on the consumption of cigarettes in American states in the year 1980 using price as a predictor. We would presume some function exists that generates the crime rate for that Brazilian state in that year, or that consumption level for that American state in that year. We would also imagine, naturally, that the covairates inequality and tobacco price would affect these outcomes. But, does it make sense to expect for some deterministic function to predict these values, given some input? No.\nThe homicide rate or cigarette consumption rate in any state anywhere is not guaranteed. In some states, homicides or tobacco consumption is high, other times its low. Why? Well for homicide, some states are wealthier and some are poorer. Some states vary by racial compositions, or will differ by factors like age composition, alcohol use, and gun ownership. Thus… some cities have high homicide rates, others have low homicide rates. We can reason accordingly for cigarette consumption of American states. Naturally, one reason for this would be the price of cigarettes, as one might expect, since people tend to not want to buy more of a good as the price increases (well… usually.) The number of young people in that state may mean that younger people are risk takers and may be more likely to smoke than adults (or alternatively, young people may perceive smoking as something for older adults and smoke less). Levels of alcohol taxation may matter as well, since alcohol may be a substitute for tobacco, so states with higher taxation may smoke more, on average. Also, plain and simple measures like culture (and other unobservable things) may play a role. We can plot these data for illustration\n\n\n\n\n\n\n\n\n\nHere, I draw a line between these input variables and the observed outcomes in question. The \\(x\\) axis represents, respectively, inequality and price and the \\(y\\) axes represent homicide rates and cigarette consumption. It is quite obvious though that no deterministic function exists here for either of these cases, as we have residuals. The line imperfectly describes the relationship between 1) inequality and homicide and 2) retail price of cigarettes and cigarette consumption per capita. So, we can’t find one line that fits perfectly to all of these data points. But, even if we cannot find a perfect, deterministic function that fits to all of these data points, how about we find the optimal line in the sense that it best estimates \\(m\\) and \\(b\\) by having the lowest possible values for \\(\\hat \\epsilon\\) at every point on the line? We can see how this relates to the grape analogy above: the line passes through all of the observed data points, meaning it is optimal in the sense that the line has the lowest possible residual values."
  },
  {
    "objectID": "ols.html#arrivederci-algebra-ciao-derivatives.",
    "href": "ols.html#arrivederci-algebra-ciao-derivatives.",
    "title": "7  OLS Explained",
    "section": "7.2 Arrivederci, Algebra, Ciao Derivatives.",
    "text": "7.2 Arrivederci, Algebra, Ciao Derivatives.\nTo do this though, we’ve now reached a point in the course where simple algebra is no longer our best guide. We now must use calculus, specifically the basics of derivatives and optimization. I mentioned the word optimal above to refer to the fit of the line, but now we’re going to get a much better sense of what is meant by this.\n\n\n\n\n\n\nImportant\n\n\n\nOkay, so here I’m kind of lying. You actually don’t need to say farewell to algebra (completely) to derive regression estimates, but that process “requires a ton of algebraic manipulation”. For those brave of heart who know algebra well, you can probably just watch the series of videos I just linked to and skip to this section of the notes, but I do not recommend this at all. I find the calculus way via optimization a lot more intuitive.\n\n\nFirstly though, a primer on derivatives. The derivative is the slope of a curve/line given a very small change in the value of the function. It is the instantaneous rate of change for a function. We do not need derivatives for linear functions, since we know what the rate of change is from real life (“a dollar fifty per pound”, “two for five”, etc.). For example, the derivative of \\(50x=y\\) is just 50, since that is the value y changes by for any increase in \\(x\\). For a constant (say, 5), the derivative is always 0, since no matter what value \\(x\\) takes, its value does not change at all.\nWhen we set the first derivative of a function to 0 and solve, we reach an optimal point on the original function, usually. An optimal point (or “critical point”) is a place on the function where the value of the function is at the lowest or highest possible value the function can reach over a given set of inputs. We can find the critical points by solving an optimization problem. An optimization problem takes the form of \\(f(x)\\) \\[\n\\operatorname*{arg min}_{\\theta \\in \\Theta } f(\\theta) \\: \\mathrm{ s.t. \\:}  g(\\theta) = 0, h(\\theta) \\leq 0,\n\\] where there’s some function \\(f(\\theta)\\) (called the objective function) that is minimized over a set of \\(g(\\theta)\\) equality constraints and \\(h(\\theta)\\) inequality constraints. The word “\\(\\operatorname*{arg min}\\)” here means “argument of the minimum”. It means that we are seeking the values, or arguments, which minimize the output of that function. The symbols underneath this, \\(\\theta \\in \\Theta\\) represents the coefficients we are solving for. These are the values which will minimize the function. But this is all abstract. Let’s do some examples.\n\n7.2.1 Power Rule\nSuppose we shoot a basketball while we stand on a 2 foot plateau, which produces a trajectory function of \\(h(t)= −5t^2 +20t+2\\). Here \\(h(t)\\) is a function representing the ball’s height over time in seconds. The \\(-5t^2\\) reflects the downward pull of gravity. The \\(20t\\) means that you threw the ball at 20 miles per hour originally. And the 2 means that we’re standing 2 feet above solid ground. We can find the maximum height of the ball by taking the derivative of the original quadratic function and solving it for 0. In this case, we use the power rule for derivatives. The power rule for dertivatives, expressed formally as \\(\\frac{d}{dx}(x^n) = nx^{n-1}\\), is where we subtract the exponent value of a function by 1 and place the original value to be multiplied by the base number. For example, the derivative of \\(y=2x^3\\) is just \\(6x^2\\), since \\(3-1=2\\) and \\(2 \\times 3 = 6\\). The derivative of \\(x^2\\) is \\(2x\\). With this in mind, we set up our objective function\n\\[\nH = \\operatorname*{arg max}_{t \\in \\mathbb R_{> 0}} \\left(−5t^2 +20t+2 \\right).\n\\]\nwhere \\(t\\) (a postitive real number) is time in seconds, expressed as \\(\\mathbb{R}_{>0} = \\left\\{ t \\in \\mathbb{R} \\mid x > 0 \\right\\}\\). We seek the value of \\(t\\) where the ball is at the maximum height possible.\n\n\n\n\n\n\n\n\n\nWe then follow the rules I’ve explained above, differentiating with respect to (w.r.t) each term.\n\\[\n\\begin{aligned}\n& h(t) = -5t^2 + 20t + 2 = \\\\\n& \\dfrac{\\mathrm{d}}{\\mathrm{d}t}(-5t^2) + \\dfrac{\\mathrm{d}}{\\mathrm{d}t}(20t) + \\dfrac{\\mathrm{d}}{\\mathrm{d}t}(2)= \\\\\n& t(5\\times 2) +1(20) = \\overbrace{-10t + 20}^{\\text{Derivative}}\n\\end{aligned}\n\\]\nLet’s break this down. The derivative of \\(-5x^2\\) must be \\(-10x\\) by the power rule. All this means is that for every additional second, the ball falls by ten more feet due to gravity. The 20 reflects the initial velocity that we threw the ball at, or the 20 feet per second I mentioned above. And the derivative for 2 is 0 because time has no influence on the height of ground we threw the ball from, we started from where we started from, gravity is what it is, and thus this should not affect the rate of change of the height of the ball. We can then solve the derivative for 0 to find the optimal point.\n\\[\n\\begin{aligned}\n& -10t + 20=0 \\Rightarrow  \\\\\n& -10t = -20 \\Rightarrow \\\\\n& \\frac{-10t}{-10} = \\frac{-20}{-10} \\Rightarrow \\\\\n& \\boxed{t=2}\n\\end{aligned}\n\\]\nOkay, so the ball is at its zenith after 2 seconds. We may now plug in the value of 2 into the original function to get the maximum height of the ball:\n\\[\n\\begin{aligned}\n& -5t^2 + 20t + 2 \\Rightarrow \\\\\n& -5(2)^2 + 20(2) + 2 \\Rightarrow \\\\\n& -20+40+2=22\n\\end{aligned}\n\\]\nFrom here, we can see why we set the derivative to 0. Since the derivative is the rate of change of the function at a specific point, and we know the ball is rising vertically since we shot it upwards, we also know the ball must be slowing down over time as it rises. The maximum point is simply the height where the speed of the ball is 0 miles per hour, and therefore not changing anymore so that it may be pulled to earth.\n\n\n\n\\(t\\)\n\\(h(t)\\)\n\\(h'(t)\\)\n\n\n\n\n0\n2\n20\n\n\n1\n17\n10\n\n\n2\n22\n0\n\n\n2.5\n21.25\n-5\n\n\n\nWe can know that we are at a maximum by taking the second derivative of our function, which would just be \\(-10\\). When the second derivative, expressed as \\(\\dfrac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}}< 0\\), we know that we are at a maximum point. And since \\(-10 < 0\\), we know that this is a maximum point.\n\n\n7.2.2 Chain Rule\nThe next rule of differentiation to know is something called the chain rule. The chain rule is called that because of the chain reaction we can think of when we think of the way the value of one function affects the value of another function. In mathematics this is called a composite function.\nA common example in economics is where we wish to maximize profits. Suppose we are the manager of shipping for a company. Our job is to ship kilos of product to a city so that a wholeseller, or a distributor, may sell to vendors who will in turn sell to customers. However, we are not doing business in a competitive market. The local distributor has a monopoly over entry ports, and is able to set prices. The distributor will give us 75,000 dollars per \\(x\\) kilos of product we give them. As the sellers, we must come up with the right amount of kilos to sell such that we maxmize our profits, given the price we face. Ideally, we could snap our fingers and sell them the product in an unlimited manner. In other words, the profit we’d make per ki would simply be \\(r(x) = 75x\\).\nBut unfortunately, we do have costs. We have a set of fixed costs comprising baseline expenditures of doing business with the distributor. For example, we may have transportation costs to move our product from home to the city, a certain amount of money for fuel, and other costs that we simply cannot avoid paying in order to do business at all (in the ball analogy, this would be how far we are off the grouund originally). We have variable costs which are things that we as the producers, in the very short run, have direct control over, say, how many drivers we have or how much plastic we use, the number of workers on our farms, and so on (in the ball analogy above, this would be how hard we throw the ball). And finally, we have marginal costs, or the costs that we bear as the business for each new ki produced (the downward pull of gravity in the above example).\nFor this example, our cost function is \\(c(x)=0.25(x+6)^2+191\\) (note that all of these numbers are in 1000s, but we shall solve the profit exactly as I’ve written it). As we’ve discussed above, the constant will at some point go away, and we will be left with only the factors of production that actually change (our marginal and variable costs). But I’m getting ahead of myself. First, we have to think about what profit means.\nIn microeconomic theory, our profit function is \\(\\pi(x)=r(x)-c(x)\\), or the difference between how much we make in dollars versus how much it costed us in dollars to produce all that we’ve sold. If we produced nothing, we’d be losing money since we still have to pay fixed costs (200 in this case). However, as we produce one more ki, we slowly increase the amount of money we make until our revenue equals our total costs. This is known as the break even point, when we’re not profiting or operating at a loss, we’re making just enough to remain in business. After this point, as we produce and sell more, our revenue begins to grow relative to our costs (say, as we get more efficient in distributing workers and tools to make the product). However at some point, the rate of change of profit will be equal to 0. Why? Well, we can’t keep producing forever because we do not have infinite resources. This means that while we may detect a change in profits from 1 kilo to 100, at some point it will not make sense to produce another kilo because the cost it takes to make another kilo is greater than the revenue we would make from selling it. While we’d still be making profit at that level, or profits would not be maximized.\nLike the ball example, we’re looking to ascend the profit function by producing more until the rate of change of profit is at 0. So, to maximize profit, we differentiate each component of the profit function, \\(\\dfrac{\\mathrm{d}\\pi(x)}{\\mathrm{d}x} = \\dfrac{\\mathrm{d}r(x)}{\\mathrm{d}x} - \\dfrac{\\mathrm{d}c(x)}{\\mathrm{d}x}\\). When we take the derivative of both revenue and cost functions, combine them together, and solve for 0, we can find the point at which our overall profits are maximized. As before, we may express this as an objective function, where our objective is to find the level of production value that maximizes our profit. Our objective function in this case looks like\n\\[\n\\operatorname*{arg max}_{x \\in \\mathbb R} \\left(\\underbrace{75x}_{r(x)} - [\\underbrace{0.25(x+6)^2+191}_{c(x)}] \\right).\n\\] We consider these functions separately \\[\n\\begin{aligned}\n&r(x) = 75x \\\\\n&c(x) = 0.25(x+6)^2 + 191\n\\end{aligned}\n\\] and then take their derivatives. Beginning with revenue, \\[\n\\begin{aligned}\n&r(x) = 75x \\\\\n&\\boxed{\\dfrac{\\mathrm{d}r}{\\mathrm{d}x} = 75}.\n\\end{aligned}\n\\]\nSimple enough. Profit increases by 75,000 per ki sold. Now, for \\(c(x) = 0.25(x+6)^2 + 191\\), we apply the chain rule. The outer function (or everything outside the parentheses here) is \\(u(y) = 0.25y^2 + 191.\\) The inner function is \\(y(x)=x+6\\).\nFirst, we differentiate the outer function \\(u(y)\\) by applying the power rule:\n\\[\n\\frac{\\mathrm{d}u}{\\mathrm{d}y} = 0.25 \\cdot 2y = 0.5y.\n\\]\nThe constant vanishes, and the 2 from the quadratic comes down and we multiply it by 0.25. Now we differentiate the inner function:\n\\[\n\\frac{\\mathrm{d}y}{\\mathrm{d}x} = \\frac{\\mathrm{d}}{\\mathrm{d}x}(x + 6) = 1.\n\\]\nThe chain rule states: \\(\\frac{\\mathrm{d}}{\\mathrm{d}x} u(y(x)) = \\frac{\\mathrm{d}u}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}x}\\). Applying the chain rule:\n\\[\n\\frac{\\mathrm{d}u}{\\mathrm{d}x} = \\frac{\\mathrm{d}u}{\\mathrm{d}y} \\cdot \\frac{\\mathrm{d}y}{\\mathrm{d}x} = 0.5y \\cdot 1 = 0.5(x + 6).\n\\]\nSo, we are left with:\n\\[\n\\frac{\\mathrm{d}u}{\\mathrm{d}x} = 0.5(x+6).\n\\]\nAfter distributing the one-half term:\n\\[\n0.5(x+6) = 0.5x + 0.5 \\cdot 6 = 0.5x + 3.\n\\]\nTherefore, the final result is:\n\\[\n\\boxed{0.5x + 3}.\n\\]\nNext, we combine the derivatives together in the original equation\n\\[\n\\begin{aligned}\n&\\dfrac{\\mathrm{d}\\pi(x)}{\\mathrm{d}x} = \\dfrac{\\mathrm{d}r(x)}{\\mathrm{d}x} - \\dfrac{\\mathrm{d}c(x)}{\\mathrm{d}x} = \\\\\n& 75 - (0.5x + 3) = \\\\\n&75-3-0.5x = \\\\\n&72 - 0.5x.\n\\end{aligned}\n\\]\nNow we solve for 0:\n\\[\n\\begin{aligned}\n&72 - 0.5x = 0 \\\\\n&72= 0.5x  \\\\\n&\\frac{72}{0.5}=\\frac{0.5}{0.5}x \\\\\n&\\boxed{\\text{Optimal Production Level: }144=x}.\n\\end{aligned}\n\\]\nTo verify that we are maximizing, we take the second derivative of the profit function, \\(\\dfrac{\\mathrm{d}^2 \\pi}{\\mathrm{d}x^2} = \\dfrac{\\mathrm{d}^2 r}{\\mathrm{d}x^2} - \\dfrac{\\mathrm{d}^2 c}{\\mathrm{d}x^2}\\). To do this, we do: \\[\n\\begin{aligned}\n&\\dfrac{\\mathrm{d}}{\\mathrm{d}x}=[75 - 0.5x - 3] = -0.5.\n\\end{aligned}\n\\] The derivative of the two constants are both 0, so those are deleted. All we’re left with is the linear term. Since \\(\\dfrac{\\mathrm{d}^2 \\pi}{\\mathrm{d}x^2}<0\\), we are at a maximum point. Therefore, the number of kilos we should sell our distributor is 144. What is profit? \\((75x) - (0.25(x+6)^2 + 191)\\) where \\(x = 144\\). First, calculate \\((x+6)^2\\): \\[\n(x+6)^2 = (144+6)^2 = 150^2\n\\] \\[\n150^2 = 22500\n\\]\nNext, calculate \\(0.25(x+6)^2\\): \\[\n0.25(x+6)^2 = 0.25 \\times 22500 = 5625\n\\]\nThis is the sum of our marginal costs and the variable costs. Next we calculate \\(75 \\times 144\\): \\[\n75 \\times 144 = 10800.\n\\]\nThis is our total revenue. Now we add the marginal, variable, and fixed costs: \\[\n5625 + 191 = 5816.\n\\]\nSubtract total revenue from total costs \\[\n10800 - 5816 = 4984\n\\]\nTherefore, since I said all these numbers were in 1000s, our profit from 144 kis sold is \\(\\boxed{4,984,000}\\). To verify that none of what we just did is voodoo, we can check this by plotting the profit function.\n\n\n\n\n\n\n\n\n\nOk, all done for now. We will use derivatives to solve for the value which minimizes the sum of residuals squared. This is known as ordinary least squares regression (OLS), also called linear regression, or simply just “regression”. OLS is the main estimator you’ll use for this class, and it is the main foundation of econometric analysis for public policy research. It will be much more involved than what we just did, but this provides the mathematical foundation for regression as an estimator."
  },
  {
    "objectID": "ols.html#an-extended-example",
    "href": "ols.html#an-extended-example",
    "title": "7  OLS Explained",
    "section": "7.3 An Extended Example",
    "text": "7.3 An Extended Example\nTo introduce OLS, we can return to the equation of a line (\\(y=mx+b\\)) where \\(m\\) and \\(b\\) are variables. Unlike the above examples where \\(m\\) and \\(b\\) were once known variables, now they are unknown quantities we must solve for. Below, \\(m\\) and \\(b\\) will take on the values of \\(\\beta_1\\) and \\(\\beta_0\\) respectively. With OLS, we have multiple predictors (typically), each of which affect the output of the function differently.\nIn the multivariable case, we take the partial derivative w.r.t. each variable (that is, assuming that the other variables do not change). If this seems at all abstract to you, I will provide a detailed, clear derivation of the betas. Note that for all of the steps below, Stata, R, or Python does (and optimizes!) this process for you. I only provide this derivation so you have a source to refer to when you wish to know how and why, exactly, the machine returns the numbers that it returns to you. I also believe a clear explanation of the math will help you understand how to interpret the results that we get.\nBefore we continue, let’s fix ideas. Suppose we wish to attend a nightclub and we wish to express how much we pay for that evening as a function (our outcome variable, a ratio level variable). At this nightclub, our outcome is a function of two things. We pay some one-time cost of money to enter, and then we pay some amount of money per new drink we buy (where number of drinks is also a ratio level variable). I say “some” because unlike the real world where we would know the price and entry fees by reading the sign, in this case we wish to estimate these values with only two known variables: how many drinks we bought and how much we paid.\n\n\n\n\n\n\n\n\n\n\n7.3.1 List the Data\nSay that we have data that looks like \\((0, 30), (1, 35), (2, 40)\\), where \\(x\\)= number of drinks we buy (0,1,2) and \\(y\\)=amount of money we spend that evening (30,35,40). In spreadsheet format, this looks like:\n\n\n\nx\ny\n\n\n\n\n0\n30\n\n\n1\n35\n\n\n2\n40\n\n\n\nIf you want to, calculate the rise-over-run of these data points to derive \\(m\\) and see what the answer might be in the end. Below though, we proceed by deriving what \\(m\\) must be.\n\\[\nm = \\frac{35 - 30}{1 - 0} = \\ldots\n\\]\n\n\n7.3.2 Define Our Econometric Model\nWe begin by defining our model. That is, we specify our outcome and the variables which affect our outcome (the values we’re solving for, entry price and drink cost). Our model of how much we pay given some entry fees and additional drink costs looks like:\n\\[\ny_i = \\beta_0 + \\beta_1x_i\n\\]\nHere, \\(y_i\\) is the total amount of money we spend that evening in dollars given the \\(i\\text{-th}\\) drink, \\(\\hat{\\beta}_{0}\\) is how much we pay (also in dollars) to enter, \\(\\hat{\\beta}_{1}\\) is how much we pay for the \\(i\\text{-th}\\) drink, and \\(x\\) is the total number of drinks we get. Nothing at all about this is different, so far, from anything we’ve discussed above. I’ve simply substituted \\(m\\) and \\(b\\) with the Greek letter \\(\\beta\\) (“beta”) into the already familiar equation for a line.\n\n\n7.3.3 Write Out the Objective Function\nNow that we have our model, next let’s think about what the objective function would be. We already know that we wish to minimize the residuals of the OLS line. So, we can represent the objective function for OLS as \\[\n\\begin{aligned}\n& S = {\\text{argmin}} \\sum_{i=1}^{n} \\hat{\\epsilon}^{2}  \\\\\n& S = {\\text{argmin}} \\sum_{i=1}^{n} \\left( y_i - \\hat{y} \\right)^{2} \\\\\n& \\text{where } \\hat{y} \\text{ is defined as} \\\\\n& S = \\underset{\\hat{\\beta}_0,\\hat{\\beta}_1}{\\text{argmin}} \\sum_{i=1}^{n} (y_i - (\\overbrace{\\hat{\\beta}_0 + \\hat{\\beta}_1x}^{\\text{Predictions}}))^2.\n\\end{aligned}\n\\]\nAs above, we call the solutions \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\) optimal if they return the lowest possible values the function \\(S\\) can produce. What values can \\(S\\) produce? The sum of the squared residuals. The sigma symbol \\(\\sum_{i=1}^{n}\\) means we are adding up the \\(i\\text{-th}\\) squared residual to the \\(n\\text{-th}\\) data point/number of observations (in this case 3). This means that the line we compute will be as close as it can be to the observed data at every single data point. By the way, just to show the objective function is not an optical illusion or arcane expression, we can literally plot the objective function in three dimensions, where we have the slope and intercept values plotted against our total squared residuals.\n\n\n\n\n\n\n\n\n\nI plot the values returned by the objective function given the datapoints and some value for our coefficients. Of course, we wish to get the values for the betas which produce the lowest possible values for this plane. Clearly, the intercept can’t be 50 and the slope 20 (as this maximizes the residuals!).\nThe middle formula, \\(\\left( y_i - \\hat{y} \\right)^{2}\\), re-expresses the residuals. This expression should look familiar. Recall the formula for the variance of a variable \\(\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\\) ? Here, we are taking our real observations \\(y\\) and subtracting some expected value \\(\\hat{y}\\) (y-hat) from it. Because we are minimizing the residuals (or, the model prediction error), another way of thinking about OLS is that is the line that maximizes the explained variance given our independent variables. The lines that has the least-wrong predictions is also the line that explains its variation patterns best.\nAs with the variance, one may ask why we are squaring the summed \\(\\hat{\\epsilon}_i\\), instead of say the absolute error. First of all, minimizing the raw sum of the predicted residuals (that is, without squaring them) is a non-differentiable function. We could use the raw sum of the errors as an objective function (called the mean absolute error instead of the mean squared error), but have fun doing that, as due to the non-differentiable nature of the absolute value function, we would need to use numerical methods, such as gradient descent, to compute its solution. By no means impossible… just computationally less tractable.\nUsing the squared residuals means that we are dealing a quadratic function which, as we did above, is easily differentiable. The squaring of residuals also penalizes worse predictions. Indeed, just as with the variance, all residuals should not be created equally. If the observed value is 20 but we predict 25, the residual is -5. Its squared residual is 25. But if the observed value is 40, and we predict 80, the “absolute” error is -40 and the squared error of is 1600. If we did not square them, we would be treating a residual of 5 as the same weight as a residual of 40. For proof, we can plot these\n\n\n\n\n\n\n\n\n\n\n\n7.3.4 Substitute Into the Objective Function\nFirst, we can substitute the real values as well as our model for prediction into the objective function. We already know the values x-takes. You either buy no drinks, 1 drink, or 2. So with this information, we can now find the amount of money we pay up front (\\(\\hat{\\beta}_0\\)) and how much it costs for each drink (\\(\\hat{\\beta}_1\\)) \\[\n\\begin{aligned}\nS = &\\underbrace{(30 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 0))^2}_{\\text{Term 1}} + \\\\\n&\\underbrace{(35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1))^2}_{\\text{Term 2}} + \\\\\n& \\underbrace{(40 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 2))^2}_{\\text{Term 3}}\n\\end{aligned}\n\\]\nAnd when we look at the notation carefully, we see all of this makes sense: we are adding up the differences between our outcome, and the predictions of our model.\n\n\n7.3.5 Take Partial Derivatives\nTo find the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), we take the partial derivatives of \\(S\\) with respect to (w.r.t.) \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). Here is a short sketch of how we do this: For simplicity, I break this into two sections, one section per coefficient. In this case, the chain rule and power rules for differentiation are our friends here. To hear more about combining the power rule and chain rule, see here. First, we differentiate w.r.t. \\(\\hat{\\beta}_0\\) (entry fees), then we do the same for \\(\\hat{\\beta}_1\\) (drink fees).\n\nPartial derivative w.r.t. \\(\\hat{\\beta}_0\\):\n\nHere is our full objective function:\n\\[\n\\frac{\\partial S}{\\partial \\hat{\\beta}_0} = \\frac{\\partial}{\\partial \\hat{\\beta}_0} \\left[ (30 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 0))^2 + (35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1))^2 + (40 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 2))^2 \\right].\n\\]\nBy the chain rule, we can take the partial derivative by applying the power rule to the outer functions and the linear differentiation rule to the inner functions: \\[\n\\frac{\\partial S}{\\partial \\hat{\\beta}_0} = \\frac{\\partial}{\\partial \\hat{\\beta}_0} (30 - \\hat{\\beta}_0+ \\hat{\\beta}_1 \\cdot 0)^2 + \\frac{\\partial}{\\partial \\hat{\\beta}_0} (35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1))^2 + \\frac{\\partial}{\\partial \\hat{\\beta}_0} (40 - (\\hat{\\beta}_0 + 2\\hat{\\beta}_1))^2.\n\\]\nIn the first term, we have \\(\\hat{\\beta}_1 \\cdot 0\\), so we keep the other part of the function but the \\(\\hat{\\beta}_1\\) goes away since that’s what anything multiplied by 0 means. So, the quadratic power goes outside the parentheses, and the derivative of \\(-\\hat{\\beta}_1\\) is just \\(-1\\). So by application of the chain rule, we get this result:\n\\[\n\\begin{aligned}\n& \\frac{\\partial S}{\\partial \\hat{\\beta}_0} = 2(30 - (\\hat{\\beta}_0)) \\cdot (-1) + 2(35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1)) \\cdot (-1) + 2(40 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 2)) \\cdot (-1).\n\\end{aligned}\n\\]\nSee how all these three terms are multiplied by \\(-1\\) and \\(2\\)? Well, by distributive property, we know we can factor out the 2 and negative 1. That returns this result:\n\\[\n\\frac{\\partial S}{\\partial \\hat{\\beta}_0} = -2 \\left[ (30 - \\hat{\\beta}_0) + (35 - \\hat{\\beta}_0 - \\hat{\\beta}_1) + (40 - \\hat{\\beta}_0 - 2\\hat{\\beta}_1) \\right].\n\\]\nNext I rearrange everything inside of the brackets: \\[\n\\begin{aligned}\n&(30 + 35 + 40) - (\\hat{\\beta}_0 - \\hat{\\beta}_0 + \\hat{\\beta}_0) + (\\hat{\\beta}_1 - 2\\hat{\\beta}_1) = \\\\\n&(105)+(- 3\\hat{\\beta}_0)+(-3\\hat{\\beta}_1).\n\\end{aligned}\n\\] Finally, we just distribute the 2:\n\\[\n\\begin{aligned}\n-2 \\left[ 105 - 3\\hat{\\beta}_0 - 3\\hat{\\beta}_1 \\right] \\\\\n= \\boxed{-210 + 6\\hat{\\beta}_0 + 6\\hat{\\beta}_1}.\n\\end{aligned}\n\\]\nThis is our partial derivative for the first coefficient, or for the entry fee.\n\nPartial derivative w.r.t. \\(\\hat{\\beta}_1\\):\n\nWe can follow a similar process for this partial derivative: \\[\n\\frac{\\partial S}{\\partial \\hat{\\beta}_1} = \\frac{\\partial}{\\partial \\hat{\\beta}_1} \\left[ (30 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 0))^2 + (35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1))^2 + (40 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 2))^2 \\right]\n\\]\nUsing the chain rule, this looks like:\n\\[\n\\frac{\\partial S}{\\partial \\hat{\\beta}_1} = \\frac{\\partial}{\\partial \\hat{\\beta}_1} (30 - \\hat{\\beta}_0)^2 + \\frac{\\partial}{\\partial \\hat{\\beta}_1} (35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1))^2 + \\frac{\\partial}{\\partial \\hat{\\beta}_1} (40 - (\\hat{\\beta}_0 + 2\\hat{\\beta}_1))^2.\n\\]\nWe apply the power rule to the outer terms and the linear differentiation rules to each inner term. As before, the 2 simply goes in from of the parentheses and the derivative of \\(\\hat{\\beta}_1\\) is taken. Note that for the first term, \\(\\hat{\\beta}_1\\) is multiplied by 0, so since this is multiplied by the outer function, the first term vanishes completely.\n\\[\n\\frac{\\partial S}{\\partial \\hat{\\beta}_1} = 2(2(35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1)) \\cdot (-1) + 2(40 - (\\hat{\\beta}_0 + 2\\hat{\\beta}_1)) \\cdot (-2).\n\\]\nAs we can see, the 2 again is a common term, which we again put outside in brackets:\n\\[\n\\frac{\\partial S}{\\partial \\hat{\\beta}_1} = 2 \\left[- 1 \\cdot (35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1)) - 2 \\cdot (40 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 2)) \\right].\n\\]\nWhen we simplify the inner terms, we get:\n\\[\n\\frac{\\partial S}{\\partial \\hat{\\beta}_1} = 2 \\left[ - (35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1)) - 2 \\cdot (40 - (\\hat{\\beta}_0 + 2\\hat{\\beta}_1)) \\right].\n\\]\nWe apply the distributive property again for the 1 and 2:\n\\[\n2 \\left[ -35 + \\hat{\\beta}_0 + \\hat{\\beta}_1 - 80 + 2\\hat{\\beta}_0 + 4\\hat{\\beta}_1 \\right].\n\\]\nNow we rearrange by putting the same terms next to each other:\n\\[\n2 \\left[ \\hat{\\beta}_0 + 2\\hat{\\beta}_0 + \\hat{\\beta}_1 + 4\\hat{\\beta}_1 - 35 - 80 \\right].\n\\]\nand simplify by combining them together:\n\\[\n2 \\left[ 3\\hat{\\beta}_0 + 5\\hat{\\beta}_1 - 115 \\right].\n\\]\nThus after distributing the 2 inside of the brackets, the partial derivative of \\(S\\) with respect to \\(\\hat{\\beta}_1\\) is:\n\\[\n\\boxed{6\\hat{\\beta}_0 + 10\\hat{\\beta}_1 - 230}.\n\\]\nNow we’ve taken the partial derivatives of both our variables, entry fees (which we presume are constant) and the number of drinks we buy. This can be represented like:\n\\[\n\\nabla S = \\begin{bmatrix}\n\\frac{\\partial S}{\\partial \\hat{\\beta}_0} \\\\\n\\frac{\\partial S}{\\partial \\hat{\\beta}_1}\n\\end{bmatrix}\n= \\begin{bmatrix}\n-210 + 6\\hat{\\beta}_0 + 6\\hat{\\beta}_1 \\\\\n-230 + 6\\hat{\\beta}_0 + 10\\hat{\\beta}_1\n\\end{bmatrix}.\n\\]\nTechnically, in mathematics, we’d call this the gradient. Before we continue though, do not lose sight of our goal: all these two equations represent are the instantaneous rates of change in our sum of squared residuals given some change in the variable in question. Our goal is still to find the the values for these betas that minimize our objective function.\n\n\n7.3.6 Get the Betas\nOkay, no more calculus. We can now return to algebraland to get our betas, with a slight modification.\nRemember how above after we calculated the normal derivative of the profit function or the ball trajectory we just solved the derivative for 0? In that case, we had only one variable, \\(t\\) or \\(x\\). Well, now we don’t just have one variable! We have 2 \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\).\nAs before, we still want to set these both equal to 0 because at the point both equal 0, our sum of squared residuals is no longer rising or falling (or, it is the the critical point on the surface I plotted above). So, let’s write these equations again (I divided the first equation by 6 since all of its terms were divisible by 6). To solve both equations simultaneously, first we add the constants to both RHS of both partial derivatives: \\[\n\\begin{aligned}\n&\\hat{\\beta}_0 + \\hat{\\beta}_1 = 35 \\\\\n&6\\hat{\\beta}_0 + 10\\hat{\\beta}_1 = 230.\n\\end{aligned}\n\\]\nHere I use a method called substitution to solve the system, but there are many such ways we can solve this. In substitution, we solve one equation first and substitute the solution for a variable into the other equation. I solve the first parital for \\(\\hat{\\beta}_0\\) since it is the easiest. So, we subtract \\(\\hat{\\beta}_1\\):\n\\[\n\\begin{aligned}\n&\\hat{\\beta}_0 + \\hat{\\beta}_1 = 35 \\Rightarrow \\\\\n&\\hat{\\beta}_0 = 35 - \\hat{\\beta}_1.\n\\end{aligned}\n\\]\nOkay, so this is our expression for \\(\\beta_0\\). Since we now know the expression for the constant (the entry fee), we can plug this into the partial for \\(\\hat{\\beta}_1\\) where the \\(\\beta_0\\) currently is and solve for \\(\\hat{\\beta}_1\\). We do: \\[\n\\begin{aligned}\n&6\\hat{\\beta}_0 + 10\\hat{\\beta}_1 = 230 \\\\\n&6(35 - \\hat{\\beta}_1) + 10\\hat{\\beta}_1 = 230.\n\\end{aligned}\n\\] Next, we distribute the 6 \\[\n210 - 6\\hat{\\beta}_1 + 10\\hat{\\beta}_1 = 230\n\\] and combine the terms \\(- 6\\hat{\\beta}_1 + 10\\hat{\\beta}_1\\) together. That returns this result: \\[\n210 + 4\\hat{\\beta}_1 = 230.\n\\] Next, we subtract 210 \\[\n4\\hat{\\beta}_1 = 20.\n\\] Finally, we divide by 4 \\[\n\\boxed{\\hat{\\beta}_1 = 5}.\n\\]\nNow, we know our value for \\(\\hat{\\beta}_1\\)!!! We know that for each drink we get, we pay 5 more dollars. Since we now know this, we substitute 5 into \\(\\hat{\\beta}_0 + \\hat{\\beta}_1 = 35\\) where \\(\\hat{\\beta}_1\\) is. Then, we have one equation to solve for, with our goal being to get the value of \\(\\hat{\\beta}_0\\). We can put this value into the partial derivative for \\(\\hat{\\beta}_0\\): \\[\n\\hat{\\beta}_0 + 5 = 35.\n\\]\nNow, we simply subtract 5 from the RHS \\[\n\\boxed{\\hat{\\beta}_0 = 30}.\n\\]\nThe entry fee is 30 dollars.\n\n\n7.3.7 Our OLS Line of Best Fit\n\n\n\n\n\n\n\n\n\nSo, our line of best fit is \\(\\hat{y} = 30 + 5x\\). In social science, you’ll hear people throw around terms like “when we controlled for this” or “adjusted for” another variable, or “when we hold constant these set of variables”. This is what they mean by it! They, in the plainest language possible, mean that the dependent variable changes by that amount for every unitary increase in an independent variable, assuming the other values of the function do not change change. That’s exactly what the partial derivative is, the change in a function given a change in one variable for that function. So here, assuming the club has a flat entry fee that does not change on a person to person basis, the amount the function changes by for every new drink is an increase of 5 dollars. Or, compared to the scenario where you only wanted to get in the club (and not drink at all, where \\(x=0\\)), you spend 5 more dollars per each new drink you get. One may ask why we did this at all. Why bother with the partial derivative approach and the messy system of equations, why not simply display a regression table and go through the practical interpretation? After all, assuming we just did the following in Stata: reg y x, we would’ve gotten the exact same set of results that I just did quicker.\nThe primary reason is pedagogical. OLS was never derived for me in quite this manner in undergrad. So I believe you should see it done with a simple, tractable, familiar example, even though you’ll never do this for any work you ever do. This way, OLS is not a computerized black box you mindlessly use for a dataset- you actually can see where the numbers come from in a simplified way."
  },
  {
    "objectID": "ols.html#inference-for-ols",
    "href": "ols.html#inference-for-ols",
    "title": "7  OLS Explained",
    "section": "7.4 Inference For OLS",
    "text": "7.4 Inference For OLS\nNow that we’ve conducted estimation, we can now conduct inference with these statistics we’ve generated. Indeed, this is the primary point of this at all, in a sense. We want to know if these estimates are different from some null hypothesis. To begin, recall the notation of \\(\\hat{\\epsilon}_i\\) which denotes our residuals for the regression predictions. We can use this to generate the standard error/the uncertainty statistic associated with the respective regression estimate. We can begin with the residual sum of squares, calcualted like \\(\\text{RSS} = \\sum (\\hat{\\epsilon}_{i})^2\\). Put another way, it is all the variation not explained by our model. If \\(RSS=0\\), as was the case in the above example, then we have no need for inference since there’s nothing our model does not explain. We then can estimate the variance of the error like \\(\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n - p}\\), where \\(n\\) is our number of observations and \\(p\\) is our number of predictors (including the constant). We divide by \\(n-p\\) because this takes into account our model’s residual degrees of freedom, or our model’s freedom to vary. Note as well that when \\(n=p\\), the error variance is not defined, meaning for OLS to be valid we need less predictors than observations. For a more detailed explanation of degrees of freedom, see Pandey and Bright (2008).\nFor an example of how a regression table is presented, consider the above example that estimates the impact of tobacco prices on consumption across states\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nT-statistic\n\\(R^2\\)\n\n\n\n\nIntercept\n323\n56.3691\n-3.31\n\n\n\nCoefficient (retprice)\n-3.17\n.958191\n-3.028\n0.22\n\n\n\nAs we know from above, this suggests that a one dollar increase in the price of cigarettes implies a reduction of 3 in the rate of tobacco sales per capita. As we’ve discussed with normal descriptive statistics/classic hypothesis testing, we can also compute confidence intervals for these regression estimates. To do this, we need a standard error for our regression estimates. We compute:\n\\[\n\\frac{\\frac{\\text{RSS}}{n-(k+1)}}{\\sum(x_i- \\bar x)^2}\n\\] We already know RSS from above. Then, we add the differences of each point for \\(x\\) and its mean. This is:\n\\[\n\\frac{\\frac{26015.2655}{37}}{765.81}=0.958191.\n\\]\nNow that we have this, we can calculate the t-statistic for the beta, which is simply the coefficient divided by the standard error. We can also calculate confidence intervals for coefficients too. The formula for this should appear quite familiar \\[\n\\beta_j \\pm t_{\\alpha/2, \\text{df}} \\cdot \\text{SE}(\\beta_j)\n\\]\nHere, \\(\\beta_j\\) is the coefficient of our model, \\(t\\) is our test statistic (1.96 ususally), \\(\\alpha\\) is our acceptance region (0.05 in most cases if we want a 95% confidence interval), SE is our standard error as we’ve computed it above. For the price example, we do \\(-3.171357+(1.96 \\times .958191)\\) and \\(-3.171357-(1.96 \\times .958191)\\), returning a 95% CI of \\([-5.112836, -1.229877]\\). There’s a little rounding error, but that’s what we get. The way to interpret the CI is as follows: given our sample size and input data, the true parameter of the effect of price on tobacco smoking rates lies within the range of -5.1 and -1.2. In other words, if some assumptions hold (which we will discuss below), a dollar increase in price may decrease the tobacco smoking rate by as much as 5 or as little as 1.\nJust as we discussed in the preceding chapters, lots of statistics is justified asymptotically, based on the law of large numbers and CLT. In other words, as \\(n\\) tends to infinity, \\(\\lim\\limits_{n\\to \\infty}\\), our betas will converege to the true population value and the standard errors will shrink. Ergo, as these shrink, the confidence intervals will tighten, meaning our estimates will be more precise. A practical consequence of this is that as a very general rule, having more observations in your dataset makes your OLS estimates more precise and less biased. For the above for example, we would not trust these estimates as much because they come from one year only. Ideally, to get a better sense of how price increases affect consumption, we’d need to collect these observations over time and adjust for other things that may affect cigarette consumption.\n\n7.4.1 Goodness of Fit Measures for OLS\nWe typically thhink of two goodness of fit statistics when using OLS, the R-squared statistics and the Root Mean Squared Error\n\\[\n\\begin{aligned}\nR^2 &= 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}} & \\text{RMSE} &= \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}.\n\\end{aligned}\n\\] Here, for R-squared, we have two terms: first, \\(\\text{SS}_{\\text{res}} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) is the sum of squared residuals. \\(\\text{SS}_{\\text{res}}\\) quantifies the amount of variance unexplained by the independent variables in the model, and \\(\\text{SS}_{\\text{tot}}\\) is just the total amount of variance of \\(y\\). Think of R-squared as a ratio/percentage of how well the model explains our outcomes. An R-squared of 1 reflects the simple example we derived above, where the model perfectly explains the variation of our outcomes. R-squared usually scales with the amount of predictors in the model (that is, as we add more variables, the R-squared will increase). However, I think the best way to think about R-squared is a measure of how well our model does, compared to the average of our outcomes. In the nightclub example, if we just took the average of our outcomes, we’d guess for that evening, you’d spend 28.3 dollars. But, in this case, the model significantly outperforms this since it explains the variation perfectly. Note, that it is possible to have a negative R-squared. It is very rare, and it basically means that your model does a worse job than the simple average of the outcomes. I’ve seen this in my work, but it is very rare. I’ve only encountered it in the wild maybe twice. In the above example, including just price as a predictor explains about 22 percent of the variation, which is not bad considering it’s only one variable!\nThe Root Mean Squared error, or RMSE, is exactly as it sounds: it is the square root of our average of our \\(\\text{SS}_{\\text{res}}\\). For the tobacco, example, our RMSE is \\(\\frac{26015.2655}{3726.516}=26.516321\\). In English, this simply means that when we use price to explain consumption for the year 1980, the model is off, on average, by about 27 packs. Again, considering that the average cigarette consumption in 1980 was 137,being off by 26 packs isn’t so bad. It suggests, as one would expect, that we’ve explained our dataset fairly well using price as an explanatory variable. As RMSE approaches 0, we explain our variation better, with an RMSE of 0 being perfection. Note that other goodness of fit metrics do exist; however, these are the most common ones you’ll encounter."
  },
  {
    "objectID": "ols.html#assumptions-of-ols",
    "href": "ols.html#assumptions-of-ols",
    "title": "7  OLS Explained",
    "section": "7.5 Assumptions of OLS",
    "text": "7.5 Assumptions of OLS\nKeep in mind, despite all the detail we’ve discussed so far, do not lose sight of the larger picture: OLS is simply a mathematical estimation method. Its validity for explaining the external world (aside from having quality data) relies on a few assumptions (collectively called the Gauss-Markov assumptions) being defensible. I say defensible instead of true because practically they are never true. After all, this is statistics: almost all of statistics is true. All statistical research (pretty much, outside of simulations) is at least partly wrong because we live in a probalistic world where we don’t have all the answers. In other words, the assumptions are only as valid as we can defend for them. Below, I detail them.\n\n7.5.1 Assumption 1: Linear in Parameters\nThe very first assumption is that the parameters for our model are linear. The classic regression model for OLS is\n\\[\ny_{i} = \\beta_{0} + \\beta_{1} x_{i1} + \\cdots + \\beta_{K} x_{iK} + \\epsilon_{i}.\n\\]\nWe call this a linear model. Why? How do we know if it’s a linear relationship, and what might violations of this look like? Let’s say we’re buying weed. Say the price per quarter ounce is 80 dollars. The impact of \\(\\beta_{1}\\) is the same everywhere in the function, \\(y=80x\\). But step back and ask ourselves, from the seller’s persepctive, if this makes sense: does it make sense for weed to cost the same for every weight amount? No! Why not? Well, for one, let’s say you’re selling a full gram or pound of weed. That’s so much weed that weed(wo)men/people will charge much much more for lone individuals who wish to buy this much. So while it may be 80 for a quarter ounce, it’ll now be, say, 900 per pound. In fact, we could express this as a piecewise function\n\\[\n\\beta_{j} =\n\\begin{cases}\n    80 \\text{ if } x < 1 \\\\\n    900 \\text{ if } x > 1. \\\\\n\\end{cases}\n\\]\nWhy might this be done? Firstly, that’s so much more product than the average person could smoke or use. So, anyone interested in this would need to pay premium prices for such an outlandish amount. Also, it allows the dealer to get the pound of weed off their hands– relative to ounces, pounds of weed are much more likely to be noticed by police and therefore punished by the law harsher. So, the quicker they sell, the quicker they may re-up. So, for the normal crowd of people who do not but pounds, they pay one price. For those who are abnormal in how much they demand (say, the distributor for the connect for cocaine markets), they pay another price altogether. We see price discrimination in legal markets too, such as Sams Club. We can see that a regression model here IS NOT linear in parameters, since the slope of the line will change at different values of the function.\nPeople often confuse this assumption with non-linear values of our independent variables as they relate to our outcome. They conflate nonlinear regression \\[\ny_{i} = \\beta_{0}^{2} + \\beta_{1}^{2} x_{i1} + \\cdots + \\beta_{K}^{2} x_{iK} + \\epsilon_{i}\n\\] with \\[\ny_{i} = \\beta_{0} + \\beta_{1} x_{i1}^{2} + \\cdots + \\beta_{K} x_{iK}^{2} + \\epsilon_{i},\n\\] or an OLS model with non-linear relations between the inputs and the outcomes. Let me explain why this is wrong, because as it turns out, we can indeed model curvature. I’ve already given an example of when we’d have a nonlinear realtionship in terms of our betas. Now I discuss non-linearities in terms of our predictors. Let’s say we wish to model how much someone can dead lift given some input of age. Let’s say the true population parameter for the OLS model is 6 (we ignore the constant for exposition purposes) \\[\ny_{i} = 6x_{i}\n\\] What is our value for 0? 0, since you’re not yet born. For age 10? 60. For age 30? 180. For age 80? 480. I think we can already see why this relationship being modeled would be silly: it presumes that the older you get, the stronger one is as a hard and fast rule. Which, generally speaking, is true… but we also know that at some point, as with all things, glory fades. Someone that was once strong and in shape will not (in general) always be that way because the body declines with the passage of time. How do we take this into account for our regression model, though?\n\\[\ny_{i} = \\beta_{1}(x_{i1} \\times x_{i}) +  x_{i} \\equiv y_{i} = \\beta_{1}x^{2}_{i}\n\\] We simply square the original value of age, keeping its linear form in the regression model. That way, when age 4 is input in the model, the number our regression model reads in the computer is 16. When age 10 is put into the model, it reads 100. Of course, as one would expect, there’s likely some critical point for this function, where people begin to be able to lift less given some values of age. We never know this of course, but OLS can be used to estimate it in the manner that we’ve done.\nAnother example of being able to account for non-linearities from economics is the idea of modeling how much produce one may produce given a set of labor inputs. Suppose we’re cooking cocaine. With just two people, you can get work done, but it won’t be a lot. With three people, you can do more, and more with each additional person. However, there’s an idea in economics called diminishing marginal returns for the factors of production (in this case labor). You may be able to cook a lot with 10 or 20 people, but when you have 40 or 50 people, at some point we end up producing less because there’s too many proverbial cooks in the kitchen. So, if we wished to model output of cocaine as a function of labor, we’d likely wish to square the “number of workers” part of our model since it does not make sense to expect production to increase perfectly linearly with every new human working with you. So you see, the linear in parameters assumption deals with our betas impact on our predictor variables, not the input values of our predictor variables.\nNote that when we include such terms in our model, called interaction terms in econometrics (I may cover this more later), we must include the linear term in the model as well. In Stata, this would look something like reg y c.labor##c.labor. Under the hood, this includes in the model reg y labor labor2.\n\n\n7.5.2 Assumption 2: Random Sample\nWe next presume that we’ve collected a random sample of our population. The name random sample is something of an antiquated, romantic name to denote the idea that the sample we’ve collected is representative of the population we wish to map on to. Suppose we wish to investigate the relationship between introducing all virtual menus at a restaurant (the kind you scan on your phone) to see if it increases how much money they make for [random marketing reasons]. We take all the restaurants in Buckhead and Sandy Springs in Atlanta as a sample, comparing the ones that did this intervention to the ones that didn’t do the intervention. We get a coefficient of 10,000, suggesting that this intervention increased money made, on average, by 10,000 dollars compared to the places that didn’t do this. The issue with this idea is that our sample is not a random sample of the population. Sandy Springs and Buckhead, in particular, are among the wealthiest areas in Atlanta. We can’t generalize the effet of this intervention to the population (restuarants in Atlanta, say) becuase our units of intertested are decidedly not representative of Atlanta’s entire culinary scence. They have very specific customers that make a generalization to the bigger population a bad idea.\nAnother example can come from sports. Say we posit a relationship between height and skill at basketball. We take a sample of NBA players, run a few regressions for relevant metrics and have our software spit out coefficients at us. Can we generalize to the population? No!! The NBA is one of the most selective sports leagues on the planet. The NBA selects for height and skill, among other things. The worst player on the NBA bench is like a god from Olympus compared to the average human, physically and in terms of skill. They are not representative of even a human 2 standard deviations above the mean.\nSo, we cannot use NBA players generalize to the population, unless of course we are concerned only with NBA players. The same would apply to the first example: if we care only about the high-income restaurants in the city, then that’s great, but assuming we wish to generalize more broadly, we will need more data from other, more diverse units that have more information encoded in their outcome about the sample.\n\n\n7.5.3 Assumption 3: No Perfect Collinearity\nThe simple way to think about this one is we cannot include redundant variables in our regressions. Suppose we wish to predict the ticket sales of NBA teams. In our regression, we include the number of games won as well as the number of games lost (2 variables). Well, these are mirror images of each other. The number of games you won is a direct function of the total games minus the number you lost, and the number you lost is a direct and perfect function of the total minus the number you won.\nBy extension, suppose we wish to compare women to men (say we wish to test that men earn more/less than women on average). We take data on 500 respondents who we’ve sampled randomly across a company. We have one column that denotes the respondent as male and the other as female. We cannot include both male and female columns in our models, these are perfect linear functions of one another. A female is necessarily not coded as male, and male is necessarily not coded as female. Practically, this means we must choose when we use a categorical variable in our models. Say our regression includes age ang gender as a predictor. If category 1 of gender is female and category 0 is male, then if the beta for “gender” is -30, we would interpret the beta for gender as “holding constant age/compared to men of the same age group, female respondents earn about 30 dollars less than men.” By extension, the coefficient for male (if we decided to include this group as the group of interest) would just be 30, with a similar interpretation in the other direction.\n\n\n7.5.4 Assumption 4: Strict Exogeneity: \\(\\mathbb{E}[\\epsilon_{i} | x_{i1}, \\ldots, x_{iK}] = 0\\)\nNext we presume strict exogeneity. Formally, this means the average of our errors, given the set of covariates we’ve controlled for, is 0. It means our predictor variables may not be correlated with the error term. Note that the error term is different from the residuals: the error term includes unobserved characteristics that also may affect the outcome. In other words, we cannot omit important variables from our model.\nFor example, say we wish to see how the number of firefighters sent to a call affects the amount of damage from that fire. We wish to measure the association in other words. We conclude that there’s a positive relationship between number of firefighters sent and damage. Say, we use the number of trucks sent to a call to predict the damage in dollars for a sample size of 10,000,000 calls. We find from the bivariate model that every truck you send increases damage by 30,000 dollars. So, we elect to send less people to future calls. Is this a good idea? No!!!! People will die like that.\nPresumably, the firefighters are not pouring gasoline on the fire, so perhaps we’ve omitted things from our model that might influence both how many people we send as well as fire damage. What else should we control for? Maybe, building size, building type, neighborhood income status, local temperature, and other relevant predictors to ensure that we are not blaming the outcomes on a spurious relationship. Indeed, on some level we would expect for the size of the fire to be correlated with the number of people sent to fight it. Thus, when we do not control for other relevant factors, our coefficients, no matter how precise, suffer from omitted variable bias. Strict exogeneity is pretty much never met in real life, but it basically posits that there’s no other critical variable missing from our regression model that may explain our outcome. This is also why it matters to critically think about the variables one will use in their regression model before they run regressions."
  },
  {
    "objectID": "treatmenteffects.html#what-is-causality",
    "href": "treatmenteffects.html#what-is-causality",
    "title": "8  Causal Inference",
    "section": "8.1 What Is Causality?",
    "text": "8.1 What Is Causality?\nAs I mentioned in the chapter on correlation, humans have evolved to think hypothetically. It is how we have survived for as long as we have. Causal inference demands that we imagine another world that we believe could exist, but doesn’t exist. In history, we’d call this a “counterfactual”, so termed because we are talking about a fictional scenario that happened in contrast to observed facts. We as human beings do this all the time.\n\nHow would the American economy have evolved post 1860 if the Civil War never happened?\nWhat if a school did a new math curriculum? Would math scores improve?\nHow would gun homicide statistics look, 6 months from now, if a state didn’t pass gun control policies?\nHow would a grocery store’s in store sales have evolved if it didn’t implement all self checkout scanners?\nDid Nebraska’s repeal of the tampon tax affect tampon use? How would tampon sales have evolved if Nebraska didn’t get rid of the tax?\nHow would New Orleans’ outward migration have looked if Hurricane Katrina didn’t happen?\n\nA counterfactual, at its heart, is the way a metric, outcome, or construct would have looked in a world where what did happen (some treatment, policy, or intervention), did not happen. However, we get but one copy of reality. We can’t literally look at the United States where the Civil War happened (the reality we have) and one where it didn’t happen (not that we’d really want to, by the way). We can’t have a school with one grade level has two math curriculums (the current one and new one) at once, and even if we could, how could we know the new curriculum is the driver of grades instead of something else? We can’t see the same city that has banned guns and not done so, or a state that both taxes and doesn’t tax tampons. Thus, counterfactuals are things we can estimate, guess about, and speculate on, but never see in real life. Before we get into how we’d estimate counterfactuals statistically, though, let’s use a more relatable example.\nSuppose I’m going to school today. I think the way I take to school (Way A) is quicker than Way B. This gives us a set of two ways to take, \\(d \\in \\{0,1\\}\\) (read as “d in 0 1”), where \\(d=0\\) means we’ve taken Way B and \\(d=1\\) means we’ve taken Way A. The outcome of interest \\(y\\) is the commute tone associated with each way we take, Each way we take, expressed formally as \\(d \\mapsto y\\left(d\\right)\\), or our commute time being a function of the road we choose. We may represent the outcomes of each way as \\(y^A\\) and \\(y^B\\), where naturally \\(y^A\\) is how long it takes if we take my way and \\(y^B\\) is how long it takes if we go the other way. The “treatment effect” of Way A is \\(\\tau = y^A - y^B\\). Here, \\(\\tau\\) (the Greek letter “t-ow”) is the difference in minutes between the way it took me by taking my way, and the time it would’ve taken me if I’d taken Way B. In fact, I did this as I wrote this. I used Google Maps to tell me how long the drive from my apartment to Georgia Tech would be. Using the highway it takes 14 minutes. But, one of the options when I avoid highways takes 23 minutes.\n\n\n\nWay Taken\nIndicator \\(d\\)\nCommute Time\nOutcome \\(y\\)\n\n\n\n\nWay A\n\\(d=1\\)\n\\(y^A = 14\\) min\n\\(y = y^A = 14\\) min\n\n\nWay B\n\\(d=0\\)\n\\(y^B = 23\\) min\n\\(y = y^B = 23\\) min\n\n\nTreatment Effect\n\\(\\tau\\)\n\\(y^A - y^B = -9\\) min\nN/A\n\n\n\nSuppose I do indeed take Way A, as I would, and that it in fact takes 14 minutes. Does this mean the effect of Way A is -9, or, my way being quicker by 9 minutes? No, not exactly. Maybe I do take \\(y^A\\), but traffic builds and it doesn’t on the other way. Or, maybe \\(y^A\\) still would take 14 minutes, but the other way, \\(y^B\\), happens to take 20 minutes instead of 23, meaning our treatment effect is now \\(14-20=-6\\). The problem inherent here is I cannot take both ways at once. I have a choice to make, and once I choose I must commmit to it. I can either take my way or the other way, I can’t do both on the same day at the same time. Thus, because of this choice, I can only guess as to what \\(y^B\\)’s travel time actually would have been for me on that day. Only one outcome exists in reality. Mathematically, we may represent this as \\(y=dy^{A}+\\left(1-d \\right)y^B\\). If we take Way A, we get \\(y=y^A \\times 1 +\\left(1-1\\right)y^B\\), or just \\(y^A\\) since anything multiplied by 0 is just 0 and \\(y^A \\times 1\\) is just \\(y^A\\). If we take Way B, we get \\(y=y^A \\times 0 +\\left(1-0\\right)y^B\\), or just \\(y^B\\) because now \\(y^A \\times 0=0\\) and \\(\\left(1-0\\right)y^B\\) is just \\(1 \\times y^B\\). This means that the counterfactual is inherently unobservable. Short of time machines where we can peer into alternate universes, the counterfactual is something we have to estimate."
  },
  {
    "objectID": "treatmenteffects.html#randomized-controlled-trials",
    "href": "treatmenteffects.html#randomized-controlled-trials",
    "title": "8  Causal Inference",
    "section": "8.2 Randomized Controlled Trials",
    "text": "8.2 Randomized Controlled Trials\nEstablishing causality and generating counterfactuals are all about comparisons. Typically, we compare a group of one or more units that did an intervention or policy to units that did not do the same policy. We use regression as a vehicle to facilitate this comparison. Before we do this for a real policy example though, let’s think about how this is done in a (close to) ideal setting.\nIn medicine, we must test drugs in order to see if they work before we allow them to be used on humans in a broader sense. We use randomized controlled trials to try to establish the efficacy of drugs. A randomzied controlled trial is a form of study design where we, as the reasearchers, assign a treatment at random to a certain number of people or units or entities. Those who get the treatment we call the treatment group, those who do not get it are called the control group (or, sometimes we call the untreated group the donor pool). When we say “random assignment”, we mean that we assign the treatment such that each person has an equal probability of getting the treatment. There are many such way to do this in reality, but at its heart we essentially use a computer to flip a coin across \\(N\\) individuals/units to determine if it gets treatment. If treatment assignment is truly random, this now means that any other covariates that may influence the outcome do not predict treatment status or outcome information.\nSay we wish to study the impact of a vaccine on recovery time. We cannot just give the vaccine to some people and not others in a non-random way because maybe other variables are influencing recovery rates. Perhaps those who took the vaccine are younger on average than those who didn’t. Or, maybe they had bettter baseline health characteristics. This means, on average, those who took the vaccine would recover from COVID (say) quicker than the control group, not completely because of the vaccine but because they were already healthier or younger on average compared to the control group. We’d say something is wrong if they didn’t recover quicker. Alternatively, maybe there are just unobserved factors we can’t see which explain why the treatment group did better, to a degree.\nWhen the coin flip decides who gets the vaccine, then in a large enough representative sample, our treatment and control groups are balanced across all confounders, on average. We say “balanced” because when all study participants of all ages, races, and so on are equally likely to be given the vaccine or not, the average difference in recovery time can be better attributed to the vaccine instead of other factors such as age. Thus, it is very important to ensure that our control group is balanced across all relevant areas which may affect the outcome. If our treatment and control groups are balanced, we may compute the average treatment effect of the treatment as \\(\\text{ATE}=\\frac{1}{N}\\sum_{i=1}^{N}y_i^1-y_i^0\\). This is just the average of the raw differences in the outcomes of the treatment group and control group, where \\(y_i^1\\) is the observed outcomes for all of our treated units (recovery time, in this case) and \\(y_i^0\\) represents the average recovery time for all those in the control group.\nTo illustrate the idea of balance in a public policy setting, I generate synthetic data on 100 individuals who, at their job, enter some program which may increase income. Individuals are aged from 18 to 50. However, age may correlate with income. Older people tend to have more work/professional experience than younger people, on average. So, simply comparing the outcomes of adults versus the outcomes of a younger group may be ill-advised, as maybe those who make more money and participated in the program would have made more money anyways, without the program, due to different baseline levels of experience due to age. To test this, I iteratively assign some probability of treatment to all of them from \\(0.05 \\leq p \\leq 0.5\\) in increments of 0.01. We can see, from the GIF below, that when the probability of being treated is \\(0.5\\), the differences across both age and pre-existing incomes vanishes. The control group is on average a year older than the treatment group, and the income difference between them vanishes to an absolute difference of 54 dollars, where the treatment group makes more than the people who didn’t do the jobs program. So now that we’ve randomized the treatment, we can have people take the program and see how it affects their incomes."
  },
  {
    "objectID": "treatmenteffects.html#problems-with-randomization",
    "href": "treatmenteffects.html#problems-with-randomization",
    "title": "8  Causal Inference",
    "section": "8.3 Problems With Randomization",
    "text": "8.3 Problems With Randomization\nThe central issue with randomization is that there are some interventions (in fact, most of them) that researchers simply cannot randomize. After all, many treatments of interest have explicit assignment mechanisims (i.e., this neighborhood has high crime rates therefore we elect to send more police as a response to crime). Even if the rationale for doing the treatment is not given, sometimes our available set of control units may differ in important ways from the unit that’s treated.\nIn February of 2023, Turkey had an earthquake. Suppose we’re interested in the effect of this earthquake on the local economic outcomes for the affected cities or the entire country. Well, researchers cannot randomize earthquakes to strike certain cities versus others, and even if we could this would be morally unacceptable. So assuming we were comparing cities in Turkey that were affected to those that weren’t, the affected areas may differ in their baseline characteristics from unaffected areas. For example, maybe poorer areas were more vulnerable than richer ones. For a cross country comparison, maybe bulding codes would explain the differences in the effect of the earthquake which, in turn, affect the economic implications for Turkey versus another unexposed nation.\nAnother example is cannabis legalization. We cannot flip coins to have some states legalize cannabis and others not. Canabis’ legality is decided by the preferences of the legislature. Thus, we run into the problem of selection bias (as in, maybe some states are more likely to legealize cannabis than others). We also run into counfounding biases. If we wish to see how legal cannabis affected alcohol sales for Oregon, then we need to consider what other factors may affect alcohol consumption aside from the policy of interest. That is, Oregon may differ from other states (say, Alabama or Mississippi) on key characteristics that makes the causal comparison unreasonable. Maybe the price of alcohol between Oregon and a set of others states was not similar enough. Maybe Oregon simply had different economic conditions that made alcohol consumption more or less likely. Perhaps cultural factors would lead to higher level of alcohol consumption anyways, absent cannabis legalization. The fact that we cannot randomize means that researchers cannot make plausible the unconfoundedness assumption (or, lack of omitted variable bias) which underlies OLS regression models."
  },
  {
    "objectID": "treatmenteffects.html#difference-in-differences",
    "href": "treatmenteffects.html#difference-in-differences",
    "title": "8  Causal Inference",
    "section": "8.4 Difference-in-Differences",
    "text": "8.4 Difference-in-Differences\nEven though we cannot randomize all treatments/policies, does this mean that we cannot do policy analysis at all? No. Modern econometrics has developed a slew of methods for doing policy analysis when the intervention of interest simply cannot be subject to a controlled expriment. I now introduce the difference-in-differences method (DD), using Proposition 99 as an example case. DD is a method used for panel data. Up until this point, we have presumed that we are working with only one collection of units at one time point. This is called a cross-sectional dataset, where we have \\(N>1\\) units and \\(T=1\\) time periods.\n\n\n\nCountry\nYear\nUnits\nValue\n\n\n\n\nMexico\n2000\n1\n50\n\n\nGuatemala\n2000\n1\n45\n\n\n\nThe opposite of this is called a time-series dataset, where we have \\(N=1\\) units and \\(T>1\\) time periods.\n\n\n\nCountry\nYear\nUnits\nValue\n\n\n\n\nMexico\n2000\n1\n50\n\n\nMexico\n2001\n1\n55\n\n\n\nA panel dataset is where \\(N>1\\) and \\(T>1\\).\n\n\n\nCountry\nYear\nUnits\nValue\n\n\n\n\nMexico\n2000\n1\n50\n\n\nMexico\n2001\n1\n55\n\n\nGuatemala\n2000\n1\n45\n\n\nGuatemala\n2001\n1\n50\n\n\n\n\n8.4.1 DD Setup\nTo implement DD, we need a few key ingredients: First, we need a treatment of interest with a clear before and after point. In our case, Proposition 99 was passed in 1988, and enacted in 1989. So, we have a clearly defined treatment point. We also need at least one unit that experienced the treatment, and at least one that doesn’t experience the treatment. In our case, we have \\(N=39\\) units. Here, each unit is indexed to the letter \\(i\\). For our purposes, \\(i=1\\) is California, and the other \\(i=\\{2 \\ldots 39 \\}\\) units are the control states. and \\(T=31\\) time periods, where, respectively, \\(T_0=19\\) refelcts the number of preintervention periods from 1970 to 1988 and \\(T_1=12\\) reflects the number of post-intervention periods. In panel data, each of the units corresponds to only one time period. Each of the 39 units, in this case, has 31 rows.\n\n\n8.4.2 Introducing Parallel Trends\nUnlike in a setting where we have a randomized trial, our control group will not be balanced on covariates/outcomes with the treatment group in the pretreatment period. That is, if we take the average of our treatment and control group outcomes in the pre policy period, the numbers will likely not have the same or very close values as they did in the GIF above when I demonstrated randomization. But that’s okay: what if we don’t need the means to be balanced exactly? What if we just need for the trends of the groups to be similar to one another?\nDD asks us, as analysts, to accept a singular condition as plausible: namely, that the intercept adjusted average of our controls is a good enough proxy for how the treated unit’s outcomes would look absent treatment. This is called the “parallel trends assumption”. It means that if the intervention never happened, the average trend of our control group would move in the same way as the average of the treated unit. The parallel trends assumption is inherently untestable, since we only actually observe \\(y=dy^{1}+\\left(1-d \\right)y^0\\). However, the key we shall focus on is the parallel-ness in the pre-intervention period, since this is the only time period we observe all of our units without treatment.\n\n\n\n\n\n\nNote\n\n\n\nI will use “parallel trends”, “parallel pre-trends”, and other words to that effect interchangeably.\n\n\n\n\n8.4.3 Quality of Parallel Trends\nFor parallel trends to hold, the control group must be as similar as possible to the treatment group before the treatment was done.\n\n\nWhy?\n\n\nThe quality of our controls is what we use to build our counterfactual. I wrote above that causal inference is predicated on comparisons. For example, let’s say Honolulu implements an anti-crime policy. Can we use New Orleans or St. Louis as comparison cities? Likely not. The latter two are high crime areas, being regularly distinguished in national crime statistics for having high murder rates and other violent crimes. They are heavily urbanized places, with vastly different cultural makeups, climates, and settings. Therefore, we wouldn’t expect for these two cities to be sufficiently comparable enough to Honolulu to warrant a good causal comparison.\n\n\nA good first way to assess the quality of parallel pre-intervention trends is to simply plot the average control outcomes and the average treatment outcomes and look at how similar their trends are in the pre-intervention period.\n\n\n\n\n\n\n\n\n\nHere, we plot the cigarette consumption of California versus the average of control states. We can see in 1970, California is fairly similar to the control average. In fact, the average of controls and California both grow in their consumption rates up until 1976. However as the years progress, the average trend of controls grows at a much faster rate than California’s. California’s smoking trends grew relatively little and begin to precipitously fall in and after 1976, whereas the control group’s trend has much higher rates of smoking. We can also investigate the quality of parallel pre-trends empirically without needing to use graphical methods, as we will show below.\n\n\n8.4.4 Estimating DD\nAs one might expect, the main workhorse of DD is simply OLS regression, the topic of our previous chapter. I will give Stata and R code below so we can streamline DD estimation, but I think it helps a lot to understand what’s happening from the perspective of regression at first.\nTo estimate basic DD models, we seek the line that minimizes the prediction error between our independent variables and our outcome. However in this case, our outcome and predictors are special. Our dependent variable in this regression model, \\(y_{1t}\\), is the pre-intervention outcomes of our treated unit, California. Our independent variable is the year-wise pre-intervention average of the 38 control states, \\(\\bar{y}_{\\text{co},t}\\). To show you that we’re not speaking about abstract concepts, below I show the dataset we’d use to do DD with. We index the year to the outcomes of California and the average of its controls.\n\n\n\nDD Dataset\n\n\nYear\nCalifornia\nControl Group Mean\n\n\n\n\n1970\n123\n120.08\n\n\n1971\n121\n123.86\n\n\n1972\n123.5\n129.18\n\n\n1973\n124.4\n131.54\n\n\n1974\n126.7\n134.67\n\n\n1975\n127.1\n136.93\n\n\n1976\n128\n141.26\n\n\n1977\n126.4\n141.09\n\n\n1978\n126.1\n140.47\n\n\n1979\n121.9\n138.09\n\n\n1980\n120.2\n138.09\n\n\n1981\n118.6\n137.99\n\n\n1982\n115.4\n136.29\n\n\n1983\n110.8\n131.25\n\n\n1984\n104.8\n124.9\n\n\n1985\n102.8\n123.12\n\n\n1986\n99.7\n120.59\n\n\n1987\n97.5\n117.59\n\n\n1988\n90.1\n113.82\n\n\n\n\n\nWe are, in effect, exploiting the correlations between the average of controls and the treated unit to produce a counterfactual. After all, we would imagine that if the average of controls is similar in trend to that of the treated unit, then this average is likely similar to the treated until on unobserved factors as well. So, if the treated unit and control group both have very similar trends in the pre-intervention period, all we now need us a time-trend to account for the time specific differences, captured by our intercept. The simple DD model looks like\n\\[\ny_{1t} = \\beta_{1}\\bar{y}_{\\text{co},t} + \\beta_0 \\quad \\text{s.t. } \\beta_1 = 1.\n\\]\nAs promised, our dependent variable is the outcomes for unit \\(i=1\\). Our independent variables are the average of our control unit outcomes, and an intercept (which we’re estimating). Notice here how we force the coefficient for the average of controls to be 1.\n\n\nWhy?\n\n\nDD presumes that a pure average of our controls is a good enough proxy for our counterfactual. In other words, in the DD world, all of our control units are treated as equally valid for predicting the counterfactual. So, to have the time-intercept reflect the pre-intervention difference between the pure arithmetic average of controls versus a weighted average of controls, \\(\\beta_{1}\\) must be equal to 1. If we multiplied it by some other value, we’d be using a weighted average, not an arithmetic average.\n\n\nAfter we estimate this model (again just for the preintervention period), we predict the remaining values for the post-intervention period. How do we do the prediction, by the way? Well, it’s easy! We already know what \\(\\beta_{1}\\) is, that’s just the mean of our control group. So now as per the model above, we only need to estimate \\(\\beta_{0}\\). Once we have \\(\\beta_{0}\\), we add or subtract whatever that value is to each value of the mean of controls! The constant here for the intercept term is roughly \\(14.359\\). I plot the results of this regression model below. I also use a table to compare the observed values of the treated unit to the values of the DD counterfactual.\n\n\n\n\n\n\n\n\n\n\nDD Predictions\n\n\nYear\nCalifornia\nDD California\n\n\n\n\n1989\n82.4\n95.304\n\n\n1990\n77.8\n91.307\n\n\n1991\n68.7\n89.983\n\n\n1992\n67.5\n89.036\n\n\n1993\n63.4\n88.336\n\n\n1994\n58.6\n87.759\n\n\n1995\n56.4\n88.799\n\n\n1996\n54.5\n86.825\n\n\n1997\n53.8\n87.43\n\n\n1998\n52.3\n86.599\n\n\n1999\n47.2\n83.236\n\n\n2000\n41.6\n77.775\n\n\n\n\n\nWell, what do we see here? We see the counterfactual produced by DID, using all 38 controls. As promised, the counterfactual predicted by DID is simply the original average of the control units minus 14.359! Seriously, that’s all it is.\nNow that we’ve predicted our counterfactial, we now calculate the treatment effect for California. We calculate the average treatment effect on the treated unit as\n\\[\n\\text{ATT} = \\frac{1}{T_1 - T_0} \\sum_{T_0 +1}^{T} (y_{1t} - \\hat{y}_{1t}),\n\\]\nor the average of the differences between what we in fact observed (California under treatment) and what we did not observe (California’s counterfactual smoking outcomes predicted by DD). The ATT suggests that Proposition 99 decreased tobacco consumption by about 27.349 packs per capita, with a 95 percent confidence interval of \\(\\left[-32.522,-22.177\\right]\\).\n\n\n8.4.5 A Second Pass at Parallel Trends Quality\nOkay, now we have this counterfactual. But is it a good one? How can we tell if the counterfactual is plausible here? Looking at graphics is fine, but how can we statistically evaluate if the DD model is a good one? We can begin by recalling a metric of fit, the RMSE. As before, the RMSE represents the normalized average error squared between our observed values and our predicted values.\nIn this case though, think of the RMSE as a metric of parallel-ness. The smaller the RMSE is, the better our pre-intervention predictions are. The bigger the RMSE is, the worse our pre-intervention predictions are. The reason this matters is because if we have good pre-intervention model predictions, we’re likely to have better post-intervention counterfactual predictions. After all, if our model tracks closely with the actually observed values in the pre period, then it likely a good representation of how the post-intervention counterfactual would look since our predictions come from untreated units only. If we have worse pre-intervention model predictions, the opposite is true because the model does not explain the pre-intervention period data well.\nLet’s apply these ideas to our example. The RMSE here is about 7. Strictly speaking, this number isn’t so bad, and it’s certainly an improvement over the pure average of all controls (without an intercept). But look at what this means practically and focus on the pre-intervention period. The DID counterfactual underpredicts the true values for California in the pre-intervention period from around 1970 to 1975. Beyond this, it also overestimates the observed California’s values between 1980 and 1988. This is particularly bad because if your predictions diverge significantly from the treated unit’s observed values in the years right before the intervention takes place, why would we think that the post-intervention smoking consumption predictions are valid?\nThis imbalance comes from a violation of the parallel trends assumption. Why might this be? Well, we included all 38 control units. All of our control units may not be comparable to California’s tobacco consumption trends. Take Kentucky or New Hampshire, for example, states that have very high smoking rates per capita. In the same way that outlier observations may corrupt an average, they equally may corrupt regression predictions for causal analysis.\n\n\n8.4.6 Alternate Control Group\nWhat if we used a different control group though? Suppose we altered the control group to be a smaller subset of controls. Say I use Montana, Colorado, Nevada, Connecticut. After all, why not? Montana, Colorado, and Nevada are all geographically quite close to California, and Connecticut has a similar preintervention trend of tobacco smoking to California.\n\n\n\n\n\n\n\n\n\nWell now! This certainly looks a lot more parallel than when we used all of our control units!! When we use the limited set of controls, \\(\\tau=-13.647\\), with a confidence interval of \\(\\left[-14.549,-12.745\\right]\\). Think of how big of a reduction this is: when we used all control units (some of which are clearly different from California’s pre-1989 tobacco smoking trends), we come up with a decrease of 27 packs per capita, but when we use a more limited pool of controls, we get an ATT which implies a reduction of 13.6 packs per capita. That’s pretty much a reduction of 100% in terms of the average treatment effect! We have cut our treatment effect in half by virtue of having a better control group.\nIn addition to the pre-intervention average trend of controls “looking” more parallel, we can confirm that the regression model with the alternate control group is superior to the original model in that the new control group reduces the bias from parallel trends imbalance. The RMSE for the alternate control group shrinks by 140% compared to the original model. The fact that our RMSE shrinks by so much indicates the value of choosing the control group judisciously. By selecting a valid pool of controls, we not only decrease our model’s prediction errors, but we also gain more confidence in the true effect of our intervention, as opposed to the original estimate whose estimates were much wider.\n\n\n\n\n\n\nNote\n\n\n\nAs with OLS, rarely will you do DD in the manner I’ve described it above. However, I feel that Stata and R, through their excellent computing capabilities, can largely obscure what’s going on when we use reg or didregress. Also, we can extend DD to instances where many units are treated at different time points. However, for the introductory level, I think DD with a single treated unit is more than adequate as a starting point.\n\n\nNext, we’re going to do the exact same example in Stata, however, we’re going to take advantage of the features that Stata allows for to automate this process.\n\n\n8.4.7 Streamlining DD in Econometrics Software\nSay that we wished to estimate this in Stata. Here is what we’d do:\n\nclear *\n\nimport delim \"https://raw.githubusercontent.com/OscarEngelbrektson/SyntheticControlMethods/master/examples/datasets/smoking_data.csv\", clear\n\ng treat = cond(state==\"California\",1,0)\n\ng post = cond(year > 1988,1,0)\n\negen id = group(state)\n\nxtset id year, y\ncls\n\nreg cig i.treat##i.post, vce(cl id)\nI create a treatment variable called treat (g is short for the full command name, generate). I do this using the condititional function. You can type in h cond to see the help file, or the way to use the syntax. However for our purposes, just know that it creates a variable equal to 1 if the state variable is “California”, else 0 (because California is our treated unit). I then use the same to create a post variable, equal to 1 if the variable year is greater than 1988. I then generate a numeric variable that is a number that uniquely identifies each state in our dataset, using the egen command and its corresponding function group. I then use Stata’s xtset command to sort our data by id and year. As promised, we have 31 rows for all 39 units. I then clear the screen using cls so we can better focus on the regression estimates.\nNow, we use reg, short for regress, to estimate the treatment effect. To do this, we use something called an interaction term. This will be covered more in other methods courses, but think about it like this: we can have a unit be treated, or not. We can have the year be before the treatment, or not. But, for our purposes, we’re interested in the effect of being both treated (California) AND being in the post-intervention period (after 1988). So, the regression model takes the form of \\(y= (d_i \\times \\text{post}_t)\\).\nBelow, we will see a table. The first coefficient we will see is 1.treat -14.359. This is the exact same number as we fot estimating the baseline differences between the average of controls and the treated unit in the pre period. In other words, it is the effect of treat being 1 and post being 0. Similarly, post is the effect of 1.post being equal to 1 and treat being 0. treat#post 1 1 -27.34911 is the effect of both treat being 1 AND post being 1. It is our treatment effect coefficient. As usual, the constant _cons is the value we predict where all of our predictor variables are equal to 0. In this case, this is simply the empirical average of the control group in the pre period. We also can see the associated confidence intervals, t-statistics, and standard errors.\nYou may at this point be asking why I didn’t just show it like this in the first place: why bother with the original regression method at all, where we manually take the average of controls and and use it as a predictor in OLS? Why not just use this method and focus my discussion on that? The reason for this is because by doing it this way, Stata is obscuring important details from you."
  }
]