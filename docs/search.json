[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics for Policy Analysis",
    "section": "",
    "text": "1 Syllabus: PMAP 4041, Fall 2024\nEvery day, governments pass laws/public policy to affect some outcome of interest. Policy usually touches thousands if not millions of people. From traffic-circles to pop/sugar sweetened beverge taxes, vaccine mandates and universal pre-k programs, cannabis legalization to minimum wages, public policy impacts us all from birth to death.\nPolicy is never self justifying. It demands evaluation. If California bans tobacco smoking in public, or if New York City implements gun control, presumably we would agree these likely impact outcomes like tobacco use or homicide rates, ideally decreasing both of them.\nIf California’s anti-tobacco policy didn’t affect smoking rates at all (or worse, if more people began to smoke) or if gun control has 0 impact on homicide rates (or increased them, paradoxically), then surely these could not be justified in the very first place. Before we continue, understand fundamentally these outcomes being affected are the point. The only reason that we, as a society, do policy is precisely because we think policy affects (or should affect) people somehow. If political science studies “who gets what where”, one summation of policy studies might be “what works?” But what policies should we care about? How can we know if they work? This is the starting point for empirical policy analysis. This class discusses the theory and process for how statistical analysis of data may be used to answer policy questions."
  },
  {
    "objectID": "index.html#course-philosophy-and-structure",
    "href": "index.html#course-philosophy-and-structure",
    "title": "Econometrics for Policy Analysis",
    "section": "1.1 Course Philosophy and Structure",
    "text": "1.1 Course Philosophy and Structure\nI believe the best way to demonstrate knowledge of policy analysis is through writing. As such, there will be no quizzes or in-class exams. Why? It is unrealistic. In real life, rarely do we have an hour and 30 minutes or a ten minute quiz window on the internet to write a full summation of our ideas or think through a question. Typically, we have much more time and resources to help us. In fact, proper use of resources is what makes a good analyst: good analysts don’t need to remember everything, but they do need to be good at finding answers and using them sensibly. In this spirit, you have one assignment. Specifically, you’ll write a paper where you derive a research question you find interesting and apply the statistical concepts we cover to answer questions about a real, existing policy. Here is the breakdown of your course grade. The class is broken up into two sections: in the first section, we go over basic probability, correlation, and regression. The remainder of the class covers research for policy analysis.\n\n35% of your grade comes from the first draft of the paper, 15% question and 20% draft.\n60% for the final paper and presentation (respectively, 30 percent each), and\n5% for attendence.\n\nYou will discuss the justification for the policy (including why we should care about understanding its effects). You’ll gather real data on the policy of interest (including information on the primary variables of interest, relevant predictors/covariates), and outcomes you’ll focus on. Finally, after you’ve defined the research question as well as collected and cleaned your dataset, you’ll use the statistical tools we cover (probability theory, descriptive statistics, and regression) to discuss the effects of the policy or intervention. The paper you produce must ask a causal question where there is at least one intervention of interest.\nIn many senses, public policy is a catch all term covering various disciplines. Public health scholars may care about how banning of abortion in Texas affected fertility rates, or how COVID-19 vaccine/mask mandates affected the COVID-19 case rate per capita compared to other jurisdictions that did not enact these policies. Criminologists may care about how the building of Cop City affected how many people are shot by police, or how a state legalizing cannabis affects crime rates or the consumption of alcohol. Policy historians may care about how Pinochet’s 1973 economic policies affected the GDP of Chile or about how Britain’s National Health Service of 1948 affected infant mortality. Economists may ask how Hurricane Katrina affected the economy of New Orleans. Environmental scholars may care about how a train derailment affected housing prices. These of course are just some fields; increasingly, advanced empirical methods are used in the business sector and government. Given the array of areas and topics that policy touches, I don’t care about what policy or research question you choose to study. To quote Noam Chomsky (who was quoting another MIT professor), the important part isn’t what we cover in class; it is about what we discover. The only two stipulations I have is that your rersearch question/outcomes must be 1) quantifiable with accessible data that you can use and also must 2) be interesting to you."
  },
  {
    "objectID": "index.html#additional-details",
    "href": "index.html#additional-details",
    "title": "Econometrics for Policy Analysis",
    "section": "1.2 Additional Details",
    "text": "1.2 Additional Details\n\nIf I feel the concept is important, it’ll be in the lecture notes or we will discuss it. I will also assign external readings to be done before class.\nThere is no required textbook (aside from this one!) for this course. Various free textbooks exist such as Introductory Econometrics with R, Introductory Statistics, Intro to Modern Statistics, Regression and Other Stories, Intro to Econometrics, Intro to Political Science Research Methods, and many others. The Policy Department at Georgia State also recommends Introduction to Research Methods or Research Methods for the Social Sciences. The corresponding lecture will focus on the content that each respective chapter covers. Note that these books cover different aspects of the course in different levels of depth (Gelman’s book Regression and Other Stories is obviously mainly about regression, one of the last math topics we cover, whereas the others are more rudimentary).\nThe same is true for software– I don’t care which of these you use, but the only ones I know well are Stata, Python, and (to a lesser degree) R. For Stata users, Statalist is a great resource for Stata. R also is backed by a vast statistician community. I will sometimes include code blocks for Stata and Python in the text."
  },
  {
    "objectID": "index.html#helpful-notes-from-me",
    "href": "index.html#helpful-notes-from-me",
    "title": "Econometrics for Policy Analysis",
    "section": "1.3 Helpful Notes from Me",
    "text": "1.3 Helpful Notes from Me\n\nSun Tzu said every battle is won before it is fought. To reverse the perspective, as Ben Franklin said, if you fail to prepare, prepare to fail. The fact that the paper is the only assignment you have, in effect, means that I expect quality questions, idea, and analyses written in a professional manner. I do not expect perfection, or material at a level beyond the main content, but preparation is your best friend in this course.\nAs corollary to the preceding points, please do contact me if you have questions. Policy data analysis is what I do in my research every day. I love what I do, and I love discussing this topic with others. If you have any questions about the ideas we cover in class or have any difficulties, you may always meet with me or contact me otherwise. Thinking of your research question early, asking me for feedback, and so on helps more the earlier you talk to me.\nDo not simply communicate with me. In addition, feel free to communicate with your classmates. This is something I only really learned the value of as a PHD student, so I figured I would advise the same to you. As an extension of this, I will consider allowing for collaboration on the final paper in groups of two, with my permission. For such papers to be considered, I must hear the research question well in advance, as well as the exact ideas on the data, analysis, and relevance of the question overall.\nAs you’ll see by skimming the sections of EPA, I frequently use graphics that I construct from real datasets which I link to. On my GitHub page, you’ll find these datsets, and more, linked to their descriptions. In lieu of finding your own dataset, you may use any of these for your class paper, should you wish."
  },
  {
    "objectID": "index.html#class-schedule",
    "href": "index.html#class-schedule",
    "title": "Econometrics for Policy Analysis",
    "section": "1.4 Class Schedule",
    "text": "1.4 Class Schedule\nBelow is the schedule. All readings for Econometrics for Policy Analysis (EPA) should be done before class. The other book chapters (unless I write otherwise) are optional.\n\n1.4.1 Week 1\n\n08-26-2024 (Monday)\n\nIntroductions and EPA, C2\n\n08-28-2024 (Wednesday)\n\nRequired: EPA C3.\nOptional: IS C2, IS C3 (skim), IDS C2, IDS C3, especially “Discrete Probability” and “Random Variables”.\nA refresher on averages. Also covers t-tests, standard errors, and confidence intervals\n\n\n1.4.2 Week 2\n\n09-02-2024 (Monday)\n\nUniversity holiday. No clase.\n\n09-04-2024 (Wednesday)\n\nBasic Asymptotic Theory (the Law of Large Numbers, Law of Iterated Expectations, and the Central Limit Theorem)\n\n\n1.4.3 Week 3\n\n09-09-2024 (Monday)\n09-11-2024 (Wednesday)\n\nCorrelation, Coeffcients, and Association (EPA, C3)\nHere we cover basic correlation in 2 Dimensions, mainly using scatterplots and contingency tables.\n\n\n1.4.4 Week 4\n\n09-16-2024 (Monday)\n\nRequired: Watch this: Partial Derivatives OLS Explained\nOptional: (ROS, C7), IS, C10. Also, Inference for OLS (Gauss-Markov Assumptions). Today, the research question is due.\n\n09-18-2024 (Wednesday)\n\nGauss-Markov Assumptions (from the previous chapter)\n\n\n1.4.5 Week 5\n\n09-23-2024 (Monday)\n\nPanel Data\n\n09-25-2024 (Wednesday)\n\nIntro to Treatment Effects\n\n\n1.4.6 Week 6\n\n09-30-2024 (Monday) Required: Data Types and Measurement (EPA, C5) Optional: RMSS, C6\n\nData Gathering/Cleaning (Sampling, Measurement)\n\n10-02-2024 (Wednesday)\n\nData Visualization\n\n\n1.4.7 Week 7\n\n10-07-2024 (Monday)\n\nWriting for Policy Analysis: The Introduction and Literature Review\n\n10-09-2024 (Wednesday)\n\nWriting for Policy Analysis: The Background\n\n\n1.4.8 Week 8\n\n10-14-2024 (Monday)\n\nWriting for Policy Analysis: Data\n\n10-16-2024 (Wednesday)\n\nWriting for Policy Analysis: Methods\n\n\n1.4.9 Week 9\n\n10-21-2024 (Monday)\n\nWriting for Policy Analysis: Results and Conclusions\n\n10-23-2024 (Wednesday)\n\nFirst Draft Due, Presentations begin.\n\n\n1.4.10 Week 10\n\n10-28-2024 (Monday)\n10-30-2024 (Wednesday)\n\n\n\n1.4.11 Week 11\n\n11-04-2024 (Monday)\n11-06-2024 (Wednesday)\n\n\n\n1.4.12 Week 12\n\n11-11-2024 (Monday)\n11-13-2024 (Wednesday)\n\n\n\n1.4.13 Week 13\n\n11-18-2024 (Monday)\n11-20-2024 (Wednesday)\n\n\n\n1.4.14 Week 14\n\n12-02-2024 (Monday)\n12-04-2024 (Wednesday)\n\n\n\n1.4.15 Week 15\n\n12-09-2024 (Monday)\n12-11-2024 (Wednesday)\n\n\n\n1.4.16 Week 16\n\n12-16-2024 (Monday)"
  },
  {
    "objectID": "module1.html",
    "href": "module1.html",
    "title": "2  Data and Policy Studies",
    "section": "",
    "text": "3 Summary\nAt this point, it’s clear that data and data analysis are critical to public policy. It allows us to visualize trends, identify the effects of interventions, and reach conclusions on the basis of this evidence. However, the “how” we reach conclusions part matters, since the methodology we use to reach conclusions fundamentally affects what we can conclude in the very first place. The next two lectures cover probability and asymptotic theory; these form the foundations of quantitative public policy analysis."
  },
  {
    "objectID": "module1.html#what-is-this-thing-called-science",
    "href": "module1.html#what-is-this-thing-called-science",
    "title": "2  Data and Policy Studies",
    "section": "2.1 What is This Thing Called Science?",
    "text": "2.1 What is This Thing Called Science?\nScience at its core is a process we use to understand observable phenonmena. It is based on using logic and observations of the senses to form coherent and simple understandings about the world. Data, or a collection of observations, is fundamental to being able to conduct scientific research. We use data in our daily lives to make conclusions; we don’t call it as such, but we do. Note here that data is not a living, breating concept: it requires interpretation by us. We use principles of science to interpret data and the analyses we conduct upon data. As we learn in middle and high school, science typically begins with asking questions or defining a problem.\nSuppose our current problem involves commute time to school or work, and we don’t wish to walk. In this case, that’s our question: “What’s the ideal way to get to school/work?” We then gather information. Chances are we may use Google Maps or Waze to guide us. In this context, these tools provide us with the information we need, namely, estimates of how long our commute will be. And, assuming we wish to get to our destination as fast as possible, we make inferences or conclusions about the ideal way to take based on the GPS’ options. If GPS says the highway takes 15 minutes but the backstreets which avoid highways take 35 minutes, we will typically elect to use the highway since that takes us to our destination the quickest.\nThere’s still two more steps to do, though: test our hypothesis and draw conclusions about the actual observed facts. This means that we must, in real life, leave home and take the way we decide to take. When we get to our destination, we form conclusions about how actually taking the highway went. Of course, we repeat this idea multiple times; eventually, we “typically” take a certain direction to work or school precisely because we have the expectation the highway way will, on average, be preferable to alternative ways. This is a simple example, yet it illustrates the central point: in scientific inquiry, we ask questions, draw on available information, form ideas, take actions based on that information, and draw conclusions or plan accordingly based on testing the validity of that observed information. We don’t call this science in daily life, but that’s exactly what it is. The steps I’ve outlined so far are present in every field from public policy to physics, albeit with a little more sophisitcation.\nAs I’ve mentioned above, a collection of observations about a set of phenomena is what we call data. Thus, in public policy analysis, data is central to all that we do. One may ask why using data matters at all; the simple reason is that it allows us to resolve disagreements. While people may conduct different data analyses and obtain different results and even reach different conclusions, the main idea is that we can look into the real world and obtain concepts that map on to metrics that we think are important and test them against our expectations. After all, everyone can have opinions or views on things, but the useful part is testing out our expectations against reality. That way, we can have a better sense of what’s more likely to be true if a certain policy happens/is passed.\nTraditionally, data analysis in the policy space has three goals in mind. The first is descriptive analysis of a phenomenon or topic. In this setting, we simply use raw or lightly transformed data to visualize understanding or relationships between variables (braodly, this is called analysis of variance). For example, we may ask (the classic political science question of) why some countries are wealthy or more developed and others poor/underdeveloped. We could classify units of analysis (schools, cities, states, or any other entity) by some criteria (Global South, Southern United States, New England, Metro Atlanta) and compare different metrics of income between them. We may take the average income of each unit and make graphics which show disparities between them. At a deeper level, we may wish to explain the factors which lead to these disparities. So say for cities, we may wish to understand how urbanicity, distance to the capital of the state, age composition, racial composition, and political status of the mayor explains variation in income levels for that city or a set of cities. These sorts of studies can help us point out disparities (for example, maybe cities of the United States that are mostly black or Native American in racial composition have 10,000 less dollars compared to mostly white areas) or identify broader trends. A second goal of policy analysis is prediction. A common problem in macroeconomics is the forecasting of GDP trends. Of course, the only way we may do this is by collecting data on GDP or some other measure we can observe across time and applying statistical techniques to try and predict how GDP/unemployment trends would look under a certain set of assumptions.\nA third need for data in policy analysis is for the purposes of estimating the impact of some policy or intervention on some outcomes. Recall the example from the syllabus of Proposition 99, where California wished to reduce tobacco smoking. This intervention raises an immediate question for policy analysis: namely, “what was the effect of this intervention on the actual smoking rates we see?” This is a question we may collect tobacco sales data on, for at least California. After data collection (or even prior, in this case), we can form hypotheses. A hyopothesis is a declarative/interrogative, testable statement about the world. It is like a hypothetical in the sense that we try to imagine the effect of a policy on an outcome so that we can answer questions about it. Here, we can hypothesize that Proposition 99 has a negative impact on tobacco smoking. Negative here is not intended in the normative sense; presumably most people reading this do not smoke (tobacco, anyways) or think that smoking is wonrg or immoral. Instead, here “negative” means that the policy might decrease the tobacco sales per capita compared to what they would have been otherwise. To test this, we can use statistical analysis to compare California to other states that didn’t do the policy.\n\n\n\n\n\n\n\n\n\nThe plot shows the cigarette pack sales per 100,000 for California from the years 1970 to 2000 (our dependent variable). The thick black line denotes the observed values for California, and the vertical black reference line shows the year that Proposition 99 (the independent variable/treatment) was passed. As I mentioned above, we typically wish to produce an estimate of California’s cigarette consumption in the years following 1989, had Proposition 99 never been passed. This line is denoted by the red dashed line. After we do our analyses/estimations, we can discuss what the implications are. In other words, was the policy effective by some appreciable margin? Are there other outcomes concerns to consider?"
  },
  {
    "objectID": "module1.html#steps-of-data-analysis",
    "href": "module1.html#steps-of-data-analysis",
    "title": "2  Data and Policy Studies",
    "section": "2.2 4 Steps of Data Analysis",
    "text": "2.2 4 Steps of Data Analysis\nBroadly speaking, we can think of data analysis being broken into 5 distinct concepts. I summarize them below.\n\n2.2.1 Identifying Policy Problems\nAs we’ve discussed above, the first step in this process is simply asking questions. What kind of questions? Policy questions. Knowing what specific questions to ask though can be tricky. Policy is a giant field. Of the thousands of questions we could ask, how do we know which ones will be the most pressing or timely? In other words, how do we know that this is a problem that policy needs to be enacted for? How can we identify programs whose analysis benefits the citizenry or other interested parties? Put simpler, who cares? Why do we want to do this study or answer this question? Who stands to benefit?\n\n\n2.2.2 Gathering Data\nEven once we’ve identified the problem, how do we go about gathering real data to answer questions? If we can’t get data that speaks to the issues that we’re concerned about, we can’t obtain answers that are useful.\n\n\n2.2.3 Cleansing Data\nIn real life, datasets do not come to us wrapped in a pretty bow ready for use. Cleaning data (or organizing it) can be a very messy affair in the best of times. In order for us to answer our questions, the data we obtain must be organized in a coherent way such that we can answer questions at all. If you wish to plot the trend lines of maternal mortality in Romania compaed to 15 other nations and your data are not sorted by nation and time, trust me, the plot you’ll get will not just look terrible, but you can’t glean any trends or patterns from it. What’s worse, you may not even know improper sorting is the casue of the problem until you bother to look at your dataset again. So, it is best to have good habits developed early.\n\n\n2.2.4 Analyzing Data\nFor analysis, we apply statistical analysis in order to answer the questions we’re asking, using the dataset we’ve now cleaned. Such techniques can range from simply descriptive statistical analysis to complex regression models. From such models, we sometimes wish to make inferences to a bigger population, but sometimes more specific statistics (e.g., the average treatment effect on the treated units) are of interest.\n\n\n2.2.5 Presenting the Results\nNow that we’ve done analysis, we can finally interpret what the findings mean. We attempt to draw conclusions based on our results and come up with avenues for future research or other relevant aspects of interest. In this section, we typically try and say why our findings are relevant."
  },
  {
    "objectID": "module1.html#identifying-policy-problems-1",
    "href": "module1.html#identifying-policy-problems-1",
    "title": "2  Data and Policy Studies",
    "section": "2.3 Identifying Policy Problems",
    "text": "2.3 Identifying Policy Problems\n\n2.3.1 Justifications For Policy\nBefore we can do any analysis though, we have to take a step back. We have to ask ourselves how we know a problem exists in the first place. There are two broad justifications that policy is based on: negative externalities and social good, but the main point of both justifucations is “harm”.\n\n\n2.3.2 Externalities\nThe idea of externalities comes from microeconomic theory, which says that efficient markets will affect only those parties who willingly participate in transactions. Particularly in the case of negative extrnalities, or externalities which harm others, we could use public policy to rectify this.\nConsider a very simple example: seatbelts. In physics, any force that is not stopped by an equal, oppostite force will keep going. So, if you’re in a car crash while driving at 60 miles per hour while unbuckled, the car stops. You, however, don’t stop: you keep going, 60 miles per hour through the windshield. No public policy is needed just yet. So far, any cost that comes from a transaction has been borne by you, the driver. By the way, I’m not kidding: one of the arguments against seatbelts was literally that using seatbelts should be a personal decision if it does not put others at risk. Additionally, industry also argued against mandatory seatbelt laws on the grounds that it was the government interfering between the transactions of a consumer and the seller.\nHowever, there are a few issues with the externality argument. Firstly, being unbuckled turns you into a human projectile. You can hit your passengers or even others outside your vehicle if you’re unbuckled. Your market exchange (you buying the car and driving it) is now potentially having second-order effects on others by you not using a seatbelt. So, the government may wish to mandate seatbelts while driving in order to prevent these negative externalities which come in the form of medical bills or death. To address the argument of indsutry above, that seatbelt laws would raise costs of production, this raises an important moral dilemma: does the harm caused to the business of having to install seatbelts matter more than the human harm caused by a society where seatbelts are optional? Also, we are human beings. We have imperfect knowledge. We know for fact that we don’t have all the answers, to paraphrase Socrates. We also don’t know if the actions we do will ultimately hurt someone else. We live in a probabalistic world (which we will return to later). Indeed, we could argue against laws banning DUI in precisely this manner, saying that we don’t know if the intoxicated driver will harm someone until they do. But, as with seatbelts, we never know if there will be another passenger on the road or a child playing in the street. So, we rarely know if we’re actually putting peoples’ lives in danger by driving drunk or unbuckled. We can’t know if an externality will occur until it does, usually. Thus, the next view (social good) adopts a different form of reasoning.\n\n\n2.3.3 Social Good\nMoreover, the externality justification isn’t typically the way we think about things from a public policy perspective. Usually, we have social welfare goals in mind. This can come in the form of harm reduction or prevention measures. When we argue for public education, for example, we typically don’t do so because we think that the private schools won’t educate citizens enough (even though they won’t), and that public school will be to decrease inefficient education markets. In fact, we typically don’t think of education (in our formative years anyways) as a market at all. We usually argue for public education because we think that education has inherent benefits, and that being denied a certain level of education necessitates an inherent harm. Imagine for a moment how the literacy rate of the United States would look if school was completely optional. We likely would not complain about GDP loss, we’d likely complain about a society where lots of people can’t read the cereal box or function within society in a decent manner. In other words, society has a vested interest in keeping people safe, educated, and healthy to some degree. So we mandate seatbelt laws, basic schooling, and other laws/regulations in service of these ends. Importantly, “these ends” does not have a right or wrong answer. The goals of policy are ultimately decided by people within the society. However, knowing the goals of a policy and reasons for its existence helps us ask meaningful questions about it. Following the above discussion, a natural research question that follows is “How did seatbelt laws affect the rate of car accident injuries and deaths?”\n\n\n2.3.4 Why Is Tobacco a Problem?\nAs we’ve discussed above, harm or necessity is typically a standard we look to in order to determine if policy is needed. As I’ve mentioned, California passed Proposition 99 in 1989 to reduce smoking rates. But, how did we know there was a problem to begin with? To do this, we can grab data on lung cancer morality rates from 1950 until today. Presumably, of course, we view lung cancer as harmful and somehthing we wish to prevent.\n\n\n\n\n\n\n\n\n\nThe shaded area represents the period before any state-wide anti-tobacco legislation was passed in the United States. We can see quite clearly the age-standarized lung cancer mortality rates rose in a fairly linear manner in the United States. However, the curve is parabolic; mortality rates were rising every single year until the zenith in 1990. Mortality began to fall when the first large scale anti-tobacco laws were passed. Of course, the degree to which these laws were the cause of this decrease is an empirical question (especially since lung cancer develops over time, the decrease after 1990 suggests other thing may have also contributed to the decline in behaviors that led to the decreease in mortality). However, given the clear increase in lung cancer rates and other obvious harms of tobacco smoking in the preceding decades, policymakers in California and the voters, in fact, became increasingly hostile to tobacco smoking in public and in other crowded areas. So, California passed legislation in 1988 (as did at least a dozen other states from 1988 to 2000) to decrease smoking rates.\nHad I not plotted this trend line, people (from the tobacco industry in 1970, for example) could simply say “Well, nobody knows if lung cancer mortality is a problem. How do we know if there’s a problem here? I don’t think one exists.” This plot makes a powerful case that lung cancer is indeed a problem which must be addressed due to the persistent rise in mortality. Data in other words provides intellectual self-defense; if you posit that a problem exists, then this should be demonstrable using datasets that speak to the issue at hand. As a consequence of this, if a problem does exist (be it tobacco smoking or the impact of racial incarceration/arrest disparities), we can then look for policies that attempt to mitigate or solve the problem. That way, we can go about doing analysis to see which policies are the most effective."
  },
  {
    "objectID": "basicprob.html",
    "href": "basicprob.html",
    "title": "3  Basic Probability Theory",
    "section": "",
    "text": "4 Summary\nProbability is the stepping stone into using statistical methods. It is the foundation of decisionmaking in business, economics, and policy analysis. For many, the concepts covered here will be new material– indeed, the term “statistics” or “data analysis” can be intimidating to people at first glance.\nI believe the best way to introduce these topics is to keep a balanced perspective between mathematics and application. However, this course only scratches the very surface; the world of quantitative methods in policy analysis is a big one. For those of you interested in graduate school or who wish to use statistics for your future job, your mastery of this essential material will not be in vain."
  },
  {
    "objectID": "basicprob.html#descriptive-statistics",
    "href": "basicprob.html#descriptive-statistics",
    "title": "3  Basic Probability Theory",
    "section": "3.1 Descriptive Statistics",
    "text": "3.1 Descriptive Statistics\nProbability is rarely used in a vacuum, though. We typically, in the policy sciences, wish to take a given outcome from a set of outcomes and draw conclusions from it. To do this, we use descriptive statistics (also called moments).\n\n3.1.1 Means: Arithmetic and Median\nThe first moment is called the average/arithmetic mean. The formula for the mean, also called the expected value (denoted by \\(\\mathbb E\\)) is \\[\n\\bar{x} = \\mathbb{E}[X]=\\frac{1}{N}\\sum_{i=1}^{N}x_i\n\\] where \\(\\bar{x}\\) is the mean, \\(N\\) is the number of values, and \\(x_i\\) represents the \\(i\\text{-th}\\) value in the sequence. The uppercase Greek letter “sigma” means summation, or \\(\\sum_{i=1}^{N} x_i\\). It adds the values from \\(i = 1\\) to \\(N.\\) For a discrete random variable, the expected value is\n\\[\n\\mathbb{E}[X] = \\sum_{i=1}^{N} x_i \\cdot P(x_i).\n\\] So for our die, the expected value is \\[\n\\mathbb{E}[X] = 1 \\cdot \\frac{1}{3} + 2 \\cdot \\frac{1}{3} + 3 \\cdot \\frac{1}{3} = \\frac{1 + 2 + 3}{3} = 2\n\\] For a coin, the expected value (assuming it is fair) is 0.5. Suppose we have a room of 10 men and 40 women, where women take the value of 1 and men the value of 0. The average number of women in the room is \\(\\frac{40}{50}\\). This means the expected value of women in the room is .8. In other words, if we randomly selected a person from the room 10 times, we’d expect about 8 of them to be women. We can take averages with things aside from die and coins too. Naturally, if the amount of water in one giant jug was 5 liters and in another jug there is 7 liters, the average liters of water in the sample is \\(\\frac{1}{N}\\sum_{i=1}^{N} x_i = \\frac{1}{2}\\sum_{i=1}^{2} 5+7=6\\) liters. Now we should distinguish between the population and sample statistics: the sample is that subset of the population that we can get. We can rarely sample every single American in the country (the population), but a random (or representative) sample of 3000 Americans, say, is just fine. This difference is important: outside of simulations, we never can get every single datapoint for all our interventions of interest. So, we collect a sample which approximiates that population we are truly interested in.\nThe median, or the middle number, is also a type of average. It is less influenced by outliers than the average is. Suppose we have a dataset of years of education across a group of peopl in a neighborhood, \\(A=\\{5,6,7,9,18\\}\\). The middle number here is 7 (since two numbers lie to the let and right of 7). But let’s consider the issue deeper: suppose we were to use the average years of education at the average. For us, we have \\[\n\\frac{1}{5} \\times \\sum_{i=1}^{5} 5 + 6 + 7 + 9 + 18 =\\frac{45}{5}=9\n\\] The mean and median produce differing values. If we were to use the mean, we’d conclude the average person in this sample is in high school. When in fact, as a raw number, the modal respondent is a middle schooler with one elementary schooler. Thus, we can see that the average is influenced by outliers (in this case, somebody in graduate school). The classic joke is that when Bill Gates walks into a bar, everyone, on average, is a billionaire.\n\n\n3.1.2 Variance\nThe variance is the second moment. For a random variable \\(X\\), the sample variance is denoted as \\(s^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\bar{x})^2\\). For an intuitive example, suppose we have two middle schoolers in a room, one who reads at 6th grade level and the other at 8th grade level, \\(A=\\{6,8\\}\\). The sum of squared differences of each of these datapoints from the mean is (7) is 2, since 6 is 1 less than 7, and 8 is 1 more than 7. So, our sample variance is 2. The variance simply reflects the average distance of each data point from the center/mean of our observations. In practice however, we must correct for uncertainty. So we subtract by 1 in the denominator. This is called Bessel’s correction. Subtracting 1 factors in uncertainty, since practically we are unsure about the “true” average in a population.\nThe square root of our sample variance is what’s called the standard deviation from the mean. Why standard? The raw variance is not in the same units as our original data. When we take the square root, we may interpret this as the “standard” distance from the mean. For this simple example, we can round the standard deviation down to 1. When we think about it, it makes sense. If you’re at a middle school where the average reading level is 7th grade, people who read at 6th grade level are simply 1 year below the average, and those at 8th grade 1 year more than the average.\nSome might wonder why we’re squaring these differences from the mean. The squared differences gives more weight to outliers, or datapoints that are very far from the mean. Suppose \\(A=\\{6, 8, 18\\}\\). The average of this is 10.6. Person 1 and 2 are only 4.6 and 2.6 years less than the mean. But someone in middle school with a graduate in college reading level at 18 years is very, very, very far from the mean (practically speaking). The squared differences themselves are 21.79, 7.13, and 53.77, and the sample standard deviation is roughly 6.43. Had we not squared the differences, we’d get 4.89. So, we square the larger differences to assign more weight to large outliers, since not doing so would basically treat the middle schooler with a college graduate reading level as roughly equal to those who are much closer to the average of 10."
  },
  {
    "objectID": "basicprob.html#hypothesis-testing",
    "href": "basicprob.html#hypothesis-testing",
    "title": "3  Basic Probability Theory",
    "section": "3.2 Hypothesis Testing",
    "text": "3.2 Hypothesis Testing\nIn public policy we oftentimes wish to test hypotheses. A hypothesis is a statement about the world that we wish to determine the validity of. For example, we could hypothesize that the average math score for a scool is 86, or we can hypothesize that black people use welfare less than white people. We are always testing our hypothesis (which we call the research hypothesis, \\(H_R\\)) against a scenario where this hypothesis is wrong (the null hypothesis, \\(H_{0}\\)). That is, we start off by assuming the math score is not different from 86 or that blacks and whites use welfare at the same rates. We only change our minds in light of compelling evidence. If this confuses you, imagine we had a courtroom where the burden of proof is now shifted on the defense to prove their client innocent. We would never be okay with presuming guilt. No, we’d say that the people making the positive claim are the ones who must supply enough evidence to convince us otherwise. In research, one way doing this is by using something called a t-test.\n\n3.2.1 One Group T-Test\nFirst we cover the one-sample t-test, where we compare our research hypothesis against some known/predefined statistic. If I ask you what you think the average literacy rate is in the population of Americans, you may give different answers like “I think it’s at the 9th grade level”, “I think the average literacy level is less than the 6th grade level” or “I think the average literacy rate is different from 0”. Each of these forms sets of testable hypotheses. In the first case, \\(H_R\\) is “The literacy rate is equal to 9th grade.” In the second case, \\(H_R\\) is “The literacy rate is less than the 6th grade level.” Finally, we’d say \\(H_R\\) for case 3 is “I think the literacy rate in America is not 0” (or, some significant portion of the population can read). Formally, we denote these hypotheses as\n\\[\n\\begin{aligned}\n& H_{R}: L=9 \\\\\n&H_{R}: L < 6 \\\\\n&H_{R}: L \\neq 0.\n\\end{aligned}\n\\]\nOur corresponding null hypotheses (the ones we start by assuming) are\n\\[\n\\begin{aligned}\n& H_{0}: L \\neq 9 \\\\\n&H_{0}: L > 6 \\\\\n&H_{0}: L = 0.\n\\end{aligned}\n\\]\nwe simply take a of sample the population somehow (which is usually taken care of for us in census data/compiled statistics). We then calculate the sample average of the grade level of our sample (ranging from 0 being illiterate and 16+ which means postgraduate). Let’s visualize this.\n\n\n\n\n\n\n\n\n\nThis is a histogram. It shows a distribution of data. To better conceptualize it, imagine the height of the histogram (the y axis) represents the number of people in the data who take on the values on the x-axis. So, roughly 70 people have a literacy level of 8. In this case I generated a sample of 500 people with a small amount of variance. The average literacy rate in this population is 8 (for 8th grade). We now wish to see if our population mean (of 8th grade) is different from the researcher mean (9, 6, and 0). To do this, we need what’s called a t-statistic, which is a measure of deviation from the mean when taking into account the standard deviation from the mean and sample size. The formula for this simple t-statistic is\n\\[\nt = \\frac{\\bar{x} - \\mu_R}{\\frac{s}{\\sqrt{n}}}.\n\\] Let’s parse these terms. In the numerator we take the difference of our sample mean (\\(\\bar x\\), the mean we in fact observe in our dataset) versus our hypothetical mean that we are testing our sample mean against, denoted as \\(\\mu_R\\) (“myoo-sub R”). The denominator is the standard error, which is the standard deviation divided by the square root of our sample size. For example, here’s how we’d do this with the null hypothesis for 0 (that is, our sample mean is different from 0).\n\\[\nt = \\frac{\\bar{x} - \\mu_R}{\\frac{s}{\\sqrt{n}}}=\\frac{7.949 - 0}{\\frac{1.9983}{\\sqrt{500}}}=88.95.\n\\] In other words, our average literacy grade level is 88 times that of what we would expect given our standard error.\n\n\n3.2.2 Two-Group T-Test\nWe can also do a 2-group t-tests, where we wish to compare average group differnces. We can compare men and women, one city to another city, one city to many cities, and so on. For our purposes though, we’ll just compare one city to another one. I generate a sample of 20000 where one city has a mean of 6 and another of 14 with respective standard deviations of 1.5 and 3.\n\n\n\n\n\n\n\n\n\nHere is how we’d calculate the t-statistic in this case.\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\nwhere \\(\\bar{x}\\) is the averge of a city and \\(s\\) denotes the variance for each city. We plug in the values. For the denominator:\n\\[\n\\sqrt{\\frac{1.5}{10000} + \\frac{3}{10000}} = \\sqrt{\\frac{1.5 + 3}{10000}} = \\sqrt{\\frac{4.5}{10000}} = \\sqrt{0.00045}\n\\] which yields \\[\n\\sqrt{0.00045} \\approx 0.0212.\n\\]\nNow, calculate the t-statistic:\n\\[\nt = \\frac{6 - 14}{0.0212} = \\frac{-8}{0.0212} \\approx -377.36\n\\]"
  },
  {
    "objectID": "basicprob.html#uncertainty-around-the-mean",
    "href": "basicprob.html#uncertainty-around-the-mean",
    "title": "3  Basic Probability Theory",
    "section": "3.3 Uncertainty Around the Mean",
    "text": "3.3 Uncertainty Around the Mean\nTypically we are concerend with the uncertainty of our estimates. Uncertainty around the mean is typically expressed through confidence intervals. A confidence interval provides a range of values that, under certain conditions, contains the true population mean.\n\n3.3.1 Confidence Intervals and the Normal Distribution\nTo understand confidence intervals, it’s essential to first grasp the role of a normal/Gaussian distribution. The normal distribution is a continuous probability distribution characterized by its bell-shaped curve. We call it a continuous distribution because unlike coin flips, other data points can take on many values such as homicide rates, COVID-19 rates, and other metrics that can’t be broken into simple, countable groups. Most real-world phenomena, when measured, tend to follow a normal distribution (we will return to this in the lecture on asymptoyic theory). A normal distribution is defined by its mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The great thing about a normal distribuition is that we can prove that 68 and 95% of the data lie within 1 and 2 standard deviations of the mean. We will exploit this fact to construct a confidence interval.\n\n\n3.3.2 Constructing a Confidence Interval\nThe most common confidence interval is the 95% confidence interval. This means that if we were to take many samples and construct confidence intervals for each of them, approximately 95% of these intervals would contain the true population mean. Typically, we have only one sample to work with (and we rely on asymptotics to argue for the validity of our confidence intervals), but methods such as bootstrapping (where we simulate many such samples) may be employed to do this too. For our current purposes though, we construct a confidence interval for the population mean \\(\\mu\\), we use the sample mean \\(\\bar{x}\\) and the standard error of the mean as we’ve defined them above.\nFor a 95% confidence interval, we use the critical value from the standard normal distribution, typically denoted as \\(t^*\\). For a 95% confidence level, \\(t^* \\approx 1.96\\) (since approximately 95% of the values lie within 1.96 standard deviations from the mean in a standard normal distribution). The 1.96 number is a good approximation for all sample sizes greater than 30; otherwise, a different t-statistic would be used. In the old days, t-tables were used to do this, but now software handles this for us. We usually interpret confidence intervals that contain 0 (say, [-1,1]) as being insignificant. By extension, if the CI does not conatain 0 (say, [3,5]), we say it is significantly different from 0 (or, that the means are much different from one another). Note, 95% CIs do not mean 95% of the sample data lies within this interval. Confidence intervals are statements about the mean. Instead, it means that if we were to take many samples and construct intervals in the same way for a mean of interest, 95% of those intervals would contain the true population mean.\n\n3.3.2.1 One Group T-Test CI\nI generated a 10,000 person sample of incomes (in 1000s). The true average is 50. We think the average is 60. The standard deviation is the square root of 2. To estimate the confidence interval, we compute \\[\n0.0277= 1.96 \\times \\frac{\\sqrt{2}}{\\sqrt{10000}}\n\\] to get our standard error of the mean. Now, in order to characterize the range that the mean falls within, we simply do \\[\n\\text{CI} = \\mu \\pm \\text{Margin of Error}=50 \\pm 0.0277=(49.986,50.014).\n\\]\nWe interpret this as “Our sample mean is 50 thousand dollars. We are 95% confident that given the data, the real mean lies between 49.986 and 50.014 thousand dollars.” Since both these numers are less than 60, our research hypothesis (\\(H_R=60\\)) is likely incorrect, as 60 does not fall within these estimates.\n\n\n3.3.2.2 Two-Group T-Test CI\nNow, we can revisit the city example using product weight from the above and see if the means significantly differ. We typically use this kind of t-test in situations where we wish to compare one group to another. The formula for the CI for the difference between two means is given by: \\[\nCI = (\\bar{x}_1 - \\bar{x}_2) \\pm t \\times \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\]\nWe know the values from the above, so we plug them in. We also use the critical value of 1.96, since this is the value we use for a 95% CI. First we compute the standard error using the variances and sample sizes for both groups: \\[\nSE = \\sqrt{\\frac{1.5}{10000} + \\frac{3}{10000}} = \\sqrt{\\frac{4.5}{10000}} = \\sqrt{0.00045} \\approx 0.0212.\n\\] We now have a margin of error (using the critical value 1.96 as above) of:\n\\[\n\\text{Margin of Error} = t  \\times SE = 1.96 \\times 0.0212 \\approx 0.0413\n\\] We already know the mean difference is -8, so now we just plug that in and solve:\n\\[\nCI = (-8) \\pm 0.0413.\n\\]\nSo, the 95% confidence interval for the difference in means is: \\[\n(-8 - 0.0413, -8 + 0.0413) = (-8.0413, -7.9587).\n\\]\nThis interval suggests that City 1 consumes significantly less, on average, than City 2. By the way, for those curious, if we just reversed the order of the numerator, we’d get the same result but it would be \\((7.9587,8.0413)\\), where we’d say that City 2 consumes significantly more than City 1."
  },
  {
    "objectID": "basicprob.html#a-brief-word-on-practical-significance",
    "href": "basicprob.html#a-brief-word-on-practical-significance",
    "title": "3  Basic Probability Theory",
    "section": "3.4 A Brief Word on Practical Significance",
    "text": "3.4 A Brief Word on Practical Significance\nTo conclude, a word of caution: as we can see, the magnitude of the t-statistic and tightness of the CIs tend to scale with sample size. Consider the two group case: \\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}.\n\\]\nWe can see that an increase in the sample size leads to a decrease in the overall denominator. Supoose for the denominator \\(s^2=4\\) for both groups. If both groups have the size of 40, then we just have \\(\\sqrt{.1+.1}\\). But if both groups have a sample size of 400, the then we have \\(\\sqrt{.01+.01}\\). The reason this matters is because researchers oftentimes interpret the t-statistic and whether it’s greater than 1.96 as a measure of practical importance. But this is wrong! Since our t-statistic is guaranteed to increase with sample size, per the formulae above, at certain sample sizes it would be hard for our confience intervals to NOT contain 0/be significantly different.\nWhat this means as a matter of practicality is to always keep in mind your sample size and what would matter practically to people in real life. If you estimate that the price of one brand of bottled water, for example, costs 0.05 dollars more than another brand across all 50 states, and your t-statistic is 70 and your CI is [0.01,0.06], then do not claim (in isolation anyways) that this difference is very meaningful or earth-shattering, since they make either a penny more or 6 cents more. In other words, the findings are statistically significant, but practically they are meaningless. The only way for this to really matter to anyone is by having some metric about how much each brand sold (in terms of individual water bottles) for us to reach any firm conclusion about how much this average difference matters. I say this because I do not want for you, in real life or in your papers, to apply these ideas mechanically. I want you to always keep in mind how statistics maps on to the real world."
  },
  {
    "objectID": "asymptotic.html",
    "href": "asymptotic.html",
    "title": "4  Asymptotic Theory",
    "section": "",
    "text": "5 Summary\nAsymptotic theory simplifies statistical analysis by encouraging us to think about the “true” population of interest. It provides us with tools to derive more accurate and precise estimates from. But, as a more general rule for policy analysis (and life), it demands us to think with an infinte mind instead of a limited one. In other words, we should always ask ourselves as researchers “if we could sample everyone, would this statistic I just calculated be close to reliable?” One practical implication of this is having a sufficiently large sample size in order to increase the probability of being closer to the true mean. However, as the previous section discusses, these asymptotics are only as justified as the quality of the sample. We need, in other words, our sample to be as representative as possible of the broader population of interest. In the next lecture on correlation, aside from sampling, we will discuss the basics of statistical design in order to have valid results from statistical analysis."
  },
  {
    "objectID": "asymptotic.html#law-of-iterated-expectations",
    "href": "asymptotic.html#law-of-iterated-expectations",
    "title": "4  Asymptotic Theory",
    "section": "4.1 Law of Iterated Expectations",
    "text": "4.1 Law of Iterated Expectations\nSuppose we wish to commute to and from Georgia State University from Marietta, Georgia. As we’ve discussed in the previous chapter, we can think of some variable \\(c\\) (commute time) as a random variable, as its value is not guaranteed until we actually leave home and arrive. Suppose we are now interested in the average time it takes us to arrive using I-75 South, conditional on us taking the South or North depending on our starting point. We think I-75 South will take 15 minutes, compared to I-75 North which we think will take 20. We can formalize this also as \\(x =\\{1,0\\}\\), where we take North being coded as 1, else as 0 if we take North. So, we leave home (or school) using either highway, and record how long it took to get to the destination. Say, we record the value of 30 minutes in the morning and 40 in the afternoon. Do we conclude that this is how long it takes to get to school on averge, and that we should use some other interstate? No. Why not? This is only one estimate from one day. There are all kinds of things that could have been going on in the morning or afternoon that might influence your travel time, most notably traffic, construction, or other random events for any given path we choose. So, what are we left to do? The only thing we can do, is collect more data.\nSo suppose we take this same highway for one month, using only 75 South and North (unlike say I-285). We record the amount of time it took us to get to school and home that day using either way (that is, we record our commute time to and from school each day for both ways, taking the average separately for each week). Mathematically, we express this as the expected commute time conditional on the chosen route \\(x\\): \\(\\mathbb{E}[c|x=1]\\) (expected commute time given we take North) and \\(\\mathbb{E}[c|x=0]\\) (expected commute time given we take South). The Law of Iterated Expectations (LIE) states that the overall expected commute time for I-75 (to Marietta, anyways) is the average of these conditional expectations, weighted by how often each route was taken. Formally: \\(\\mathbb{E}[c] = \\mathbb{E}[\\mathbb{E}[c|x]]\\). This means that the overall average commute time \\(\\mathbb{E}[c]\\) is the expected value of the conditional expectations \\(\\mathbb{E}[c|x]\\)\nSo, if after a month of data collection, we find that the average commute time is 17 minutes, this is the unconditional expectation \\(\\mathbb{E}[c]\\), calculated by taking the average of the commute times across both routes across all days. This approach, where we repeatedly take the average of an outcome/variable given some condition (here, the route we take), reflects the Law of Iterated Expectations."
  },
  {
    "objectID": "asymptotic.html#law-of-large-numbers",
    "href": "asymptotic.html#law-of-large-numbers",
    "title": "4  Asymptotic Theory",
    "section": "4.2 Law of Large Numbers",
    "text": "4.2 Law of Large Numbers\nWhy might this be true, though? Why, after taking all of the averages across a whole month do we arrive at 17, which is a lot closer to 15 than we first thought? Surely, this number is much quicker than the 30 minutes we took the first day? The reason for us getting this value is because of what we call the Law of Large Numbers, or \\[\n\\lim_{N \\to \\infty} P\\left(\\left|\\frac{1}{N} \\sum_{i=1}^{N} x_i - \\mu\\right| < \\epsilon\\right) = 1.\n\\] Here is what the math says: as our sample size \\(N\\) increases without bound (that is, as we take the highway more and more and more and more and more… to an infinite amount of times), the probability that the average of our individual empirical daily commute times \\(x_i\\) approaches the true population value is 1. In other words, the more estimates we take, the closer, in probability, we come to our population estimate. You see, the first day of 30 minutes was simply a sample of 1. We had nothing else to base our ideas off of aside from whatever GPS tells us. Maybe there was traffic or some other unforeseen thing. However, as we take the highway more times, we tend to get a better sense of how long it’ll take to get places, what lanes to use, and so on and so forth. To further prove this, the expected value of a three sided die numbered 1 to 3 is 2. If we cast the die three times and then take the average of what we get, we should see the cumulative empirical average converge to the theoretical average.\n\n\n\n\n\n\n\n\n\nAs it turns out, that’s exactly what we do see. As researchers, what this means is that drawing from a large sample tends to be better than a small one for empirical. For example, if someone has a sample size of 20, we likely would not be okay with generalizing one particular aspect of this sample (say, weight or political affiliation) to everyone in the same city, as we’d need more data points to average over. Ideally, if we’re trying to sample American public opinion, we don’t want to use only 2 American states, ideally we’d have all the state data available to use."
  },
  {
    "objectID": "asymptotic.html#central-limit-theorem",
    "href": "asymptotic.html#central-limit-theorem",
    "title": "4  Asymptotic Theory",
    "section": "4.3 Central Limit Theorem",
    "text": "4.3 Central Limit Theorem\nBy understanding the Law of Iterated Expectations (LIE) and the Law of Large Numbers (LLN), we can now delve into the Central Limit Theorem (CLT), which helps us characterize the overall distribution of commute times. That way, we can do things like calculate the confidence interval of our commute times, hoping it will approximate the true one. The CLT says: \\[\n\\frac{\\bar{x_i} - \\mu}{\\sigma / \\sqrt{N}} \\xrightarrow{d} N(0, 1)\n\\] or that as the sample size increases, the probability of our empirical distribution approaches[a normal distribution. Aside from the distribution of our measurements, our sample mean, by LLN, approaches the population mean. If this seems at all abstract to you, we may simulate this. Here, I define the average commute time to be 15 with a standard deviation of about 3 minutes (between 12 and 18 minutes). I then simulate the commute time across 20000 commutes (since I didn’t wish to drive down Interstate 75 20,000 times!). We can clearly see from the GIF that the empirical distribution (that is, the commute times we experience) quickly approaches the true population average time as we commute more and more.\n\nThis convergence supports the LLN, which tells us that our sample mean will approach the true mean \\(\\mu=15\\) as we collect more data. Additionally, notice how our confidence intervals get tighter and tighter given an increase in sample size. Note that this result is intuitive: as we collect more data, it would make sense that we have a better picture about the underlying data (such as, the sample mean and variance). So, with this in mind, it makes sense that we have a better sense of our uncertainty of our estimates, as the confidence interval shows. So, with the commuting example, the first day took us a half hour. We suspected that the commute would be 15 and 20 minutes respectively (the expected value of which is 17.5 minutes). So, in terms of minutes, we could hypothesize that the true travel time takes between 15 minutes or 45 minutes. In other words, we’re quite uncertain. But, as we collect more data, the uncertainty decreases, tightening to be around the true value.\n\nAdditionally, the CLT allows us to visualize how even if the original distribution of the variable is not normal (say, a coin toss where we only have two outcomes, heads or tails), the distribution, given enough samples, converges to a normal distribution.The plot above flips a coin 1000 times, plotting the empirircal distribution after every 100 flips. We can see that the distribution, despite having only two outcomes, converges to a normal one. Thus, CLT allows us to construct confidence intervals and make inferences about a population, given a large enough sample."
  },
  {
    "objectID": "asymptotic.html#sampling",
    "href": "asymptotic.html#sampling",
    "title": "4  Asymptotic Theory",
    "section": "4.4 Sampling",
    "text": "4.4 Sampling\nBefore we continue however, we need to say a few words about sampling, and the idea of collecting a random, probibalistic sample versus a non-random sample. The reason this matters is because no matter how we calculate our statistics, we need to be sure that our statistics can actually map on to the actual constructs we want them to. A random sample, in principle, is the idea that everyone from a given population has the same chance to be included in the sample for some study. For example, if we wanted to survey the public opinion of Georgia State students, one potential way of doing this would be to get every single active GSU email from the University and put it into a spreadsheet. We then could generate a variable in the spreadsheet called a “Bernoulli” variable, or a variable that takes on the values 0 or 1. We can then, after setting the number of observations (defined as the number of emails we have), define the probability with which a given variable takes on 0 or 1 to be 0.5. In other words, everyone has an equal chance of being included in the survey, 1 means youre included, else 0. Note, there are other, much more sophisticated ways to do this, but this is one way of taking a random sample.\nWhat would a non-random sample look like, then? Well, in principle it’s where everyone in the population does not have an equal chance of being included in the sample. But, this is a simple definition. It does not explain why it’s bad or why it’s harmful to researchers. So, let’s consider a few kinds of non-random sampling. The most obvious one is something called convenience sampling. As the name suggests, we take a sample based on whoever is around us. This can be in our class, on the street, or in our friend group. For example, I have two coworkers, one goes to MIT and the other went to Columbia, both for econ degrees. If I wanted to know how good the average person in the United States is at calculus or math, then I cannot simply assess their skillset and reach a conclusion. Why not? Well, MIT and Columbia are incredibly selective schools that select for math skills. Furthermore, the fact that I know them or work with them is likely correlated to my research interests which demand a good background in statistics/applied mathematics. Similarly, even if we personally randomly asked people on the street, this is still not a random sample. Why? Well, there are likely underlying reasons to people being in certain locations. If you’re in the biology department, it’s likely that most people you meet will have an uncanny knowledge about germ theory or anatomy. And we’d expect this, we’d say something’s wrong if these were not true.\nAt this point, you’re likely asking what any of this has to do with the previous discussion of asymptotic theory or policy analysis for that matter. Well, the sample we collect is directly related to the results that we obtain. If our goal is to approximate math knowledge in America, it’s inapprorpiate to only sample students from the engineering department at Georgia Tech or the economics students at Chicago. To say it differently: even if we did collect data from every single student at every single engineering department in the United States, this would map on to the population of engineering students, at best, not the United States as a whole. In statistics language, our estimate of the “true” mean would be biased, in this case significantly upwards. Even though our confidence intervals would be more precise as our sample size increases (that is, the more engineering students we ask), we would still never converge to the true parameter because our sample is so dissimilar to the population of people we care about, effectively giving us the right answer to the wrong question. When we discuss regression, the importance of sampling will become even more apparent, but for now suffice to say that it’s important that the sample be as representative of the underlying population as possible."
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "5  Correlation and Association",
    "section": "",
    "text": "6 Summary\nCorrelation and causation is a very delicate topic in statistics. Whenever we’re doing research, we must always be careful to structure our studies in such as way that our findings are not corrupted by other factors, even if our software tells us we’ve found a “statistically significant” correlation. I’m sure if we took the t-statistic of music programs and graudation (or funding of music programs, as my scatterplot does), we’d find a very high t-statistic for the correlation coefficient. Equally, we’d find a high t-statistic for countries reporting increases in loneliness and low fertility rates. But I don’t care, and neither should you, since there are other factors which contribute to graduation aside from music, and a lot more factors driving fertility rates than simple loneliness or lack of opportunity to meet people. Thinking causally can be a challenging thing. After multiple years of torment, you will learn to think like this as if by muscle memory since it’ll be so routine to you. We will cover this in more detail in our chapter on treatment effect estimation."
  },
  {
    "objectID": "correlation.html#a-prelude-to-regression",
    "href": "correlation.html#a-prelude-to-regression",
    "title": "5  Correlation and Association",
    "section": "5.1 A Prelude To Regression",
    "text": "5.1 A Prelude To Regression\nTypically, when we discuss correlation, we use scatterplots to visualize the association between variables. A scatterplot is simply a chart which depicts the co-movement of variables. On the x-axis we plot our independent variables/predictors (or, the things we think explain a given outcome) and the y axis plots the variable we think is being affected by the one on the x-axis. Below, I plot in the left panel the homicide rate per 100,000 versus the state-specific Gini coefficient for 27 Brazilian states in the year 1990. The right panel plots the average retail price of cigarettes versus cigarette consumption per capita for 39 American states in the year 1980.\n\n\n\n\n\n\n\n\n\nWell now, what do we see here? We see the plotted datapoints along with the Pearson’s \\(r\\). We can see a negative correlation coefficient reported, where a one unit increase in the Gini coefficient leads to a decrease in the homicide rate in Brazil for this year. We also observe a negative relationship between the retail price of cigarettes and the consumption of cigarettes, where an increase in price leads to a decrease in the amount of cigarettes consumed. This result especially should be pretty intuitive: all else equal, as the price of a good increases, the demand for said good generally decreases. However, what about the leftmost plot? The Gini coefficient is a measure for inequality, where 1 denotes one person has all the money and everyone else nada/nothing. A Gini coefficient of 0 means everyone has the same amount of money, and a measure of anything in between is some intermittent level of inequality. Well, this result is puzzling in a bivariate lens: typically, we associate income inequality and poverty with an increase in things like homicide and other kinds of crime. This is where we begin to think atypically. While correlation is a useful measure sometimes, it alone is inadequate for serious policy analysis. Below, I explain why."
  },
  {
    "objectID": "correlation.html#the-first-exercise-of-the-statistical-mind",
    "href": "correlation.html#the-first-exercise-of-the-statistical-mind",
    "title": "5  Correlation and Association",
    "section": "5.2 The First Exercise of the Statistical Mind",
    "text": "5.2 The First Exercise of the Statistical Mind\nEarlier this morning, I was on Facebook and one of my friends posted a picture that cited a story of a public school music teacher named Annie Ray winning a Grammy. In the article, they say [caps theirs]\n\nTHE FACTS ABOUT THE IMPORTANCE OF MUSIC EDUCATION DON’T LIE… Schools that have music programs have significantly higher graduation rates than do those without music programs (90.2 percent as compared to 72.9 percent).\n\nFor visualization, let’s do some graphing shall we? The plot on the left plots the rates from the quote, the right plot is simulated data.\n\n\n\n\n\n\n\n\n\nThis quote caused fire alarms to sound in my head. Why? Because the article (and reporting on music education more broadly) misleadingly discusses these statistics. This does NOT mean these statistics are wrong in terms of their computation. Presumably whoever did this used statistical software to get these numbers, and I trust that the numbers are accurate.\nMy criticism is about practical implication. The heavy suggestion from the block quotation is the music programs are causing this 17.3 percentage point difference in graduation rates. When we read things like “The facts don’t lie”, there’s this air of certainty that these statistics are being reported with.\nAnd in fairness, this is not a completely crazy idea: music does indeed help people learn languages. It likely helped me learn Spanish and made me a better overall reader. It’s also correlated with spatial skills. In fact, in another life, I was a music theorist who could do harmonic analysis of chord progressions and tell you what I thought the artist was trying to communicate. And as a matter of fact, music studies likely made me an even better econometrician, because one thing music analysis teaches you about is context. And in larger context, these statistics seem misleading.\nThe suggestion is that if more schools just had music programs, we’d see higher graduation rates by 17.3 percentage points, on average. And yes, to some degree, if a school now has a music program and did not have one before, it is true students now have the opportunity to learn music. They may even go on to study and succeed in music now that they have the opportunity to do so. But how could we estimate this? How many people would this even affect, exactly?\nMost people do not want to be musicians. Learning music, like any other art form or professional skillset, is a non-trivial investment of time and money. Everyone doesn’t have the means or honestly dedication to pursue it, esepcially in light of other potential desires or opportunities. Those who do decide to become musicians (not just in classically trained in school, but generally) may have personal qualities that differ from other students in ways that affect whether they graduate or other aggregate metrics of success. So if 20 more students in a school of 5000 and a graduating class of 300 reap the benfits of music, is this really enough to move the proverbial needle on the graduation rate for an entire school or county? Not likely.\nMore to the point, not every school has the means to have a music program, nevermind a well funded music program. According to statistics reported by Yamaha, 8% of all public schools in the United States don’t have any arts programs at all (music, theatre, or dance).\nIn the U.S., schools are funded by property taxes in public schools and by extremely wealthy donors at private schools. This means that the wealthier public schools will be located in wealthier districts which naturally has a bigger tax base. What do those districts have more of? Money! Status, class, opportunity. Instruments do not grow on trees, they cost money; not every school has 80,000 dollars for a Steinway piano. By the way (no, I did not look this up before writing it), the same study, according to Yamaha’s reporting, said\n\nThe study also noted that a disproportionate number of these students without access to arts education are concentrated in major urban or very rural areas where the majority of students are Black, Hispanic or Native American, and at schools with the highest percentage of students eligible for free/reduced-price meals.\n\nI have to emphasize again, I didn’t suspect this because I looked it up before, I suspected it because this is what it means to think like a statistician. It means you have to think in a multivariate way, where more than one thing can affect something else. Here, this idea is pretty obvious: how do we know that these graduation rates are not explained in part (if not entirely) by baseline differences in socioeconomic status of communities (poverty, low employment rates, larger contextual factors), opportunities of individual families (say, personal connections individual families may have that others don’t), as well as the effects those factors have on individual students? When we think of it like this, simply suggesting that music programs would be a great solution is not so convincing.\nThese aside, there may also be what we call “selection bias” here, where some students are able to select into/decide to go to a given school. For example, some schools have magnet programs. The one I was in was for the International Baccalaureate Program; other high schools have STEM magnet programs or music/arts programs where some schools literally recruit middle school students who want to pursue these things. And when they select for these qualities, it becomes hard to isolate the impact of a music program on a graduattion rate, becasue they’re already selecting for highly motivated students (who mostly but not exclusively are from wealthier districts or better-to-do families). These are the kinds of students who would perform well anyways, even if music wasn’t there.\nThis again does not discount the real benefits of music education or the arts more generally! But when we read statistics, as with harmonic analysis of music chords, we also need to understand the context they exist within, and when we do this we begin to see that the relationship is not as clear cut as simple descriptive statistics might seem. As we see from the simulated plot, some schools have a graduation rate of almost nobody, which also happen to have low levels of music funding. And while I couldn’t find specific examples of this when I looked up data on it, numbers this low do happen anecdotally. And when we see numbers like this, we must ask ourselves “How are these sets of schools different from these sets of schools?”\nAs we will see when we cover regression analysis, the world rarely works off of pretty, linear functions. Measures of simple association do not mean that something is causing another thing, the world is much too complex for that. As researchers and as human beings, we must constantly be skeptical of simplistic claims and investigate them when they seem too good to be true."
  },
  {
    "objectID": "correlation.html#tying-this-in-with-asymptotic-theory",
    "href": "correlation.html#tying-this-in-with-asymptotic-theory",
    "title": "5  Correlation and Association",
    "section": "5.3 Tying This in With Asymptotic Theory",
    "text": "5.3 Tying This in With Asymptotic Theory\nAs we learned before, statistics is justified oftentimes on large scale asymptotics, where we consider an infinte population of units to sample from. In this framework, there’s the idea of the existence of a population coefficient/average which exists only in theory and a sample (a subset of the population) which we use to calculate a mean which approximates that “true” value.\nSuppose we have a spreadsheet at hand for the year 2018. The first column of said dataset/spreadsheet is the name of an American high school. The second column of the spreadsheet is some variable \\(m \\in \\{0,1\\}\\). This, in English, means the column takes the value 0 or 1, where 0 means a school does not have a music program and 1 means they do have one. The third column of the dataset is the respective graduation rate of that school for 2018.\n\n\n\nHigh School\nMusic\nGraduation\n\n\n\n\nBen Franklin\n1\n92\n\n\nPaul Revere\n0\n75\n\n\nAlexander Hamilton\n1\n90\n\n\nThomas Jefferson\n0\n73\n\n\nGeorge Washington\n1\n88\n\n\n\nSuppose now we’re interested in the effect of music programs on graduation rates. Formally, we may denote our observed outcomes as \\[\n\\begin{aligned}\ny_{i} =\n\\begin{cases}\n    y^{0}_{i} & \\forall \\: i\\in \\mathcal{N}_0\\\\\n    y^{1}_{i} & \\forall \\: i\\in \\mathcal{N}_1\n\\end{cases}\n\\end{aligned}\n\\] where \\(\\mathcal{N}_1\\) is the set of schools that has a music program versus \\(\\mathcal{N}_0\\), or the set of schools that do not, where \\(y^1\\) is the graduation rate we observe when \\(m=1\\) and \\(y^0\\) is the graduation rate we observe when \\(m=0\\).\nIf we simply believed the article I cited above, we’d say that there is some effect of music education on the graduation rate (or the average of the difference between the graduation rates of schools with and without music programs, \\(\\frac{1}{N}\\sum_{i=1}^{N}\\left(y^1-y^0\\right)\\)). Following the article, we’d say that this difference around 17.3 percentage points (in the table above, it’s around 16).\nBut this cannot be! As we’ve discussed, many more factors will affect not just whether a school has a music program and their graduation rates. So, if we were to simply compute \\(\\frac{1}{N}\\sum_{i=1}^{N}\\left(y^1-y^0\\right)\\), the number 16 (or 17) would be wrong, even if we had access to every single high school and college on the planet. Because of the baseline diffenreces between these schools, our average mean difference (think a two-group t-test) would never converge to the population mean for the “effect” of music education. We would say that this is a biased estimate becasue our estimate would be very far from the true effect, since the true effect is likely much smaller than 17."
  },
  {
    "objectID": "correlation.html#implications",
    "href": "correlation.html#implications",
    "title": "5  Correlation and Association",
    "section": "5.4 Implications",
    "text": "5.4 Implications\nThe reason this matters for policy analysts is because when taken to its conclusion, mistaken correlation for causation or not thinking about things in a systemic, multivariate way could result in pumping more money into music programs in order to fix failing schools, a much wider and more sophisticated problem.\nIt leads to things like pumping more money into police deparmtents to decrease crime rates, even though crime has been falling in general for decades in the U.S. and there’s no real link between militant policing measures and crime reduction.\nIt also leads to things like mass like many modern governemnts doing things like making dating apps and sponsoring mass dating events (yes, really!) in order to increase birth and marriage rates.\nThe idea of course is that people aren’t having kids or getting married because they’re not meeting one another. And if more people met, the more kids they’d have. And to a degree this is true due to things like social media, so the underlying logic makes plenty of sense. But then, once we agree to this probelm, we have to ask why are people not meeting one another and having kids or getting married? The real problem is a lot more systemic. Especially in South Korea, Japan, and China, the reasons are mostly having to do with labor issues, changes in gender norms, and broader social factors which lead to people not wanting to have kids."
  },
  {
    "objectID": "ols.html",
    "href": "ols.html",
    "title": "6  OLS Explained",
    "section": "",
    "text": "7 Summary\nThis undoubtably is the most weighty chapter, both in terms of mathematics and in terms of practical understanding. Regression is one of the building blocks for policy analysis, in addition to solid theoretical background and contextual knowledge of the policy being studied. The reason I chose to cover this first, in the first few weeks of the class instead of waiting until the end, is because I believe that the only way to truly understand regression is by use in applied examples. This is what you’ll explore more of in your papers."
  },
  {
    "objectID": "ols.html#a-primer-on-data-types",
    "href": "ols.html#a-primer-on-data-types",
    "title": "6  OLS Explained",
    "section": "6.1 A Primer on Data Types",
    "text": "6.1 A Primer on Data Types\nFor any dataset you ever work with, you’ll likely have different variables (columns). The predictors for regression must be numeric, naturally. These take a few different types.\n\n6.1.1 Ratio Variable\nThe most common kind is a ratio variable (a value we may express as a fraction/continuous variable), such as the employment rate.\n\n\n\nState\nYear\nEmployment Rate (%)\n\n\n\n\nAlabama\n1990\n55.3\n\n\nAlabama\n1991\n56.1\n\n\nCalifornia\n1990\n62.1\n\n\nCalifornia\n1991\n61.5\n\n\nGeorgia\n1990\n58.4\n\n\nGeorgia\n1991\n59.2\n\n\n\nA dummy variable is a binary variable that indicates the presence or absence of a characteristic. A dummy variable (also called an indicator or categorical variable) is a variable that takes on the values 0 or 1. For example, a simple dummy indicates whether a respondent in a survey is a man or a woman.\n\n\n\nRespondent ID\nGender (Male=1, Female=0)\n\n\n\n\n1\n1\n\n\n2\n0\n\n\n3\n1\n\n\n4\n0\n\n\n\nDummies can also be used to capture unobserved variation across groups. For instance, when predicting homicide rates across states like Alabama, California, and Georgia for 1990 and 1991, we can include dummy variables for each state. These dummies help account for unique, stable characteristics of each state, such as culture, that are hard to measure directly.\n\n\n\nState\nYear\nAlabama (1/0)\nCalifornia (1/0)\nGeorgia (1/0)\n\n\n\n\nAlabama\n1990\n1\n0\n0\n\n\nAlabama\n1991\n1\n0\n0\n\n\nCalifornia\n1990\n0\n1\n0\n\n\nCalifornia\n1991\n0\n1\n0\n\n\nGeorgia\n1990\n0\n0\n1\n\n\nGeorgia\n1991\n0\n0\n1\n\n\n\nThere is also a notion of an ordinal variable, where the data at hand must obey a specific order. Suppose we ask people in a survey how religious they are on a scale from 1 to 10, where 1=Atheist and 10=Extremely Religious. Here, order matters, because 1 has a very different meaning from 10 in this instance. An ordinal variable has a clear, ordered ranking between its values.\n\n\n\nRespondent ID\nReligiosity (1-10)\n\n\n\n\n1\n3\n\n\n2\n7\n\n\n3\n5\n\n\n4\n10"
  },
  {
    "objectID": "ols.html#review-of-lines-and-functions",
    "href": "ols.html#review-of-lines-and-functions",
    "title": "6  OLS Explained",
    "section": "6.2 Review of Lines and Functions",
    "text": "6.2 Review of Lines and Functions\nIn middle school, we learn about the basics of functions in that when we plug in a number, we get another number in return. For \\(2x=y\\) for example, if we plug in 2, we get 4. If we plug in 5, we get 10. If you’re at the grocery store and grapes are 1 dollar and 50 cents per pound, we just weigh the grapes and multiply that number by 1.5. This could take the form of \\((0,0), (1,1.5), (2,3)\\), and so on. In fact, we can represent these data points in a table like this\n\n\n\nx\ny\n\n\n\n\n0\n0\n\n\n1\n1.5\n\n\n2\n3\n\n\n\nThese points form a line, the equation for which being \\(y=mx+b\\). We can also think of this line as a function, where we get some value of \\(y\\) given some values for the other variables. Here, \\(y\\) is how much we pay in total, \\(m\\) is the change in how much we pay for every 1 pound of grapes bought, and \\(b\\) is our value we pay if we get no grapes.\n\n\n\n\n\n\n\n\n\nThe way we find the \\(m\\) and \\(b\\) for a straight line is the “rise over run” method, in this case\n\\[\nm = \\frac{y_2 - y_1}{x_2 - x_1} = \\frac{3 - 0}{2 - 0} = \\frac{3}{2} = 1.5\n\\]\nFor this case, the function for the line is \\(y=1.5x\\). For here, \\(b=0\\) because in this case, how much we pay is a function of pounds of grapes only. We could add a constant/\\(b\\), though. Suppose we’d already spent 10 dollars, and now how much we spend is a function of both some previous constant level of spending, and new amount of grapes bought. Now, our function is \\(y=1.5x+10\\). Either way, constant or not, notice here how the line explains how much we pay perfectly (in other words, the line exactly matches the data points, as the line intersects perfectly with the dots on the plot). This means our residuals, or the difference between the prediction of the function and what we pay are 0.\nFor the above, \\(y_i\\) and \\(x_i\\) maps on to the \\(i\\text{-th}\\) real data point for each repsective variable. So for example, \\(y_2\\) for the above table is 1.5, as it represents the second row of the y variable, and \\(x_1\\) is just 0. \\(\\hat{y}_i\\) (which we call y-hat) is the \\(i\\text{-th}\\) prediction point for the line/function. Here, the letter epsilon (\\(\\hat{\\epsilon}_i\\)) is just a variable for the (Euclidean) distance from the \\(i\\text{-th}\\) predicted point to the observed point. For example, if the observed value for our first data point is 10 but \\(\\hat{y}_1=11\\), then \\(\\hat{\\epsilon}_1=10-11=-1\\). Going forward, I will use the words “outcome (variable” or “dependent variable” to refer to the thing that we are studying the change of, and “predictors”, “covariates”, or “independent variables” to refer to the variables we think affect our outcome.\nWith all this being said though, the example above is very simplistic. This is a case where all the necessary information is known (price and weight). The algebra is so simple we that we intuitively understand that this is how we calculate expenses. But…. what if the data we have at hand are not nice and neat in terms of a function?\n\n\n\n\n\n\n\n\n\nTake the idea of predicting crime rates in Brazilian states in the year 1990 using the inequality level as a predictor, or data on the consumption of cigarettes in American states in the year 1980 using price as a predictor. Now for these examples, how do we calculate the rise over run? We would presume some function exists that generates the crime rate for that Brazilian state in that year, or that consumption level for that American state in that year.\nHowever, is quite obvious that no deterministic function exists here for either of these cases. Indeed, when I draw a line between these datapoints for both examples, we find that non-zero residuals still exists (in other words, the model imperfectly predicts homicide rates and cigarette consumption levels in both datasets.) The residuals are plotted below the scatterplots above. They are simply the difference between the line and the observed value. For the left graph, the predicted homicide rate is around 35, but we in fact observe around a homicide rate of 18. So, our residual here is maybe -17, since our observed homicide rate is 17 less than our predicted one.\nBefore, we simply would throw our hands up, in a sense, and say that there’s no solution. Algebra has failed us in that we cannot find a function which perfectly explains these data points, as we could with the grape example above. But, we should not dismay. After all, this is what the real world is like, right? This is what it means to think in a multivariate way. Homicide rates and cigarette sales are random variables in the sense that they are produced by some latent probability function that is determined by a wide variety of factors.\nThe homicide rate or cigarette consumption rate in any state anywhere is not guaranteed. Sometimes in some states, homicides (or crime in general) is high, other times its low. Why? Well, some states are wealthier and some are poorer. Some states vary by racial compositions, or will differ by factors like age composition, income inequality, alcohol use, and gun ownership. Thus… some cities have high homicide rates, others have low homicide rates.\nWe can reason accordingly for cigarette consumption of American states. American states will vary on cigarette consumption based on a variety of factors. Natrually, one factor would be the price of cigarettes, as one might expect, since people tend to not want to buy more of a good as the price increases (well… usually.) The number of young people in that state may mean that younger people are risk takers and may be more likely to smoke than adults (or alternatively, young people may perceive smoking as something for older adults and smoke less). Levels of alcohol taxation may matter as well, since alcohol may be a substitute for tobacco, so states with higher taxation may smoke more, on average. Also, plain and simple measures like culture (and otehr unobservable things) may play a role. After all this, Indeed, it would be very unreasonable to expect to find a singular function that perfectly explains the variation in either of these datasets. The real world is simple too complicated to be thought of like that.\nSo, what can we do? We can’t find a function for the line that perfectly explains these data… But, how about we instead seek the best possible straight line? As it turns out, this is not a fool’s errand."
  },
  {
    "objectID": "ols.html#arrivederci-algebra-ciao-derrivatives.",
    "href": "ols.html#arrivederci-algebra-ciao-derrivatives.",
    "title": "6  OLS Explained",
    "section": "6.3 Arrivederci, Algebra, Ciao Derrivatives.",
    "text": "6.3 Arrivederci, Algebra, Ciao Derrivatives.\nTo do this though, we’ve now reached a point in the course where simple algebra is no longer our best guide. We now must use calculus, specifically the basics of derivatives and optimization.\n\n\n\n\n\n\nImportant\n\n\n\nOkay, so here I’m kind of lying. You actually don’t need to say farewell to algebra (completely) to derive regression estimates, but that process “requires a ton of algebraic manipulation”. For those brave of heart who know algebra well, you can probably just watch the series of videos I just linked to and skip to this section of the notes, but I do not recommend this. I find the calculus way via optimization a lot more intuitive.\n\n\nBefore we do this though, a primer on derivatives. The derivative is the slope of a curve/line given a very small change in the value of the function. One very useful property about derivatives is that when we set the first derivative of a function to 0 and solve for the variable (I do an example below), we reach a maximum or minimum point on the original function, usually. We use derivatives in the context of an optimization problem to minimize the squared residuals. Optimization problems take the form of \\[\n\\min_{\\theta \\in \\Theta } f(\\theta) \\: \\mathrm{ s.t. \\:}  g(\\theta) = 0, h(\\theta) \\leq 0,\n\\] where there’s some function \\(f(\\theta)\\) (called the objective function) that is minimized (or sometimes maximized) over a set of \\(g(\\theta)\\) equality constraints and \\(h(\\theta)\\) inequality constraints.\n\n6.3.1 Power Rule\nLet’s do a real example of an optimization problem. Suppose we shoot a basketball while we stand on a 2 foot plateau, which produces a trajectory function of \\(h(t)= −5t^2 +20t+2\\). Here \\(h(t)\\) is a function representing the ball’s height over time in seconds and the 2 represents the fact that we are standing 2 feet above flat ground. We can find the maximum height of the ball by taking the derivative of the original quadratic function and solving it for 0.\n\n\n\n\n\n\n\n\n\nIn this case, we use the power rule for derivatives. The power rule for dertivatives is where we subtract the exponent value of a function by 1 and place the original value to be multiplied by the base number. For example, the derivative of \\(y=2x^3\\) is just \\(6x^2\\), since \\(3-1=2\\) and \\(2 \\times 3 = 6\\). Here, we can apply this exact principle to obtain the derivative for the function of the ball’s trajectory:\n\\[\n\\begin{aligned}\n& h(t) = -5t^2 + 20t + 2 = \\\\\n& \\frac{d}{dt}(-5t^2) + \\frac{d}{dt}(20t) + \\frac{d}{dt}(2)= \\\\\n& t(5\\times 2) +1(20) = \\overbrace{-10t + 20}^{\\text{Derivative}}\n\\end{aligned}\n\\]\nWe can then solve the derivative for 0.\n\\[\n\\begin{aligned}\n& -10t + 20=0 \\Rightarrow  \\\\\n& -10t = -20 \\Rightarrow \\\\\n& \\frac{-10t}{10} = \\frac{-20}{-10} \\Rightarrow \\\\\n& \\boxed{t=2}\n\\end{aligned}\n\\]\nWe may now plug in the value of 2 to get the maximum height of the ball:\n\\[\n\\begin{aligned}\n& -5t^2 + 20t + 2 \\Rightarrow \\\\\n& -5(2)^2 + 20(2) + 2 \\Rightarrow \\\\\n& -20+40+2=22\n\\end{aligned}\n\\]\nSo, the ball at its maximum is at 22 feet after 2 seconds (strictly speaking, if we wanted to be sure that this was a maximum instead of a minimum, we could take the second derivative). It is the highest point our ball reaches.\n\n\n6.3.2 Chain Rule\nThe next rule of differentiation to know is something called the chain rule. The chain rule is called that because of the chain reaction of one function affecting the value of another function, in mathematics ebing reffered to as a composite function. A common example in economics is where we wish to maximize profits. Consider this example below.\nSuppose we are the manager of shipping for a drug cartel, sometimes called the connect. Our job as connect is to ship cocaine to Miami so that a wholeseller, or a distributor, may sell to smaller gangs who will sell to customers. The distributor is willing to give us 69,000 dollars per kilo. And, as the connect, it is within the cartel’s best interest, therefore ours, to maximize the profits that we make from selling to our distributor. However, we are not a monopoly. We cannot set the price of a kilo, given that other connects are potentially available in the near term. Thus, we must come up with an amount of cocaine to produce such that we maxmize our profits, given some price of cocaine.\nThe revenue we make per ki is simply \\(r(x) = 69x\\). In math, it just means the distributor pays us 69,000 per ki we sell them. If we could run a business without any costs, we’d do just that. But, we do indeed have costs. We have a set of fixed costs that comprises baseline expenditures of doing business with the distributor. For example, we may have transportation costs to move our product from home to Miami, a certain amount of money for fuel, and other costs that we simply cannot avoid to not pay in order to do business. We have variable costs which are things that we as the producers, in the very short run, have direct control over, say, how many drivers we have or how much plastic we use, the number of workers on our farms, and so on. And finally, we have marginal costs, or the costs that we bear as the business for each new unit of product produced. Our cost function for our purposes is \\(c(x)=-0.25x^2+1.5x - 200\\). The variable costs are 1500 dollars, and the fixed costs, or those we pay no matter what, are 200,000 dollars.\nAs I say above, we wish to maximize profit. In microeconomic theory, we’d say the profit we make is the difference between the total revenue from sales and total costs of production, or \\(\\pi=r(x)-c(x)\\). The point on this fuction where we maximize profit is the point at which we cannot produce any more cocaine without decreasing our profits. In other words, all production is not created equally. In other words, if we wish to produce only one ki, a few people could likely do this themselves. But, for every new ki we wish to sell, we need to expand upon things such as the number of workers, the size of our farms, and so on. We may gradually produce more cocaine going from 40 to 50 workers. We may see a big increase of production from 500 to 1000 people… but at some point, we simply can’t have any more people to do work because people wouldn’t be able to fit on the land or do work on it. If we had 500,000 workers, not 1000, this likely would harm profits since there’d be too many cooks in the proverbial kithchen. People wouldn’t be able to move or fit in the area, so this massive increase of humans would actually slow down business and thus decrease profits.\nAnyways, our profit is affected by these two functions. To maximize profit in this case, we have to have to differentiate with respect to each function within the profit function, \\(\\pi(x) = r(x) - c(x)\\). To aid in this (and to presage how I will use this in regression below), we can think of the profit we make as being composed of an “inner” and “outer” function, comprised of the two we’ve discussed so far. Our objective function in this case looks like\n\\[\n\\underset{x}{\\text{argmin}} \\left(\\underbrace{69x}_{r(x)} - \\underbrace{0.25x^2 - 1.5x - 20}_{c(x)} \\right)\n\\]\nFirst, we denote our inner functions \\(r(c)\\) and \\(c(x)\\) \\[\n\\begin{aligned}\nf_1 &= r(x) = 69x \\\\\nf_2 &= c(x)=1.5x - 0.25x^2 - 200.\n\\end{aligned}\n\\]\nHere are our outer functions: \\[\n\\pi(x) = f_{1} + f_{2}.\n\\]\nWe begin by differentiating the inner functions. \\[\n\\begin{aligned}\n&r'(x) = 69 \\\\\n&c'(x) = 1.5 - 0.50x.\n\\end{aligned}\n\\]\nWhy these results? Well, \\(f_{1}\\) is just a linear function. It is the change of the function given a one unit increase in \\(x\\). Since it is linear, the slope is the same everywhere! So, it is just 69. The 200 goes away because it is fixed. We pay those costs anyhow. The derivative for \\(f_{2}\\) comes from the power rule. We get \\(1.5\\) for the first term for the reasons I just stated, then we differentiate the marginal costs term \\(0.25 \\times 2\\), and so we get \\(0.50\\). The outer functions have no variables or exponents attached to them, so we leave them alone. The chain rule for \\(x\\), then, is the difference (in this case) of the derivatives of the individual inner functions themselves, or:\n\\[\n\\dfrac{\\mathrm{d}\\pi}{\\mathrm{d}x} = \\dfrac{\\mathrm{d}r}{\\mathrm{d}x}-\\dfrac{\\mathrm{d}c}{\\mathrm{d}x}.\n\\] This returns \\(\\pi'(x) = 67.5 - 0.50x.\\) In order to find the value of \\(x\\) that profit is maximized at, we solve for 0, or \\(67.5 - 0.50x = 0\\). We compute:\n\\[\n\\begin{aligned}\n& \\text{add to RHS} \\\\\n&67.5 = 0.5x \\\\\n& \\text{and now we divide by the RHS} \\\\\n&x = \\frac{67.5}{0.5} \\\\\n& \\text{which returns our solution} \\\\\n&x \\approx 135\n\\end{aligned}\n\\]\nSo, in order to maximize profits that we will earn from this distributor, we must sell 135 kis. We can prove this to be true by using optimization libraries in Python.\n\n\n\n\n\n\n\n\n\nWe will use these to solve for the value which minimizes the sum of residuals squared. This is known as ordinary least squares regression (OLS), also called linear regression, or simply just “regression”. OLS is the main estimator you’ll use for this class, and it is the main foundation of econometric analysis for public policy research."
  },
  {
    "objectID": "ols.html#an-extended-example",
    "href": "ols.html#an-extended-example",
    "title": "6  OLS Explained",
    "section": "6.4 An Extended Example",
    "text": "6.4 An Extended Example\nTo introduce OLS, we can return to the equation of a line (\\(y=mx+b\\)) where \\(m\\) and \\(b\\) are variables. Unlike the above examples where \\(m\\) and \\(b\\) were once known variables, now they are unknown quantities we must solve for. Below, \\(m\\) and \\(b\\) will take on the values of \\(\\beta_1\\) and \\(\\beta_0\\) respectively. With OLS, we have multiple predictors (typically), each of which affect the output of the function differently.\nIn the multivariable case, we take the partial derivative with resepct to each variable (that is, assuming that the other variables do not change). If this seems at all abstract to you, I will provide a detailed, clear derivation of the betas. Note that for all of the steps below, Stata, R, or Python does (and optimizes!) this process for you. I only provide this derivation so you have a source to refer to when you wish to know how and why, exactly, the machine returns the numbers that it returns to you.\nBefore we continue, let’s fix ideas. Suppose we wish to attend a nightclub and we wish to express how much we pay for that evening as a function (our outcome variable, a ratio level variable). At this nightclub, our outcome is a function of two things. We pay some one-time cost of money to enter, and then we pay some amount of money per new drink we buy (where number of drinks is also a ratio level variable). I say “some” because unlike the real world where we would know the price and entry fees by reading the sign, in this case we wish to estimate these values with only two known variables: how many drinks we bought and how much we paid.\n\n\n\n\n\n\n\n\n\n\n6.4.1 List the Data\nSay that we have data that looks like \\((0, 30), (1, 35), (2, 40)\\), where \\(x\\)= number of drinks we buy (0,1,2) and \\(y\\)=amount of money we spend that evening (30,35,40). In spreadsheet format, this looks like:\n\n\n\nx\ny\n\n\n\n\n0\n30\n\n\n1\n35\n\n\n2\n40\n\n\n\nIf you want to, calculate the rise-over-run of these data points to derive \\(m\\) and see what the answer might be in the end. Below though, we proceed by deriving what \\(m\\) must be.\n\\[\nm = \\frac{35 - 30}{1 - 0} = \\ldots\n\\]\n\n\n6.4.2 Define Our Econometric Model\nWe begin by defining our model. That is, we specify our outcome and the variables which affect our outcome (the values we’re solving for, entry price and drink cost). Our model of how much we pay given some entry fees and additional drink costs looks like:\n\\[\ny_i = \\beta_0 + \\beta_1x\n\\]\nHere, \\(y_i\\) is the total amount of money we spend that evening in dollars given the \\(i\\text{-th}\\) drink, \\(\\hat{\\beta}_{0}\\) is how much we pay (also in dollars) to enter, \\(\\hat{\\beta}_{1}\\) is how much we pay for the \\(i\\text{-th}\\) drink, and \\(x\\) is the total number of drinks we get. Nothing at all about this is different, so far, from anything we’ve discussed above. I’ve simply substituted \\(m\\) and \\(b\\) with the Greek letter \\(\\beta\\) (“beta”) into the already familiar $y=mx+b y_i = _1x+_0 $.\n\n\n6.4.3 Write Out the Objective Function\nNow that we have our model, next let’s think about what the objective function would be. We already know that we wish to minimize the residuals of the OLS line. So, we can represent the objective function for OLS as \\[\n\\begin{aligned}\n& S = {\\text{argmin}} \\sum_{i=1}^{n} \\hat{\\epsilon}^{2}  \\\\\n& S = {\\text{argmin}} \\sum_{i=1}^{n} \\left( y_i - \\hat{y} \\right)^{2} \\\\\n& \\text{where } \\hat{y} \\text{ is defined as} \\\\\n& S = \\underset{\\hat{\\beta}_0,\\hat{\\beta}_1}{\\text{argmin}} \\sum_{i=1}^{n} (y_i - (\\overbrace{\\hat{\\beta}_0 + \\hat{\\beta}_1x}^{\\text{Predictions}}))^2.\n\\end{aligned}\n\\]\nLet’s explain what this means. The word “\\(\\text{argmin}\\)” here means “argument of the minimum”. It means that we are minimizing a function and seeking the values, or arguments, which minimize the ourtput of that function. The symbols underneath \\(S\\), \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\), are the coefficients/inputs we are solving for. I call the objective function “\\(S\\)” for “spending”, but we can call it any letter we’d like. We call the solutions \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\) optimal if they return the lowest possible values the function \\(S\\) can produce. What values can \\(S\\) produce? The sum of the squared residuals. The sigma symbol \\(\\sum_{i=1}^{n}\\) means we are adding up the \\(i\\text{-th}\\) squared residual to the \\(n\\text{-th}\\) data point/number of observations (in this case 3). This means that the line we compute will be as close as it can be to the observed data at every single data point.\nNow that we’ve defined our terms, we can think about what the mathematics of the objective function is actually saying. \\(\\hat{\\epsilon}_i\\) is our predicted residuals, as we’ve mentioned above. The middle formula re-expresses the residuals. This expression should look familiar. Recall the formula for the variance of a variable \\(\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\\) ?. Here, we are taking our real observations \\(y\\) and subtracting some expected value \\(\\hat{y}\\) (y-hat) from it. Because we are minimizing the residuals, another way of thinking about OLS is that is the line that maximizes the explained variance given our independent variables.\nAs with the variance, one may ask why we are squaring the summed \\(\\hat{\\epsilon}_i\\), instead of say the absolute error. First of all, minimizing the raw sum of the predicted residuals (that is, without squaring them) is a non-differentiable function. We could use the raw sum of the errors as an objective function (called the mean absolute error instead of the mean squared error), but have fun doing that, as due to the non-differentiable nature of the aboslute value function, we would need to use numerical methods to compute its solution such as gradient descent. By no means impossible… just computationally less tractable.\nUsing the squared residuals means that we are dealing a quadratic function which, as we did above, is easily differentiable. The squaring of residuals also penalizes worse predictions. Indeed, just as with the variance, all residuals should not be created equally. If the observed value is 20 but we predict 25, the residual is -5. Its squared residual is 25. But if the observed value is 40, and we predict 80, the “absolute” error is -40 and the squared error of is 1600. If we did not square them, we would be treating a residual of 5 as the same weight as a residual of 40. For proof, we can plot these\n\n\n\n\n\n\n\n\n\n\n\n6.4.4 Simplify the Objective Function\nFirst, we can substitute the real values as well as our model for prediction into the objective function. We already know the values x-takes. You either buy no drinks, 1 drink, or 2. So with this information, we can now find the amount of money we pay up front (\\(\\hat{\\beta}_0\\)) and how much it costs for each drink (\\(\\hat{\\beta}_1\\)) \\[\n\\begin{aligned}\nS = &\\underbrace{(30 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 0))^2}_{\\text{Term 1}} + \\\\\n&\\underbrace{(35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1))^2}_{\\text{Term 2}} + \\\\\n& \\underbrace{(40 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 2))^2}_{\\text{Term 3}}\n\\end{aligned}\n\\]\n\n\n6.4.5 Take Partial Derivatives\nTo find the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), we take the partial derivatives of \\(S\\) with respect to (w.r.t.) \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). Here is a short sketch of how we do this: For simplicity, I break this into two sections, one section per coefficient. In this case, the chain rule and power rules for differentiation are our friends here. To hear more about combining the power rule and chain rule, see here. I define one set of inner and outer functions, beginning with the power rule for the outer function, and then taking the derivative for the inner function. First, we differentiate w.r.t. \\(\\hat{\\beta}_0\\) (entry fees), then we do the same for \\(\\hat{\\beta}_1\\) (drink fees).\n\nPartial derivative w.r.t. \\(\\hat{\\beta}_0\\):\n\nFirst, we denote our inner functions. \\[\n\\begin{aligned}\nf_1 &= 30 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 0) \\\\\nf_2 &= 35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1) \\\\\nf_3 &= 40 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 2)\n\\end{aligned}\n\\]\nHere are our outer functions: \\[\nS = f_{1}^{2} + f_{2}^{2} + f_{3}^{2}.\n\\]\nNow, following the chain rule, we have:\n\\[\n\\frac{\\partial S}{\\partial \\hat{\\beta}_0} = \\frac{\\partial S}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial \\hat{\\beta}_0} + \\frac{\\partial S}{\\partial f_2} \\cdot \\frac{\\partial f_2}{\\partial \\hat{\\beta}_0} + \\frac{\\partial S}{\\partial f_3} \\cdot \\frac{\\partial f_3}{\\partial \\hat{\\beta}_0.}\n\\] The first step of the chain rule is using the power rule for the outer functions: \\[\n\\begin{aligned}\n&\\frac{\\partial S}{\\partial f_1} = 2f_1 \\\\\n&\\frac{\\partial S}{\\partial f_2} = 2f_2 \\\\\n&\\frac{\\partial S}{\\partial f_3} = 2f_3.\n\\end{aligned}\n\\] All we’ve done here is used the power rule, taking the 2 from the exponent and putting it on the outside of each term’s outer function. For the next step of the chain rule, we take the derivatives of each term’s inner function w.r.t. \\(\\hat{\\beta}_0\\): \\[\n\\begin{aligned}\n\\frac{\\partial f_1}{\\partial \\hat{\\beta}_0} &= -1 \\\\\n\\frac{\\partial f_2}{\\partial \\hat{\\beta}_0} &= -1 \\\\\n\\frac{\\partial f_3}{\\partial \\hat{\\beta}_0} &= -1.\n\\end{aligned}\n\\] Since the coefficient for \\(-\\hat{\\beta}_0\\) is negative 1, each inner function’s derivative for \\(\\hat{\\beta}_0\\) is just -1. Now, we substitute these inner derivatives back into the \\(f_i\\) functions: \\[\n\\begin{aligned}\n&\\frac{\\partial S}{\\partial \\hat{\\beta}_0} = 2f_1 \\cdot (-1) + 2f_2 \\cdot (-1) + 2f_3 \\cdot (-1) \\\\\n&= -2f_1 - 2f_2 - 2f_3 \\\\\n&= -2(30 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 0)) \\\\\n& - 2(35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1)) \\\\\n&-2(40 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 2))\n\\end{aligned}\n\\]\nNow, we simplify this expression. Let’s first expand each term by distributing the 2:\n\\[\n= -60 + 2\\hat{\\beta}_0 - 70 + 2\\hat{\\beta}_0 + 2\\hat{\\beta}_1 - 80 + 2\\hat{\\beta}_0 + 4\\hat{\\beta}_1.\n\\]\nNext I rearrange. I add parentheses for readability:\n\\[\n= (-60 - 70 - 80) + (2\\hat{\\beta}_0 + 2\\hat{\\beta}_0 + 2\\hat{\\beta}_0) + (2\\hat{\\beta}_1 + 4\\hat{\\beta}_1).\n\\] Finally, when we combine like terms, we end with \\[\n= \\boxed{-210 + 6\\hat{\\beta}_0 + 6\\hat{\\beta}_1}.\n\\]\n\nPartial derivative w.r.t. \\(\\hat{\\beta}_1\\):\n\nFollowing from the above, here are our inner functions \\[\n\\begin{aligned}\nf_1 &= 30 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 0) \\\\\nf_2 &= 35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1) \\\\\nf_3 &= 40 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 2).\n\\end{aligned}\n\\]\nThen the outer functions:\n\\[\nS = f_1^2 + f_2^2 + f_3^2.\n\\]\nJust as before, here’s the chain rule to get the partial derivative \\[\n\\frac{\\partial S}{\\partial \\hat{\\beta}_1} = \\frac{\\partial S}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial \\hat{\\beta}_1} + \\frac{\\partial S}{\\partial f_2} \\cdot \\frac{\\partial f_2}{\\partial \\hat{\\beta}_1} + \\frac{\\partial S}{\\partial f_3} \\cdot \\frac{\\partial f_3}{\\partial \\hat{\\beta}_1}.\n\\]\nFirst I apply the power rule to the the outer functions:\n\\[\n\\begin{aligned}\n\\frac{\\partial S}{\\partial f_1} &= 2f_1 \\\\\n\\frac{\\partial S}{\\partial f_2} &= 2f_2 \\\\\n\\frac{\\partial S}{\\partial f_3} &= 2f_3.\n\\end{aligned}\n\\]\nNext, we’ll do the inner functions:\n\\[\n\\begin{aligned}\n\\frac{\\partial f_1}{\\partial \\hat{\\beta}_1} &= 0 \\\\\n\\frac{\\partial f_2}{\\partial \\hat{\\beta}_1} &= -1 \\\\\n\\frac{\\partial f_3}{\\partial \\hat{\\beta}_1} &= -2\n\\end{aligned}\n\\] Note how \\(\\hat{\\beta}_1\\) isn’t in term 1 since it is multiplied by 0 \\[\n\\underbrace{(30 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 0))^2}_{\\text{Term 1}},\n\\] its derivative must be 0. This means the first outer function goes away because \\(f_1 \\times 0=0\\). So, I omit this from my derivation. Next, we’ll substitute these remaining two derivatives back into the expression for the outer function:\n\\[\n\\begin{aligned}\n&\\frac{\\partial S}{\\partial \\hat{\\beta}_1} = 2f_2 \\cdot (-1) + 2f_3 \\cdot (-2) \\\\\n&= -2f_2 - 4f_3 \\\\\n&= 2(35 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1)) - 4(40 - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 2)).\n\\end{aligned}\n\\] Now, we combine like terms: \\[\n\\frac{\\partial S}{\\partial \\hat{\\beta}_1} = -2(35 - \\hat{\\beta}_0 - \\hat{\\beta}_1) - 4(40 - \\hat{\\beta}_0 - 2\\hat{\\beta}_1).\n\\] To simplify this expression, first we’ll expand each term by distributing the -2 and -4. Here is term 1:\n\\[\n-2(35 - \\hat{\\beta}_0 - \\hat{\\beta}_1) = -70 + 2\\hat{\\beta}_0 + 2\\hat{\\beta}_1.\n\\] Now we expand term 2: \\[\n-4(40 - \\hat{\\beta}_0 - 2\\hat{\\beta}_1) = -160 + 4\\hat{\\beta}_0 + 8\\hat{\\beta}_1.\n\\] Now, combine like terms: \\[\n= (-70 + 2\\hat{\\beta}_0 + 2\\hat{\\beta}_1) + (-160 + 4\\hat{\\beta}_0 + 8\\hat{\\beta}_1).\n\\] I move the constants outside of the parentheses. I add adding parentheses for readability:\n\\[\n= (-70 - 160) + (2\\hat{\\beta}_0 + 4\\hat{\\beta}_0) + (2\\hat{\\beta}_1 + 8\\hat{\\beta}_1).\n\\]\nAfter subtracting the constants and combining like terms once more, we’re left with\n\\[\n\\boxed{-230 + 6\\hat{\\beta}_0 + 10\\hat{\\beta}_1}.\n\\]\n\n\n6.4.6 Get the Betas\nNow we’ve set up our partial derivatives, so we can now return to normal scalar algebra to get our betas (in practice this is done with matrix algebra). Either way, we must solve a system of equations. The reason we must solve them as a system of equations is because we need for BOTH values to be the ones which minimize the distance between the model’s predictions and the observed data. Here are the equations we must solve for (note, we have two equations because we have two variables, but this can readily be extended to more variables): \\[\n\\begin{aligned}\n&\\hat{\\beta}_0 + \\hat{\\beta}_1 = 35 \\\\\n&6\\hat{\\beta}_0 + 10\\hat{\\beta}_1 = 230.\n\\end{aligned}\n\\]\nHere I use a method called substitution to solve the system, but there are many such ways we can solve this. I choose to solve the first parital first since it is by far the easiest.\n\n6.4.6.1 Solve for \\(\\hat{\\beta}_0\\):\nThe first step of substitution is to solve for one of our variables. \\[\n\\begin{aligned}\n&\\hat{\\beta}_0 + \\hat{\\beta}_1 = 35 \\Rightarrow \\\\\n&\\hat{\\beta}_0 + (\\hat{\\beta}_1-\\hat{\\beta}_1) = 35-\\hat{\\beta}_1 \\Rightarrow \\\\\n&\\hat{\\beta}_0 = 35 - \\hat{\\beta}_1\n\\end{aligned}\n\\] Before I continue, notice a few interesting things about this: we know, by construction of the problem, that the entry fee \\(\\hat{\\beta}_0\\) is some positive number. We also know \\(\\hat{\\beta}_1\\) has to be less than 35. Imagine a \\(\\hat{\\beta}_1 \\geq 35\\). This would mean \\(\\hat{\\beta}_0\\) (our entry costs) would be kind of unusual. In this setting we at least get in for free or, even sillier, are paid to enter when \\(\\hat{\\beta}_1 > 35\\), which does not make sense given the problem at hand. We also know that our entry fees \\(\\hat{\\beta}_0\\) can’t be 35, because then this means that \\(\\hat{\\beta}_1\\) would have to be 0, or a scenario where drinks are free (or, being paid to drink if \\(\\hat{\\beta}_0 > 35\\)).\n\n\n\n\n\n\nImportant\n\n\n\nThe reason I’m making such a fuss about this and carrying on with the example in this manner is because I want you to think of regression as a way of actually explaining the world, not just a set of arcane mathematical equations. This maps nicely on to the example we’ve developed.\n\n\nNow, we continue to solve.\n\n\n6.4.6.2 Substitute \\(\\hat{\\beta}_0\\):\nSince we know the expression for the constant (the entry fee), we can plug it into the partial for \\(\\hat{\\beta}_1\\), \\[\n\\begin{aligned}\n&6\\hat{\\beta}_0 + 10\\hat{\\beta}_1 = 230. \\\\\n&6(35 - \\hat{\\beta}_1) + 10\\hat{\\beta}_1 = 230.\n\\end{aligned}\n\\] Next, we distribute the 6 \\[\n210 - 6\\hat{\\beta}_1 + 10\\hat{\\beta}_1 = 230\n\\] and combine the terms \\(- 6\\hat{\\beta}_1 + 10\\hat{\\beta}_1\\) together \\[\n210 + 4\\hat{\\beta}_1 = 230.\n\\] Next, we subtract 210 \\[\n4\\hat{\\beta}_1 = 20.\n\\] Finally, we divide by 4 \\[\n\\boxed{\\hat{\\beta}_1 = 5}.\n\\]\nNow, we know our value for \\(\\hat{\\beta}_1\\)!!! We know that for each drink we get, we pay 5 more dollars. Since we now know this, we substitute 5 into \\(\\hat{\\beta}_0 + \\hat{\\beta}_1 = 35\\) where \\(\\hat{\\beta}_1\\) is. Then, we have one equation to solve for, with our goal being to get the value of \\(\\hat{\\beta}_0\\). \\[\n\\hat{\\beta}_0 + 5 = 35.\n\\]\nNow, we simply subtract 5 from the RHS \\[\n\\boxed{\\hat{\\beta}_0 = 30}.\n\\]\nThe entry fee is 30 dollars.\n\n\n\n6.4.7 Our OLS Line of Best Fit\nSo, our line of best fit is \\(\\hat{y} = 30 + 5x\\). In social science, you’ll hear people throw around terms like “when we controlled for this” or “adjusted for” another variable, or “when we hold constant these set of variables”. This is what they mean by it. They, in the plainest language possible, mean that the dependent variable changes by some amount for every unitary increase in an independent variable, assuming the other values of the function don’t change. So here, assuming the club has a flat entry fee that does not change on a person to person basis, the amount the function changes by for every new drink is an increase of 5 dollars. Or, compared to the scenario where you only wanted to get in the club (and not drink at all, where \\(x=0\\)), you spend 5 more dollars per each new drink you get.\nAt the outset, one may ask why we did this at all. Why bother with the partial derivative approach and the messy system of equations? The reason is firstly pedagogical. OLS was never derived for me in quite this manner in undergrad, so I believe you should see it done with a simple, tractable, familiar example. Additionally, when we understand how the partial derivative allows us to look at the change of an overall function given the change of just one variable in that function, we can quickly see how we can attempt to isolate the causal impact of multiple variables. Strictly speaking, we did just that right here in this derivation! We considered the impact of BOTH entry fees and the number of drinks we bought too (after all, suppose entry was free, then we have no constant)."
  },
  {
    "objectID": "ols.html#inference-for-ols",
    "href": "ols.html#inference-for-ols",
    "title": "6  OLS Explained",
    "section": "6.5 Inference For OLS",
    "text": "6.5 Inference For OLS\nNow that we’ve conducted estimation, we can now conduct inference with these statistics we’ve generated. Indeed, this is the primary point of this at all, in a sense. We want to know if these estimates are different from some null hypothesis. To begin, recall the notation of \\(\\hat{\\epsilon}_i\\) which denotes our residuals for the regression predictions. We can use this to generate the standard error/the uncertainty statistic associated with the respective regression estimate. We can begin with the residual sum of squares, calcualted like \\(\\text{RSS} = \\sum (\\hat{\\epsilon}_{i})^2\\). Put another way, it is all the variation not explained by our model. If \\(RSS=0\\), as was the case in the above example, then we have no need for inference since there’s nothing our model does not explain. We then can estimate the variance of the error like \\(\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n - p}\\), where \\(n\\) is our number of observations and \\(p\\) is our number of predictors (including the constant). We divide by \\(n-p\\) because this takes into account our model’s residual degrees of freedom, or our model’s freedom to vary. Note as well that when \\(n=p\\), the error variance is not defined, meaning for OLS to be valid we need less predictors than observations. For a more detailed explanation of degrees of freedom, see Pandey and Bright (2008).\nWith this, we can estimate the variance-covariance matrix as \\[\n\\text{Var}(\\hat{\\beta}) = \\hat{\\sigma}^2 (X^T X)^{-1}\n\\] This allows us to compute our standard error: \\(\\text{SE}(\\hat{\\beta}_j) = \\sqrt{\\text{Var}(\\hat{\\beta})_{jj}}\\).\n\n\n\n\n\n\nImportant\n\n\n\nAgain, I will never ever ask you to calculate the standard error or t-statistic without the necessary coefficients to calculate it algebraically. The reason I’m showing you this is just so you have a very clear sense of where these numbers are coming from,\n\n\nAs we’ve discussed with normal descriptive statistics/classic hypothesis testing, we can also compute confidence intervals for these regression estimates. The formula for this should appear quite familiar \\[\n\\beta_j \\pm t_{\\alpha/2, \\text{df}} \\cdot \\text{SE}(\\beta_j)\n\\]\nHere, \\(\\beta_j\\) is the coefficient of our model, \\(t\\) is our test statistic (1.96 ususally), \\(\\alpha\\) is our acceptance region (0.05 in most cases if we want a 95% confidence interval), SE is our standard error as we’ve computed it above and df is our degrees of freedom. Then, we can interpret our coefficients accordingly. Suppose a confidence interval for a coefficient 6.7 lies within 6.06 and 16.44. We interpret this as follows: given the dataset and input values (and assuming the OLS assumptions are valid, which we will detail below), we are 95% confident that the true value for that coefficient lies within the range of 6.06 and 16.44. In other words, there’s a “width” of this confidence interval by about 10.\nJust as we discussed in the preceding chapters, OLS’s estimates (conditional on the assumptions being met, which we will cover after this) is only justified, asymptotically, based on the law of large numbers and CLT. In other words, as n tends to infinity, \\(\\lim\\limits_{n\\to \\infty}\\), our betas will converege to the true population value and the standard errors will shrink. Ergo, as these shrink, the confidence intervals will tighten, meaning our estimates will be more precise. A practical consequence of this is that as a very general rule, having more observations in your dataset makes your OLS estimates more precise and less biased.\n\n6.5.1 Goodness of Fit Measures for OLS\nWe typically thhink of two goodness of fit statistics when using OLS, the R-squared statistics and the Root Mean Squared Error\n\\[\n\\begin{aligned}\nR^2 &= 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}} & \\text{RMSE} &= \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}.\n\\end{aligned}\n\\] Here, for R-squared, we have two terms: first, \\(\\text{SS}_{\\text{res}} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\) is the sum of squared residuals. \\(\\text{SS}_{\\text{res}}\\) quantifies the amount of variance unexplained by the independent variables in the model, and \\(\\text{SS}_{\\text{tot}}\\) is just the total amount of variance of \\(y\\). Think of R-squared as a ratio/percentage of how well the model explains our outcomes. An R-squared of 1 reflects the simple example we derived above, where the model perfectly explains the variation of our outcomes. R-squared usually scales with the amount of predictors in the model (that is, as we add more variables, the R-squared will increase). However, I think the best way to think about R-squared is a measure of how well our model does, compared to the average of our outcomes. In the nightclub example, if we just took the average of our outcomes, we’d guess for that evening, you’d spend 28.3 dollars. But, in this case, the model significantly outperforms this since it explains the variation perfectly. Note, that it is possible to have a negative R-squared. It is very rare, and it basically means that your model does a worse job than the simple average of the outcomes. I’ve seen this in my work, but I’ve only encountered it in the wild maybe twice.\nThe Root Mean Squared error, or RMSE, is exactly as it sounds: it is the square root of our average of our \\(\\text{SS}_{\\text{res}}\\). For the tobacco dataset for example that I include above, our sum of residuals squared is 26015.2655. When we divide this by our residual degrees of freedom, or \\(38-1\\), we get 703.115283. When we take the square root, we get 26.516. In English, this simply means that when we use price to explain consumption for the year 1980, the model is off, on average, by about 27 packs. Considering that the average cigarette consumption in 1980 was 137.6308, being off by 26 packs isn’t bad, and it suggests, as one would expect, that we’ve explained our dataset fairly well. As RMSE approaches 0, we explain our variation better, with an RMSE of 0 being perfection. Note that other goodness of fit metrics do exist; however, these are the most common ones you’ll encounter."
  },
  {
    "objectID": "ols.html#assumptions-of-ols",
    "href": "ols.html#assumptions-of-ols",
    "title": "6  OLS Explained",
    "section": "6.6 Assumptions of OLS",
    "text": "6.6 Assumptions of OLS\nKeep in mind, despite all the detail we’ve discussed so far, do not lose sight of the larger picture: OLS is simply a mathematical estimation method. Its validity for explaining the external world (aside from having quality data) relies on a few assumptions (collectively called the Gauss-Markov assumptions) being defensible. I say defensible instead of true because practically they are never true. After all, this is statistics: almost all of statistics is true. All statistical research (pretty much, outside of simulations) is at least partly wrong because we live in a probalistic world where we don’t have all the answers. In other words, the assumptions are only as valid as we can defend for them. Below, I detail them.\n\n6.6.1 Assumption 1: Linear in Parameters\nThe very first assumption is that the parameters for our model are linear. The classic regression model for OLS is\n\\[\ny_{i} = \\beta_{0} + \\beta_{1} x_{i1} + \\cdots + \\beta_{K} x_{iK} + \\epsilon_{i}.\n\\]\nWe call this a linear model. Why? How do we know if it’s a linear relationship, and what might violations of this look like? Let’s say we’re buying weed. Say the price per quarter ounce is 80 dollars. The impact of \\(\\beta_{1}\\) is the same everywhere in the function, \\(y=80x\\). But step back and ask ourselves, from the seller’s persepctive, if this makes sense: does it make sense for weed to cost the same for every weight amount? No! Why not? Well, for one, let’s say you’re selling a full gram or pound of weed. That’s so much weed that weed(wo)men/people will charge much much more for lone individuals who wish to buy this much. So while it may be 80 for a quarter ounce, it’ll now be, say, 900 per pound. In fact, we could express this as a piecewise function\n\\[\n\\beta_{j} =\n\\begin{cases}\n    80 \\text{ if } x < 1 \\\\\n    900 \\text{ if } x > 1. \\\\\n\\end{cases}\n\\]\nWhy might this be done? Firstly, that’s so much more product than the average person could smoke or use. So, anyone interested in this would need to pay premium prices for such an outlandish amount. Also, it allows the dealer to get the pound of weed off their hands– relative to ounces, pounds of weed are much more likely to be noticed by police and therefore punished by the law harsher. So, the quicker they sell, the quicker they may re-up. So, for the normal crowd of people who do not but pounds, they pay one price. For those who are abnormal in how much they demand (say, the distributor for the connect for cocaine markets), they pay another price altogether. We see price discrimination in legal markets too, such as Sams Club. We can see that a regression model here IS NOT linear in parameters, since the slope of the line will change at different values of the function.\nPeople often confuse this assumption with non-linear values of our independent variables as they relate to our outcome. They conflate nonlinear regression \\[\ny_{i} = \\beta_{0}^{2} + \\beta_{1}^{2} x_{i1} + \\cdots + \\beta_{K}^{2} x_{iK} + \\epsilon_{i}\n\\] with \\[\ny_{i} = \\beta_{0} + \\beta_{1} x_{i1}^{2} + \\cdots + \\beta_{K} x_{iK}^{2} + \\epsilon_{i},\n\\] or an OLS model with non-linear relations between the inputs and the outcomes. Let me explain why this is wrong, because as it turns out, we can indeed model curvature. I’ve already given an example of when we’d have a nonlinear realtionship in terms of our betas. Now I discuss non-linearities in terms of our predictors. Let’s say we wish to model how much someone can dead lift given some input of age. Let’s say the true population parameter for the OLS model is 6 (we ignore the constant for exposition purposes) \\[\ny_{i} = 6x_{i}\n\\] What is our value for 0? 0, since you’re not yet born. For age 10? 60. For age 30? 180. For age 80? 480. I think we can already see why this relationship being modeled would be silly: it presumes that the older you get, the stronger one is. Which, generally speaking, is true… but we also know that at some point, as with all things, glory fades. Someone that was once strong and in shape will not (in general) always be that way because the body declines with the passage of time. How do we take this into account for our regression model, though?\n\\[\ny_{i} = \\beta_{1}(x_{i1} \\times x_{i}) +  x_{i} \\equiv y_{i} = \\beta_{1}x^{2}_{i}\n\\] We simply square the original value of age, keeping its linear form in the regression model. That way, when age 4 is input in the model, the number our regression model reads in the computer is 16. When age 10 is put into the model, it reads 100. Of course, as one would expect, there’s likely some critical point for this function, where people begin to be able to lift less given some values of age.\nAnother example of being able to account for non-linearities from economics is the idea of modeling how much produce one may produce given a set of labor inputs. Suppose we’re cooking cocaine. With just two people, you can get work done, but it won’t be a lot. With three people, you can do more, and more with each additional person. However, there’s an idea in economics called diminishing marginal returns for the factors of production (in this case labor). You may be able to cook a lot with 10 or 20 people, but when you have 40 or 50 people, at some point we end up producing less because there’s too many proverbial cooks in the kitchen. So, if we wished to model output of cocaine as a function of labor, we’d likely wish to square the “number of workers” part of our model since it does not make sense to expect production to increase perfectly linearly with every new human working with you. So you see, the linear in parameters assumption deals with our betas impact on our predictor variables, not the input values of our predictor variables.\n\n\n6.6.2 Assumption 2: Random Sample\nWe next presume that we’ve collected a random sample of our population. The name random sample is something of an antiquated, romantic name to denote the idea that the sample we’ve collected is representative of the population we wish to map on to. Suppose we wish to investigate the relationship between introducing all virtual menus at a restaurant (the kind you scan on your phone) to see if it increases how much money they make for [random marketing reasons]. We take all the restaurants in Buckhead and Sandy Springs in Atlanta as a sample, comparing the ones that did this intervention to the ones that didn’t do the intervention. We get a coefficient of 10,000, suggesting that this intervention increased money made, on average, by 10,000 dollars compared to the places that didn’t do this. The issue with this idea is that our sample is not a random sample of the population. Sandy Springs and Buckhead, in particular, are among the wealthiest areas in Atlanta. We can’t generalize the effet of this intervention to the population (restuarants in Atlanta, say) becuase our units of intertested are decidedly not representative of Atlanta’s entire culinary scence. They have very specific customers that make a generalization a bad idea.\nAnother example can come from sports. Say we posit a relationship between height and skill at basketball. We take a sample of NBA players, run a few regressions for relevant metrics and have our software spit out coefficients at us. Can we generalize to the population? No!! The NBA is one of the most selective sports leagues on the planet. The NBA selectd for height and skill, among other things. The worst player on the NBA bench is like a god from Olympus compared to the average human, physically and in terms of skill. They are not representative of even a human 2 standard deviations above the mean. So, we cannot use NBA players generalize to the population, unless of course we are concerned only with NBA players. The same would apply to the first example: if we care only about the high-income restaurants in the city, then that’s great, but assuming we wish to generalize more broadly, we will need more data from other, more diverse units that have more information encoded in their outcome about the sample.\n\n\n6.6.3 Assumption 3: No Perfect Collinearity\nThe simple way to think about this one is we cannot include redundant variables in our regressions. Suppose we wish to predict the ticket sales of NBA teams. In our regression, we include the number of games won as well as the number of games lost (2 variables). Well, these are mirror images of each other. The number of games you won is a direct function of the total games minus the number you lost, and the number you lost is a direct and perfect function of the total minus the number you won.\nBy extension, suppose we wish to compare women to men (say we wish to test that men earn more/less than women on average). We take data on 500 respondents who we’ve sampled randomly across a company. We have one column that denotes the respondent as male and the other as female. We cannot include both male and female columns in our models, these are perfect linear functions of one another. A female is necessarily not coded as male, and male is necessarily not coded as female. Practically, this means we must choose when we use a categorical variable in our models. Say our regression includes age ang gender as a predictor. If category 1 of gender is female and category 0 is male, then if the beta for “gender” is -30, we would interpret the beta for gender as “holding constant age/compared to men of the same age group, female respondents earn about 30 dollars less than men.” By extension, the coefficient for male (if we decided to include this group as the group of interest) would just be 30, with a similar interpretation in the other direction.\n\n\n6.6.4 Assumption 4: Strict Exogeneity: \\(\\mathbb{E}[\\epsilon_{i} | x_{i1}, \\ldots, x_{iK}] = 0\\)\nNext we presume strict exogeneity. Formally, this means the average of our errors, given the set of covariates we’ve controlled for, is 0. It means our predictor variables may not be correlated with the error term. In other words, we cannot omit important variables from our model. For example, say we wish to see how the number of firefighters sent to a call affects the amount of damage from that fire. We conclude that there’s a positive relationship between number of firefighters sent and damage, so we elect to send less people to future calls, using a simple bivariate model as justification. Is this a good idea? No!!!! People will die like that. Presumably, the firefighters are not pouring gasoline on the fire, so perhaps we’ve omitted things from our model that might influence both how many people we send as well as fire damage. What else should we control for? Maybe, building size, building type, neighborhood income status, temperature, and other relevant predictors to ensure that we are not blaming the outcomes on a spurious relationship. Indeed, on some level we would expect for the size of the fire to be correlated with the number of people sent to fight it. Thus, when we do not control for other factors, our coefficients, no matter how precise, suffer from omitted variable bias. Strict exogeneity is pretty much never met in real life, but it basically posits that there’s no other critical variable missing from our regression model that may explain our outcome. This is also why it matters to critically think about the variables one will use in their regression model before they run regressions."
  },
  {
    "objectID": "treatmenteffects.html",
    "href": "treatmenteffects.html",
    "title": "7  Causal Inference",
    "section": "",
    "text": "An intro to causality…"
  }
]